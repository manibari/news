#!/usr/bin/env python3
"""
è³‡æ–™é·ç§»å·¥å…· - SQLite åˆ° PostgreSQL

å°‡ç¾æœ‰ SQLite è³‡æ–™é·ç§»è‡³ PostgreSQL

ä½¿ç”¨æ–¹å¼:
    python tools/migrate_data.py --source sqlite --target postgresql
    python tools/migrate_data.py --tables news --incremental
    python tools/migrate_data.py --dry-run

ç’°å¢ƒè®Šæ•¸:
    DB_HOST, DB_PORT, DB_NAME, DB_USER, DB_PASSWORD
"""

import argparse
import sys
from pathlib import Path
from datetime import datetime

# å°‡å°ˆæ¡ˆæ ¹ç›®éŒ„åŠ å…¥ Python path
sys.path.insert(0, str(Path(__file__).parent.parent))

# å˜—è©¦è¼‰å…¥ .env
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass


def get_sqlite_client():
    """å–å¾— SQLite å®¢æˆ¶ç«¯"""
    from src.data.sqlite_client import SQLiteClient
    return SQLiteClient()


def get_postgresql_client():
    """å–å¾— PostgreSQL å®¢æˆ¶ç«¯"""
    from src.data.postgresql_client import PostgreSQLClient
    return PostgreSQLClient()


def migrate_news(source, target, batch_size=1000, dry_run=False):
    """é·ç§»æ–°èè³‡æ–™"""
    print("\nğŸ“° é·ç§»æ–°èè³‡æ–™...")

    # å–å¾—æ‰€æœ‰æ–°è
    offset = 0
    total_migrated = 0

    while True:
        news_list = source.get_news(limit=batch_size, offset=offset)
        if not news_list:
            break

        if dry_run:
            print(f"  [DRY RUN] å°‡é·ç§» {len(news_list)} ç­†æ–°è (offset={offset})")
        else:
            count = target.insert_news_bulk(news_list)
            total_migrated += count
            print(f"  å·²é·ç§» {count} ç­†æ–°è (offset={offset})")

        offset += batch_size

    print(f"  âœ… æ–°èé·ç§»å®Œæˆï¼Œå…± {total_migrated} ç­†")
    return total_migrated


def migrate_watchlist(source, target, dry_run=False):
    """é·ç§»è¿½è¹¤æ¸…å–®"""
    print("\nğŸ“Š é·ç§»è¿½è¹¤æ¸…å–®...")

    watchlist = source.get_watchlist(active_only=False)

    if dry_run:
        print(f"  [DRY RUN] å°‡é·ç§» {len(watchlist)} æª”è‚¡ç¥¨")
        return len(watchlist)

    count = 0
    for item in watchlist:
        if target.add_to_watchlist(
            symbol=item["symbol"],
            name=item.get("name"),
            market=item.get("market"),
            sector=item.get("sector"),
            industry=item.get("industry")
        ):
            count += 1

    print(f"  âœ… è¿½è¹¤æ¸…å–®é·ç§»å®Œæˆï¼Œå…± {count} æª”")
    return count


def migrate_daily_prices(source, target, batch_size=5000, dry_run=False):
    """é·ç§»æ¯æ—¥åƒ¹æ ¼"""
    print("\nğŸ’¹ é·ç§»æ¯æ—¥åƒ¹æ ¼...")

    # å…ˆå–å¾—æ‰€æœ‰è‚¡ç¥¨ä»£ç¢¼
    symbols = source.get_symbols()
    total_migrated = 0

    for symbol in symbols:
        prices = source.get_daily_prices(symbol)
        if not prices:
            continue

        if dry_run:
            print(f"  [DRY RUN] {symbol}: {len(prices)} ç­†åƒ¹æ ¼")
            total_migrated += len(prices)
        else:
            count = target.insert_daily_prices_bulk(prices)
            total_migrated += count
            print(f"  {symbol}: {count} ç­†åƒ¹æ ¼")

    print(f"  âœ… åƒ¹æ ¼é·ç§»å®Œæˆï¼Œå…± {total_migrated} ç­†")
    return total_migrated


def migrate_macro_indicators(source, target, dry_run=False):
    """é·ç§»ç¸½ç¶“æŒ‡æ¨™å®šç¾©"""
    print("\nğŸ“ˆ é·ç§»ç¸½ç¶“æŒ‡æ¨™å®šç¾©...")

    indicators = source.get_macro_indicators(active_only=False)

    if dry_run:
        print(f"  [DRY RUN] å°‡é·ç§» {len(indicators)} å€‹æŒ‡æ¨™")
        return len(indicators)

    count = 0
    for indicator in indicators:
        if target.insert_macro_indicator(indicator):
            count += 1

    print(f"  âœ… æŒ‡æ¨™å®šç¾©é·ç§»å®Œæˆï¼Œå…± {count} å€‹")
    return count


def migrate_macro_data(source, target, dry_run=False):
    """é·ç§»ç¸½ç¶“æ•¸æ“š"""
    print("\nğŸ“‰ é·ç§»ç¸½ç¶“æ•¸æ“š...")

    indicators = source.get_macro_indicators(active_only=False)
    total_migrated = 0

    for indicator in indicators:
        series_id = indicator["series_id"]
        data_list = source.get_macro_data(series_id)

        if not data_list:
            continue

        if dry_run:
            print(f"  [DRY RUN] {series_id}: {len(data_list)} ç­†æ•¸æ“š")
            total_migrated += len(data_list)
        else:
            count = target.insert_macro_data_bulk(series_id, data_list)
            total_migrated += count
            print(f"  {series_id}: {count} ç­†æ•¸æ“š")

    print(f"  âœ… ç¸½ç¶“æ•¸æ“šé·ç§»å®Œæˆï¼Œå…± {total_migrated} ç­†")
    return total_migrated


def migrate_market_cycles(source, target, dry_run=False):
    """é·ç§»å¸‚å ´é€±æœŸ"""
    print("\nğŸ”„ é·ç§»å¸‚å ´é€±æœŸ...")

    # å–å¾—æœ€æ–°é€±æœŸï¼ˆç›®å‰ API åªæ”¯æ´å–æœ€æ–°ä¸€ç­†ï¼‰
    latest = source.get_latest_cycle()

    if not latest:
        print("  âš ï¸ ç„¡å¸‚å ´é€±æœŸè³‡æ–™")
        return 0

    if dry_run:
        print(f"  [DRY RUN] å°‡é·ç§»é€±æœŸ: {latest.get('date')} - {latest.get('phase')}")
        return 1

    if target.insert_market_cycle(latest):
        print(f"  âœ… å¸‚å ´é€±æœŸé·ç§»å®Œæˆ: {latest.get('date')} - {latest.get('phase')}")
        return 1

    return 0


def main():
    parser = argparse.ArgumentParser(
        description="è³‡æ–™é·ç§»å·¥å…· - SQLite åˆ° PostgreSQL",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¯„ä¾‹:
  python tools/migrate_data.py                    # é·ç§»æ‰€æœ‰è³‡æ–™
  python tools/migrate_data.py --tables news     # åªé·ç§»æ–°è
  python tools/migrate_data.py --dry-run         # æ¨¡æ“¬åŸ·è¡Œï¼Œä¸å¯¦éš›å¯«å…¥
  python tools/migrate_data.py --tables news,watchlist  # é·ç§»å¤šå€‹è¡¨

å¯ç”¨çš„è¡¨æ ¼:
  news, watchlist, daily_prices, macro_indicators, macro_data, market_cycles
        """
    )

    parser.add_argument(
        "--source",
        type=str,
        default="sqlite",
        choices=["sqlite"],
        help="ä¾†æºè³‡æ–™åº« (é è¨­: sqlite)"
    )
    parser.add_argument(
        "--target",
        type=str,
        default="postgresql",
        choices=["postgresql", "supabase"],
        help="ç›®æ¨™è³‡æ–™åº« (é è¨­: postgresql)"
    )
    parser.add_argument(
        "--tables",
        type=str,
        default="all",
        help="è¦é·ç§»çš„è¡¨æ ¼ï¼Œä»¥é€—è™Ÿåˆ†éš” (é è¨­: all)"
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=1000,
        help="æ‰¹æ¬¡å¤§å° (é è¨­: 1000)"
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="æ¨¡æ“¬åŸ·è¡Œï¼Œä¸å¯¦éš›å¯«å…¥"
    )

    args = parser.parse_args()

    print("=" * 60)
    print("è³‡æ–™é·ç§»å·¥å…·")
    print("=" * 60)
    print(f"ä¾†æº: {args.source}")
    print(f"ç›®æ¨™: {args.target}")
    print(f"è¡¨æ ¼: {args.tables}")
    print(f"æ‰¹æ¬¡å¤§å°: {args.batch_size}")
    print(f"æ¨¡æ“¬åŸ·è¡Œ: {args.dry_run}")
    print("=" * 60)

    # å»ºç«‹å®¢æˆ¶ç«¯
    try:
        source = get_sqlite_client()
        print("âœ… SQLite ä¾†æºé€£ç·šæˆåŠŸ")
    except Exception as e:
        print(f"âŒ SQLite é€£ç·šå¤±æ•—: {e}")
        sys.exit(1)

    try:
        if args.target == "postgresql":
            target = get_postgresql_client()
        else:
            from src.data.supabase_client import SupabaseClient
            target = SupabaseClient()
        print(f"âœ… {args.target} ç›®æ¨™é€£ç·šæˆåŠŸ")
    except Exception as e:
        print(f"âŒ {args.target} é€£ç·šå¤±æ•—: {e}")
        sys.exit(1)

    # æ±ºå®šè¦é·ç§»çš„è¡¨æ ¼
    all_tables = [
        "news", "watchlist", "daily_prices",
        "macro_indicators", "macro_data", "market_cycles"
    ]

    if args.tables == "all":
        tables = all_tables
    else:
        tables = [t.strip() for t in args.tables.split(",")]

    # åŸ·è¡Œé·ç§»
    start_time = datetime.now()
    results = {}

    for table in tables:
        if table == "news":
            results["news"] = migrate_news(source, target, args.batch_size, args.dry_run)
        elif table == "watchlist":
            results["watchlist"] = migrate_watchlist(source, target, args.dry_run)
        elif table == "daily_prices":
            results["daily_prices"] = migrate_daily_prices(source, target, args.batch_size, args.dry_run)
        elif table == "macro_indicators":
            results["macro_indicators"] = migrate_macro_indicators(source, target, args.dry_run)
        elif table == "macro_data":
            results["macro_data"] = migrate_macro_data(source, target, args.dry_run)
        elif table == "market_cycles":
            results["market_cycles"] = migrate_market_cycles(source, target, args.dry_run)
        else:
            print(f"âš ï¸ æœªçŸ¥çš„è¡¨æ ¼: {table}")

    # ç¸½çµ
    end_time = datetime.now()
    duration = (end_time - start_time).total_seconds()

    print("\n" + "=" * 60)
    print("é·ç§»å®Œæˆç¸½çµ")
    print("=" * 60)

    for table, count in results.items():
        print(f"  {table}: {count} ç­†")

    print(f"\nç¸½è€—æ™‚: {duration:.2f} ç§’")

    if args.dry_run:
        print("\nâš ï¸ é€™æ˜¯æ¨¡æ“¬åŸ·è¡Œï¼Œå¯¦éš›è³‡æ–™æœªè¢«ä¿®æ”¹")
        print("ç§»é™¤ --dry-run åƒæ•¸ä»¥åŸ·è¡Œå¯¦éš›é·ç§»")


if __name__ == "__main__":
    main()
