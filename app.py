"""
æ–°èç€è¦½ & è‚¡ç¥¨æ•¸æ“š Streamlit æ‡‰ç”¨ç¨‹å¼

æ”¯æ´å¤šç¨®è³‡æ–™åº«å¾Œç«¯ï¼š
- SQLite (DB_TYPE=sqlite)
- PostgreSQL (DB_TYPE=postgresql)
- Supabase (DB_TYPE=supabase)
"""

import sqlite3
import os
from datetime import datetime, date, timedelta
from pathlib import Path
from collections import defaultdict

import pandas as pd
import numpy as np
import streamlit as st
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# åŠ å…¥åˆ†ææ¨¡çµ„
import sys
sys.path.insert(0, str(Path(__file__).parent))
from src.finance.analyzer import TechnicalAnalyzer
from src.finance.portfolio_strategy import PortfolioStrategy
from src.finance.macro_database import MacroDatabase
from src.finance.cycle_analyzer import MarketCycleAnalyzer
from src.finance.cycle_strategy import CycleBasedStrategySelector
from src.finance.cycle_backtest import CycleBacktester
from src.finance.sentiment_backtest import SentimentBacktester, DailyHotStocksAnalyzer

# ==================== è³‡æ–™å±¤åˆå§‹åŒ– ====================
# ä½¿ç”¨çµ±ä¸€çš„è³‡æ–™æŠ½è±¡å±¤ï¼Œé€é DB_TYPE ç’°å¢ƒè®Šæ•¸é¸æ“‡å¾Œç«¯
from src.data import get_client, get_client_info

# å»¶é²åˆå§‹åŒ–è³‡æ–™å®¢æˆ¶ç«¯
DATA_CLIENT = None
DB_TYPE = os.getenv("DB_TYPE", "sqlite").lower()

# å‘å¾Œå…¼å®¹ï¼šUSE_SUPABASE æ¨™èªŒ
USE_SUPABASE = DB_TYPE == "supabase"
SUPABASE_CLIENT = None  # ä¸å†ç›´æ¥ä½¿ç”¨ï¼Œæ”¹ç”¨ DATA_CLIENT


def _get_data_client():
    """å–å¾—è³‡æ–™å®¢æˆ¶ç«¯ï¼ˆå»¶é²åˆå§‹åŒ–ï¼‰"""
    global DATA_CLIENT
    if DATA_CLIENT is None:
        DATA_CLIENT = get_client()
    return DATA_CLIENT

# ==================== Supabase å¿«å–å±¤ ====================
@st.cache_data(ttl=300)  # å¿«å– 5 åˆ†é˜
def _cached_supabase_news(date_str: str):
    """å¿«å–æ–°èæŸ¥è©¢ - ä½¿ç”¨ collected_at ä½œç‚ºä¸»è¦æ—¥æœŸç¯©é¸"""
    # å–å¾—ç•¶å¤©æ”¶é›†çš„æ–°è (é PTT)
    result1 = SUPABASE_CLIENT.table("news").select(
        "id, title, content, url, source, category, source_type, published_at, collected_at"
    ).neq("source_type", "ptt").gte("collected_at", f"{date_str}T00:00:00").lte(
        "collected_at", f"{date_str}T23:59:59"
    ).limit(500).execute()

    # å–å¾—ç•¶å¤©ç™¼å¸ƒçš„ PTT æ–°è
    result2 = SUPABASE_CLIENT.table("news").select(
        "id, title, content, url, source, category, source_type, published_at, collected_at"
    ).eq("source_type", "ptt").gte("published_at", f"{date_str}T00:00:00").lte(
        "published_at", f"{date_str}T23:59:59"
    ).limit(200).execute()

    # åˆä½µä¸¦æ’åº
    all_news = (result1.data or []) + (result2.data or [])
    all_news.sort(key=lambda x: x.get("collected_at") or x.get("published_at") or "", reverse=True)
    return all_news

@st.cache_data(ttl=300)
def _cached_supabase_weekly_news(start_str: str, end_str: str):
    """å¿«å–é€±æ–°èæŸ¥è©¢ - ä½¿ç”¨ collected_at ä½œç‚ºä¸»è¦æ—¥æœŸç¯©é¸"""
    # é PTT: ç”¨ collected_at
    result1 = SUPABASE_CLIENT.table("news").select(
        "id, title, content, url, source, category, source_type, published_at, collected_at"
    ).neq("source_type", "ptt").gte("collected_at", start_str).lte(
        "collected_at", f"{end_str}T23:59:59"
    ).limit(1500).execute()

    # PTT: ç”¨ published_at
    result2 = SUPABASE_CLIENT.table("news").select(
        "id, title, content, url, source, category, source_type, published_at, collected_at"
    ).eq("source_type", "ptt").gte("published_at", start_str).lte(
        "published_at", f"{end_str}T23:59:59"
    ).limit(500).execute()

    all_news = (result1.data or []) + (result2.data or [])
    all_news.sort(key=lambda x: x.get("collected_at") or x.get("published_at") or "", reverse=True)
    return all_news

@st.cache_data(ttl=600)  # å¿«å– 10 åˆ†é˜
def _cached_supabase_watchlist():
    """å¿«å–è‚¡ç¥¨æ¸…å–®"""
    result = SUPABASE_CLIENT.table("watchlist").select(
        "symbol, name, market, sector, industry"
    ).eq("is_active", True).order("market").order("symbol").execute()
    return result.data if result.data else []

@st.cache_data(ttl=300)
def _cached_supabase_prices(symbol: str, start_str: str, end_str: str):
    """å¿«å–è‚¡åƒ¹æŸ¥è©¢"""
    result = SUPABASE_CLIENT.table("daily_prices").select(
        "date, open, high, low, close, volume"
    ).eq("symbol", symbol).gte("date", start_str).lte("date", end_str).order("date").execute()
    return result.data if result.data else []

@st.cache_data(ttl=3600)  # å¿«å– 1 å°æ™‚
def _cached_supabase_available_dates():
    """å¿«å–å¯ç”¨æ—¥æœŸ - ä½¿ç”¨ collected_at"""
    end_date = date.today()
    start_date = end_date - timedelta(days=90)
    result = SUPABASE_CLIENT.table("news").select("collected_at").gte(
        "collected_at", start_date.isoformat()
    ).order("collected_at", desc=True).limit(5000).execute()
    dates_set = set()
    for r in result.data or []:
        if r.get("collected_at"):
            dates_set.add(r["collected_at"][:10])
    return sorted(dates_set, reverse=True)

# ==================== æ–°èç¯©é¸å™¨ ====================
# ç¤¾è«–/è©•è«–é—œéµå­— (æ¨™é¡Œä¸­å‡ºç¾é€™äº›è©æœƒè¢«éæ¿¾)
EDITORIAL_KEYWORDS = [
    # è‹±æ–‡
    "opinion", "editorial", "commentary", "column", "op-ed", "analysis:",
    "perspective", "viewpoint", "my view", "i think", "in my opinion",
    # ä¸­æ–‡
    "ç¤¾è«–", "è©•è«–", "å°ˆæ¬„", "è§€é»", "çœ‹æ³•", "æˆ‘èªç‚º", "å€‹äººè§€é»", "æ·ºè¦‹",
]

# ä¸å¯é ä¾†æº (é€™äº›ä¾†æºçš„æ–‡ç« æœƒè¢«éæ¿¾)
UNRELIABLE_SOURCES = [
    # å¯ä»¥æ ¹æ“šéœ€è¦æ·»åŠ 
]


def extract_ptt_push_count(content: str) -> int:
    """å¾ PTT å…§å®¹å­—ä¸²æå–æ¨æ–‡æ•¸

    æ ¼å¼: "[æ¨æ•¸] ä½œè€…: xxx" æˆ– "[çˆ†] ä½œè€…: xxx" æˆ– "[X1] ä½œè€…: xxx"
    """
    if not content:
        return 0

    try:
        # å–å¾— [] å…§çš„å…§å®¹
        if "]" in content:
            push_str = content.split("]")[0].replace("[", "").strip()

            # çˆ† = 100+ æ¨
            if "çˆ†" in push_str:
                return 100

            # X é–‹é ­ = è² æ¨ (å™“)
            if push_str.startswith("X"):
                return -1

            # ç´”æ•¸å­—
            if push_str.isdigit():
                return int(push_str)

            # ç©ºç™½æˆ–å…¶ä»–
            return 0
    except:
        return 0

    return 0


def is_editorial_content(news: dict) -> bool:
    """åˆ¤æ–·æ˜¯å¦ç‚ºç¤¾è«–/è©•è«–é¡å…§å®¹"""
    title = (news.get("title") or "").lower()
    source = (news.get("source") or "").lower()

    # æª¢æŸ¥ä¾†æº
    if source in [s.lower() for s in UNRELIABLE_SOURCES]:
        return True

    # æª¢æŸ¥æ¨™é¡Œé—œéµå­—
    for keyword in EDITORIAL_KEYWORDS:
        if keyword.lower() in title:
            return True

    return False


def filter_news(news_list: list, ptt_min_push: int = 30, exclude_editorial: bool = True) -> list:
    """éæ¿¾æ–°èåˆ—è¡¨

    Args:
        news_list: æ–°èåˆ—è¡¨
        ptt_min_push: PTT æœ€ä½æ¨æ–‡æ•¸ (é è¨­ 30)
        exclude_editorial: æ˜¯å¦æ’é™¤ç¤¾è«–/è©•è«– (é è¨­ True)

    Returns:
        éæ¿¾å¾Œçš„æ–°èåˆ—è¡¨
    """
    filtered = []

    for news in news_list:
        source_type = news.get("source_type") or ""

        # PTT æ–‡ç« ï¼šæª¢æŸ¥æ¨æ–‡æ•¸
        if source_type == "ptt":
            push_count = extract_ptt_push_count(news.get("content") or "")
            if push_count < ptt_min_push:
                continue

        # é PTT æ–‡ç« ï¼šæª¢æŸ¥æ˜¯å¦ç‚ºç¤¾è«–
        elif exclude_editorial:
            if is_editorial_content(news):
                continue

        filtered.append(news)

    return filtered


# é é¢è¨­å®š
st.set_page_config(
    page_title="è‚¡ç¥¨æ•¸æ“šèˆ‡æ–°èåˆ†æ",
    page_icon="ğŸ“ˆ",
    layout="wide",
)

# è³‡æ–™åº«è·¯å¾‘ (å„ªå…ˆä½¿ç”¨å®Œæ•´è³‡æ–™åº«ï¼Œè‹¥ä¸å­˜åœ¨å‰‡ä½¿ç”¨ç¤ºç¯„è³‡æ–™åº«)
_base_path = Path(__file__).parent
DB_PATH = _base_path / "news.db" if (_base_path / "news.db").exists() else _base_path / "demo_news.db"
FINANCE_DB_PATH = _base_path / "finance.db" if (_base_path / "finance.db").exists() else _base_path / "demo_finance.db"
DEMO_MODE = not USE_SUPABASE and "demo" in str(DB_PATH)

# æ–°èåˆ†é¡é—œéµå­—
MACRO_KEYWORDS = {
    "Fed/åˆ©ç‡": ["fed", "federal reserve", "interest rate", "rate cut", "rate hike", "monetary policy", "fomc"],
    "é€šè†¨": ["inflation", "cpi", "pce", "consumer price", "deflation"],
    "GDP/ç¶“æ¿Ÿæˆé•·": ["gdp", "economic growth", "recession", "economy grow", "economic expansion"],
    "å°±æ¥­": ["jobs", "unemployment", "employment", "labor market", "jobless", "payroll", "hiring"],
    "ç¾å…ƒ/åŒ¯ç‡": ["dollar", "currency", "forex", "exchange rate", "yen", "euro", "yuan"],
    "é»ƒé‡‘/é¿éšª": ["gold", "silver", "precious metal", "safe haven"],
    "å‚µåˆ¸/æ®–åˆ©ç‡": ["bond", "treasury", "yield", "10-year", "debt"],
    "è²¿æ˜“/é—œç¨…": ["tariff", "trade war", "trade deal", "import", "export", "trade policy"],
    "æ”¿åºœæ”¿ç­–": ["government shutdown", "fiscal", "stimulus", "budget", "congress", "white house"],
}

INDUSTRY_KEYWORDS = {
    "åŠå°é«”": [
        "semiconductor", "chip", "chipmaker", "foundry", "wafer",
        "nvidia", "nvda", "amd", "intel", "qualcomm", "broadcom", "texas instruments",
        "tsmc", "å°ç©é›»", "asml", "lam research", "applied materials",
        "micron", "sk hynix", "samsung semiconductor"
    ],
    "è»Ÿé«”/é›²ç«¯": [
        "software", "cloud", "saas", "paas", "iaas",
        "microsoft", "msft", "azure", "salesforce", "oracle", "sap",
        "adobe", "servicenow", "workday", "snowflake", "datadog",
        "crowdstrike", "palo alto", "fortinet", "zscaler"
    ],
    "ç¶²è·¯/ç¤¾ç¾¤": [
        "social media", "search", "advertising", "digital ad",
        "meta", "facebook", "instagram", "google", "alphabet", "youtube",
        "tiktok", "bytedance", "snap", "pinterest", "twitter", "x corp",
        "linkedin", "reddit"
    ],
    "ç¡¬é«”/æ¶ˆè²»é›»å­": [
        "hardware", "computer", "pc", "laptop", "smartphone", "phone",
        "apple", "aapl", "iphone", "ipad", "mac", "è˜‹æœ",
        "dell", "hp", "lenovo", "samsung electronics", "xiaomi",
        "server", "data center", "ä¼ºæœå™¨", "è³‡æ–™ä¸­å¿ƒ"
    ],
    "AIäººå·¥æ™ºæ…§": [
        "ai", "artificial intelligence", "machine learning", "deep learning",
        "generative ai", "genai", "chatgpt", "openai", "anthropic", "claude",
        "copilot", "gemini", "llm", "large language model",
        "palantir", "c3.ai", "ai chip", "ai server"
    ],
    "é‡‘è/éŠ€è¡Œ": ["bank", "financial", "jpmorgan", "goldman", "morgan stanley", "wells fargo", "credit", "lending"],
    "é†«ç™‚ä¿å¥": ["healthcare", "pharma", "biotech", "drug", "fda", "hospital", "unitedhealth", "humana", "medicare"],
    "é€šè¨Šæœå‹™": [
        "telecom", "verizon", "at&t", "t-mobile", "comcast", "charter",
        "netflix", "disney", "warner", "paramount", "spotify",
        "streaming", "media", "entertainment", "broadcast", "5g network",
        "ä¸­è¯é›»", "å°ç£å¤§", "é å‚³"
    ],
    "å·¥æ¥­": [
        "industrial", "manufacturing", "machinery", "caterpillar", "deere",
        "honeywell", "3m", "ge", "general electric", "siemens",
        "defense", "lockheed", "raytheon", "northrop", "aerospace",
        "construction", "infrastructure", "railroad", "union pacific"
    ],
    "å…¬ç”¨äº‹æ¥­": [
        "utility", "utilities", "electric utility", "power grid",
        "nextera", "duke energy", "southern company", "dominion",
        "water utility", "natural gas utility", "regulated utility",
        "å°é›»", "ä¸­æ²¹", "electricity", "power plant"
    ],
    "åŸºç¤ææ–™": [
        "materials", "mining", "copper", "aluminum", "lithium",
        "dow", "basf", "dupont", "linde",
        "gold mining", "iron ore", "commodity", "raw material",
        "fertilizer", "paper"
    ],
    "é‹¼éµ/çŸ³åŒ–/æ°´æ³¥": [
        "steel", "é‹¼éµ", "ä¸­é‹¼", "ä¸­é´»", "è±èˆˆ", "nucor", "us steel", "é‹¼åƒ¹",
        "petrochemical", "çŸ³åŒ–", "å°å¡‘", "å—äº", "å°åŒ–", "å°å¡‘åŒ–", "å¡‘åŒ–", "ä¹™çƒ¯", "pvc",
        "cement", "æ°´æ³¥", "å°æ³¥", "äºæ³¥", "ç‡Ÿå»ºææ–™"
    ],
    "æ±½è»Š": ["auto", "car", "ev", "electric vehicle", "tesla", "gm", "ford", "toyota", "byd"],
    "èƒ½æº": ["oil", "gas", "energy", "crude", "opec", "renewable", "solar", "wind", "petroleum"],
    "é›¶å”®/æ¶ˆè²»": ["retail", "consumer", "walmart", "amazon", "target", "costco", "spending", "e-commerce"],
    "èˆªç©º/é‹è¼¸": ["airline", "aviation", "boeing", "airbus", "ups", "fedex", "shipping", "logistics"],
    "æˆ¿åœ°ç”¢": ["real estate", "housing", "mortgage", "home price", "property", "reit"],
    "åŠ å¯†è²¨å¹£": ["crypto", "bitcoin", "ethereum", "blockchain", "defi", "web3", "btc", "eth"],
}

# ç§‘æŠ€ç”¢æ¥­éˆé—œéµå­—
TECH_SUPPLY_CHAIN_KEYWORDS = {
    "AIæ‡‰ç”¨/å¹³å°": [
        "genai", "ç”Ÿæˆå¼ai", "å¤§å‹èªè¨€æ¨¡å‹", "llm", "æ©Ÿå™¨å­¸ç¿’", "machine learning",
        "copilot", "chatgpt", "anthropic", "openai", "claude", "gemini", "grok",
        "ai agent", "aiåŠ©ç†", "è‡ªå‹•é§•é§›", "autonomous", "computer vision",
        "palantir", "pltr", "c3.ai", "soundhound", "bigbear"
    ],
    "SaaS/é›²æœå‹™": [
        "saas", "paas", "iaas", "cloud service", "é›²æœå‹™", "è¨‚é–±",
        "aws", "amazon web services", "azure", "gcp", "google cloud",
        "salesforce", "crm", "servicenow", "now", "workday", "wday",
        "snowflake", "snow", "datadog", "ddog", "mongodb", "mdb",
        "crowdstrike", "crwd", "zscaler", "okta", "twilio", "hubspot"
    ],
    "ç§‘æŠ€å·¨é ­": [
        "microsoft", "msft", "å¾®è»Ÿ", "meta", "facebook", "è‡‰æ›¸",
        "alphabet", "google", "googl", "goog", "è°·æ­Œ",
        "amazon", "amzn", "äºé¦¬éœ", "apple", "aapl", "è˜‹æœ",
        "nvidia", "nvda", "è¼é”", "tesla", "tsla", "ç‰¹æ–¯æ‹‰",
        "magnificent seven", "mag7", "ä¸ƒå·¨é ­", "ç§‘æŠ€ä¸ƒé›„"
    ],
    "AIåŸºç¤è¨­æ–½": [
        "ai chip", "aiæ™¶ç‰‡", "gpu", "data center", "è³‡æ–™ä¸­å¿ƒ",
        "nvidia", "h100", "h200", "b100", "b200", "blackwell", "hopper",
        "ai server", "aiä¼ºæœå™¨", "æ¶²å†·", "liquid cooling",
        "é«˜é€Ÿé‹ç®—", "hpc", "è¶…ç´šé›»è…¦", "supercomputer"
    ],
    "æ™¶åœ“ä»£å·¥": [
        "foundry", "æ™¶åœ“ä»£å·¥", "å°ç©é›»", "tsmc", "2330", "ä¸‰æ˜Ÿæ™¶åœ“", "samsung foundry",
        "å…ˆé€²è£½ç¨‹", "3nm", "2nm", "5nm", "7nm", "è£½ç¨‹", "wafer", "æ™¶åœ“å» ",
        "è¯é›»", "umc", "2303", "æ ¼èŠ¯", "globalfoundries"
    ],
    "ICè¨­è¨ˆ": [
        "ic design", "fabless", "è¯ç™¼ç§‘", "mediatek", "2454", "é«˜é€š", "qualcomm",
        "åšé€š", "broadcom", "amd", "intel", "marvell", "ç‘æ˜±", "2379",
        "è¯è© ", "3034", "novatek", "é©…å‹•ic", "é›»æºç®¡ç†ic", "pmic"
    ],
    "è¨˜æ†¶é«”": [
        "dram", "nand", "memory", "è¨˜æ†¶é«”", "hbm", "é«˜é »å¯¬è¨˜æ†¶é«”",
        "ä¸‰æ˜Ÿ", "samsung memory", "sk hynix", "æµ·åŠ›å£«", "ç¾å…‰", "micron",
        "å—äºç§‘", "2408", "è¯é‚¦é›»", "2344", "æ—ºå®", "2337"
    ],
    "å°æ¸¬": [
        "packaging", "å°è£", "æ¸¬è©¦", "osat", "æ—¥æœˆå…‰", "aseh", "3711",
        "çŸ½å“", "spil", "åŠ›æˆ", "6239", "äº¬å…ƒé›»", "2449",
        "å…ˆé€²å°è£", "cowos", "chiplet", "2.5d", "3då°è£"
    ],
    "PCB/è¼‰æ¿": [
        "pcb", "é›»è·¯æ¿", "è¼‰æ¿", "substrate", "abfè¼‰æ¿",
        "æ¬£èˆˆ", "3037", "å—é›»", "8046", "æ™¯ç¢©", "3189",
        "è‡»é¼", "å¥é¼", "è¯é€š", "2313"
    ],
    "é¢æ¿/é¡¯ç¤º": [
        "panel", "display", "é¢æ¿", "lcd", "oled", "mini led", "micro led",
        "å‹é”", "2409", "ç¾¤å‰µ", "3481", "å½©æ™¶", "6116",
        "äº¬æ±æ–¹", "boe", "lg display", "ä¸‰æ˜Ÿé¡¯ç¤º"
    ],
    "è¢«å‹•å…ƒä»¶": [
        "mlcc", "passive", "è¢«å‹•å…ƒä»¶", "é›»å®¹", "é›»é˜»", "é›»æ„Ÿ",
        "åœ‹å·¨", "2327", "è¯æ–°ç§‘", "2492", "æ‘ç”°", "murata",
        "tdk", "yageo"
    ],
    "ç¶²é€šè¨­å‚™": [
        "networking", "ç¶²é€š", "äº¤æ›å™¨", "switch", "router", "è·¯ç”±å™¨",
        "æ€ç§‘", "cisco", "arista", "æ™ºé‚¦", "2345", "å•Ÿç¢", "6285",
        "ä¸­ç£Š", "5388", "wifi", "5g", "å…‰é€šè¨Š", "optical"
    ],
    "ä¼ºæœå™¨/ODM": [
        "server", "ä¼ºæœå™¨", "odm", "oem", "ç™½ç‰Œ",
        "å»£é”", "2382", "ç·¯å‰µ", "3231", "è‹±æ¥­é”", "2356",
        "é´»æµ·", "2317", "foxconn", "å’Œç¢©", "4938",
        "supermicro", "æˆ´çˆ¾", "dell", "hpe"
    ],
    "æ¶ˆè²»é›»å­": [
        "smartphone", "æ‰‹æ©Ÿ", "ç­†é›»", "notebook", "pc", "å¹³æ¿", "tablet",
        "è˜‹æœ", "apple", "iphone", "mac", "ipad",
        "ä¸‰æ˜Ÿæ‰‹æ©Ÿ", "å°ç±³", "xiaomi", "oppo", "vivo"
    ],
    "åŠå°é«”è¨­å‚™": [
        "semiconductor equipment", "åŠå°é«”è¨­å‚™", "å…‰åˆ»æ©Ÿ", "lithography",
        "asml", "è‰¾å¸æ‘©çˆ¾", "æ‡‰æ", "applied materials", "amat",
        "lam research", "ç§‘æ—ç ”ç™¼", "tokyo electron", "æ±äº¬å¨åŠ›"
    ],
}

# æƒ…ç·’åˆ†æé—œéµå­—
POSITIVE_KEYWORDS = [
    "surge", "soar", "jump", "gain", "rise", "rally", "record high", "beat", "exceed",
    "growth", "profit", "boom", "optimis", "bullish", "upgrade", "strong", "recover",
    "success", "breakthrough", "expand", "increase", "positive", "better than expected",
    "outperform", "upbeat", "confident", "improve", "advance", "climb",
    # Fed/åˆ©ç‡ç›¸é—œ - é™æ¯å°è‚¡å¸‚æ˜¯åˆ©å¤š
    "rate cut", "cut rate", "cuts rate", "é™æ¯", "å¯¬é¬†", "dovish", "easing"
]

NEGATIVE_KEYWORDS = [
    "plunge", "crash", "fall", "drop", "decline", "slump", "tumble", "sink", "lose",
    "loss", "layoff", "fire", "recession", "crisis", "fear", "worry", "concern",
    "pessimis", "bearish", "downgrade", "weak", "miss", "disappoint", "warn", "threat",
    "risk", "uncertain", "volatile", "trouble", "struggle", "fail", "worse than expected",
    "shutdown", "default", "bankruptcy",
    # Fed/åˆ©ç‡ç›¸é—œ - å‡æ¯å°è‚¡å¸‚æ˜¯åˆ©ç©º
    "rate hike", "hike rate", "hikes rate", "å‡æ¯", "ç·Šç¸®", "hawkish", "tightening"
]


def analyze_sentiment(news_items: list) -> tuple:
    """
    åˆ†ææ–°èæƒ…ç·’ï¼Œå›å‚³ (ç‡ˆè™Ÿ, åˆ†æ•¸)
    ğŸŸ¢ æ­£é¢ | ğŸŸ¡ ä¸­æ€§ | ğŸ”´ è² é¢
    """
    if not news_items:
        return "ğŸŸ¡", 0

    positive_count = 0
    negative_count = 0

    for news in news_items:
        text = (news["title"] + " " + (news["content"] or "")).lower()

        for kw in POSITIVE_KEYWORDS:
            if kw in text:
                positive_count += 1

        for kw in NEGATIVE_KEYWORDS:
            if kw in text:
                negative_count += 1

    total = positive_count + negative_count
    if total == 0:
        return "ğŸŸ¡", 0

    score = (positive_count - negative_count) / total

    if score > 0.2:
        return "ğŸŸ¢", score
    elif score < -0.2:
        return "ğŸ”´", score
    else:
        return "ğŸŸ¡", score


def extract_price_movements(text: str) -> list:
    """å¾æ–‡å­—ä¸­æå–è‚¡åƒ¹æ¼²è·Œå¹…"""
    import re
    movements = []
    # åŒ¹é…å„ç¨®æ¼²è·Œå¹…æ ¼å¼: up 5%, down 3%, +5%, -3%, æ¼²5%, è·Œ3%
    patterns = [
        r'(up|rise|gain|jump|surge|soar|climb)\s*(\d+(?:\.\d+)?)\s*%',
        r'(down|fall|drop|decline|plunge|tumble|sink)\s*(\d+(?:\.\d+)?)\s*%',
        r'[+ï¼‹](\d+(?:\.\d+)?)\s*%',
        r'[-ï¼](\d+(?:\.\d+)?)\s*%',
        r'æ¼²\s*(\d+(?:\.\d+)?)\s*%',
        r'è·Œ\s*(\d+(?:\.\d+)?)\s*%',
        r'(\d+(?:\.\d+)?)\s*%\s*(higher|lower|up|down)',
    ]
    for pattern in patterns:
        matches = re.findall(pattern, text.lower())
        movements.extend(matches)
    return movements[:3]  # æœ€å¤šè¿”å›3å€‹


def extract_companies(text: str, category: str) -> list:
    """æ ¹æ“šåˆ†é¡æå–ç›¸é—œå…¬å¸åç¨±"""

    # å„ç”¢æ¥­é¡åˆ¥å°æ‡‰çš„å…¬å¸ï¼ˆåªé¡¯ç¤ºè©²ç”¢æ¥­ç›¸é—œå…¬å¸ï¼‰
    category_companies = {
        # ç”¢æ¥­æ¿å¡Š
        "åŠå°é«”": [
            ("nvidia", "NVIDIA"), ("nvda", "NVIDIA"), ("è¼é”", "NVIDIA"),
            ("å°ç©é›»", "å°ç©é›»"), ("tsmc", "å°ç©é›»"),
            ("è¯ç™¼ç§‘", "è¯ç™¼ç§‘"), ("mediatek", "è¯ç™¼ç§‘"),
            ("amd", "AMD"), ("intel", "Intel"), ("qualcomm", "é«˜é€š"),
            ("broadcom", "Broadcom"), ("åšé€š", "Broadcom"),
            ("asml", "ASML"), ("è‰¾å¸æ‘©çˆ¾", "ASML"),
            ("micron", "Micron"), ("ç¾å…‰", "Micron"),
            ("sk hynix", "SKæµ·åŠ›å£«"), ("æµ·åŠ›å£«", "SKæµ·åŠ›å£«"),
            ("samsung", "ä¸‰æ˜Ÿ"), ("ä¸‰æ˜Ÿ", "ä¸‰æ˜Ÿ"),
        ],
        "è»Ÿé«”/é›²ç«¯": [
            ("microsoft", "Microsoft"), ("msft", "Microsoft"), ("å¾®è»Ÿ", "Microsoft"),
            ("salesforce", "Salesforce"), ("snowflake", "Snowflake"),
            ("servicenow", "ServiceNow"), ("crowdstrike", "CrowdStrike"),
            ("datadog", "Datadog"), ("mongodb", "MongoDB"),
            ("adobe", "Adobe"), ("oracle", "Oracle"),
        ],
        "ç¶²è·¯/ç¤¾ç¾¤": [
            ("meta", "Meta"), ("facebook", "Meta"),
            ("alphabet", "Google"), ("googl", "Google"), ("google", "Google"),
            ("netflix", "Netflix"), ("spotify", "Spotify"),
            ("snap", "Snap"), ("pinterest", "Pinterest"),
        ],
        "ç¡¬é«”/æ¶ˆè²»é›»å­": [
            ("apple", "Apple"), ("aapl", "Apple"), ("è˜‹æœ", "Apple"),
            ("samsung", "ä¸‰æ˜Ÿ"), ("ä¸‰æ˜Ÿ", "ä¸‰æ˜Ÿ"),
            ("sony", "Sony"), ("lg", "LG"),
            ("é´»æµ·", "é´»æµ·"), ("foxconn", "é´»æµ·"),
            ("å’Œç¢©", "å’Œç¢©"), ("pegatron", "å’Œç¢©"),
        ],
        "AIäººå·¥æ™ºæ…§": [
            ("nvidia", "NVIDIA"), ("nvda", "NVIDIA"), ("è¼é”", "NVIDIA"),
            ("openai", "OpenAI"), ("chatgpt", "OpenAI"), ("anthropic", "Anthropic"),
            ("microsoft", "Microsoft"), ("google", "Google"), ("meta", "Meta"),
            ("palantir", "Palantir"), ("pltr", "Palantir"),
        ],
        "é‡‘è": [
            ("jpmorgan", "JPMorgan"), ("jp morgan", "JPMorgan"),
            ("goldman sachs", "Goldman"), ("goldman", "Goldman"),
            ("morgan stanley", "Morgan Stanley"),
            ("bank of america", "ç¾éŠ€"), ("citigroup", "èŠ±æ——"),
            ("berkshire", "Berkshire"), ("visa", "Visa"), ("mastercard", "Mastercard"),
        ],
        "é†«ç™‚ä¿å¥": [
            ("unitedhealth", "UnitedHealth"), ("pfizer", "è¼ç‘"),
            ("eli lilly", "ç¦®ä¾†"), ("novo nordisk", "è«¾å’Œè«¾å¾·"),
            ("johnson & johnson", "J&J"), ("merck", "é»˜å…‹"),
            ("abbvie", "AbbVie"), ("moderna", "Moderna"),
        ],
        "èƒ½æº": [
            ("exxon", "Exxon"), ("chevron", "Chevron"),
            ("conocophillips", "ConocoPhillips"), ("schlumberger", "Schlumberger"),
            ("å°å¡‘åŒ–", "å°å¡‘åŒ–"), ("ä¸­æ²¹", "ä¸­æ²¹"),
        ],
        "æ±½è»Š": [
            ("tesla", "Tesla"), ("tsla", "Tesla"), ("ç‰¹æ–¯æ‹‰", "Tesla"),
            ("gm", "GM"), ("ford", "Ford"), ("toyota", "è±ç”°"),
            ("byd", "æ¯”äºè¿ª"), ("rivian", "Rivian"), ("lucid", "Lucid"),
        ],
        "é›¶å”®/æ¶ˆè²»": [
            ("walmart", "Walmart"), ("amazon", "Amazon"), ("amzn", "Amazon"),
            ("costco", "Costco"), ("target", "Target"),
            ("home depot", "Home Depot"), ("starbucks", "Starbucks"),
        ],
        "èˆªç©º/é‹è¼¸": [
            ("boeing", "Boeing"), ("airbus", "Airbus"),
            ("ups", "UPS"), ("fedex", "FedEx"),
            ("delta", "Delta"), ("united airlines", "United"),
            ("é•·æ¦®èˆª", "é•·æ¦®èˆª"), ("è¯èˆª", "è¯èˆª"),
        ],
        "é€šè¨Šæœå‹™": [
            ("verizon", "Verizon"), ("at&t", "AT&T"), ("t-mobile", "T-Mobile"),
            ("comcast", "Comcast"), ("disney", "Disney"),
            ("ä¸­è¯é›»", "ä¸­è¯é›»"), ("å°ç£å¤§", "å°ç£å¤§"), ("é å‚³", "é å‚³"),
        ],
        "å·¥æ¥­": [
            ("caterpillar", "Caterpillar"), ("cat", "Caterpillar"),
            ("deere", "Deere"), ("john deere", "Deere"),
            ("honeywell", "Honeywell"), ("general electric", "GE"), ("ge", "GE"),
            ("siemens", "Siemens"), ("3m", "3M"),
            ("lockheed", "Lockheed"), ("raytheon", "Raytheon"), ("northrop", "Northrop"),
            ("union pacific", "Union Pacific"), ("ups", "UPS"),
        ],
        "å…¬ç”¨äº‹æ¥­": [
            ("nextera", "NextEra"), ("duke energy", "Duke Energy"),
            ("southern company", "Southern Co"), ("dominion", "Dominion"),
            ("å°é›»", "å°é›»"),
        ],
        "åŸºç¤ææ–™": [
            ("dow", "Dow"), ("basf", "BASF"), ("dupont", "DuPont"), ("linde", "Linde"),
            ("ä¸­é‹¼", "ä¸­é‹¼"), ("å°å¡‘", "å°å¡‘"), ("å—äº", "å—äº"),
            ("freeport", "Freeport"), ("newmont", "Newmont"),
            ("å°æ³¥", "å°æ³¥"), ("äºæ³¥", "äºæ³¥"),
        ],
        "é‹¼éµ/çŸ³åŒ–/æ°´æ³¥": [
            ("ä¸­é‹¼", "ä¸­é‹¼"), ("ä¸­é´»", "ä¸­é´»"), ("è±èˆˆ", "è±èˆˆ"),
            ("å°å¡‘", "å°å¡‘"), ("å—äº", "å—äº"), ("å°åŒ–", "å°åŒ–"), ("å°å¡‘åŒ–", "å°å¡‘åŒ–"),
            ("å°æ³¥", "å°æ³¥"), ("äºæ³¥", "äºæ³¥"),
            ("nucor", "Nucor"), ("us steel", "US Steel"),
        ],
        # ç§‘æŠ€ç”¢æ¥­éˆ
        "AIæ™¶ç‰‡": [
            ("nvidia", "NVIDIA"), ("nvda", "NVIDIA"), ("è¼é”", "NVIDIA"),
            ("amd", "AMD"), ("intel", "Intel"),
            ("google tpu", "Google TPU"), ("amazon trainium", "AWS"),
        ],
        "è¨˜æ†¶é«”": [
            ("micron", "Micron"), ("ç¾å…‰", "Micron"),
            ("sk hynix", "SKæµ·åŠ›å£«"), ("æµ·åŠ›å£«", "SKæµ·åŠ›å£«"),
            ("samsung", "ä¸‰æ˜Ÿ"), ("å—äºç§‘", "å—äºç§‘"),
        ],
        "æ™¶åœ“ä»£å·¥": [
            ("å°ç©é›»", "å°ç©é›»"), ("tsmc", "å°ç©é›»"),
            ("globalfoundries", "GlobalFoundries"), ("è¯é›»", "è¯é›»"),
            ("samsung foundry", "ä¸‰æ˜Ÿ"),
        ],
        "å°æ¸¬": [
            ("æ—¥æœˆå…‰", "æ—¥æœˆå…‰"), ("ase", "æ—¥æœˆå…‰"),
            ("çŸ½å“", "çŸ½å“"), ("äº¬å…ƒé›»", "äº¬å…ƒé›»"),
        ],
        "ICè¨­è¨ˆ": [
            ("è¯ç™¼ç§‘", "è¯ç™¼ç§‘"), ("mediatek", "è¯ç™¼ç§‘"),
            ("ç‘æ˜±", "ç‘æ˜±"), ("è¯è© ", "è¯è© "), ("novatek", "è¯è© "),
            ("é«˜é€š", "é«˜é€š"), ("qualcomm", "é«˜é€š"), ("broadcom", "Broadcom"),
        ],
        "ä¼ºæœå™¨/è³‡æ–™ä¸­å¿ƒ": [
            ("supermicro", "Supermicro"), ("smci", "Supermicro"),
            ("å»£é”", "å»£é”"), ("quanta", "å»£é”"),
            ("ç·¯å‰µ", "ç·¯å‰µ"), ("wistron", "ç·¯å‰µ"),
            ("ç·¯ç©", "ç·¯ç©"), ("è‹±æ¥­é”", "è‹±æ¥­é”"),
            ("dell", "Dell"), ("hpe", "HPE"),
        ],
        "ç¶²é€šè¨­å‚™": [
            ("cisco", "Cisco"), ("arista", "Arista"),
            ("juniper", "Juniper"), ("æ™ºé‚¦", "æ™ºé‚¦"),
        ],
        "PCB/æ•£ç†±": [
            ("å°éƒ¡", "å°éƒ¡"), ("æ¬£èˆˆ", "æ¬£èˆˆ"), ("å—é›»", "å—é›»"),
            ("å¥‡é‹", "å¥‡é‹"), ("é›™é´»", "é›™é´»"),
        ],
        "é›»æºä¾›æ‡‰": [
            ("å°é”é›»", "å°é”é›»"), ("delta", "å°é”é›»"),
            ("å…‰å¯¶", "å…‰å¯¶"), ("ç¾¤å…‰", "ç¾¤å…‰"),
        ],
        "é¢æ¿/é¡¯ç¤º": [
            ("å‹é”", "å‹é”"), ("auo", "å‹é”"),
            ("ç¾¤å‰µ", "ç¾¤å‰µ"), ("innolux", "ç¾¤å‰µ"),
            ("lg display", "LG Display"),
        ],
        "æ‰‹æ©Ÿä¾›æ‡‰éˆ": [
            ("é´»æµ·", "é´»æµ·"), ("foxconn", "é´»æµ·"),
            ("å’Œç¢©", "å’Œç¢©"), ("pegatron", "å’Œç¢©"),
            ("å¤§ç«‹å…‰", "å¤§ç«‹å…‰"), ("ç‰æ™¶å…‰", "ç‰æ™¶å…‰"),
        ],
        "AIæ‡‰ç”¨/å¹³å°": [
            ("openai", "OpenAI"), ("anthropic", "Anthropic"),
            ("palantir", "Palantir"), ("c3.ai", "C3.ai"),
        ],
        "SaaS/é›²æœå‹™": [
            ("salesforce", "Salesforce"), ("snowflake", "Snowflake"),
            ("servicenow", "ServiceNow"), ("workday", "Workday"),
            ("datadog", "Datadog"), ("mongodb", "MongoDB"),
        ],
        "ç§‘æŠ€å·¨é ­": [
            ("microsoft", "Microsoft"), ("msft", "Microsoft"), ("å¾®è»Ÿ", "Microsoft"),
            ("meta", "Meta"), ("facebook", "Meta"),
            ("alphabet", "Google"), ("googl", "Google"), ("google", "Google"),
            ("amazon", "Amazon"), ("amzn", "Amazon"), ("äºé¦¬éœ", "Amazon"),
            ("apple", "Apple"), ("aapl", "Apple"), ("è˜‹æœ", "Apple"),
            ("nvidia", "NVIDIA"), ("nvda", "NVIDIA"), ("è¼é”", "NVIDIA"),
            ("tesla", "Tesla"), ("tsla", "Tesla"), ("ç‰¹æ–¯æ‹‰", "Tesla"),
        ],
        "AIåŸºç¤è¨­æ–½": [
            ("nvidia", "NVIDIA"), ("nvda", "NVIDIA"),
            ("supermicro", "Supermicro"), ("smci", "Supermicro"),
            ("å»£é”", "å»£é”"), ("ç·¯å‰µ", "ç·¯å‰µ"),
            ("arista", "Arista"), ("vertiv", "Vertiv"),
        ],
    }

    # å–å¾—è©²é¡åˆ¥çš„å…¬å¸åˆ—è¡¨ï¼Œå¦‚æœæ²’æœ‰å‰‡ä½¿ç”¨é€šç”¨åˆ—è¡¨
    company_patterns = category_companies.get(category, [])

    # å¦‚æœé¡åˆ¥æ²’æœ‰ç‰¹å®šå…¬å¸åˆ—è¡¨ï¼Œä¸æå–å…¬å¸åç¨±
    if not company_patterns:
        return []

    text_lower = text.lower()
    companies_found = []
    seen = set()
    for pattern, company in company_patterns:
        if pattern in text_lower and company not in seen:
            companies_found.append(company)
            seen.add(company)
            if len(companies_found) >= 3:
                break

    return companies_found


def extract_key_event(news_items: list) -> str:
    """å¾æ–°èä¸­æå–é—œéµäº‹ä»¶"""
    # äº‹ä»¶é—œéµå­—ï¼ˆæŒ‰å„ªå…ˆé †åºæ’åˆ—ï¼‰
    event_keywords = [
        # é‡å¤§äº‹ä»¶å„ªå…ˆ
        ("layoff", "è£å“¡"), ("cut job", "è£å“¡"), ("job cut", "è£å“¡"),
        ("plunge", "æš´è·Œ"), ("crash", "å´©ç›¤"), ("surge", "å¤§æ¼²"), ("soar", "é£†æ¼²"),
        ("record high", "å‰µæ–°é«˜"), ("all-time high", "æ­·å²æ–°é«˜"),
        # è²¡å ±ç›¸é—œ
        ("earnings", "è²¡å ±"), ("quarterly", "å­£å ±"), ("revenue", "ç‡Ÿæ”¶"),
        ("profit", "ç²åˆ©"), ("guidance", "è²¡æ¸¬"),
        ("beat", "å„ªæ–¼é æœŸ"), ("miss", "ä¸å¦‚é æœŸ"), ("disappoint", "ä»¤äººå¤±æœ›"),
        # å…¬å¸å‹•æ…‹
        ("acquire", "æ”¶è³¼"), ("merger", "åˆä½µ"), ("buyout", "ä½µè³¼"),
        ("ipo", "IPO"), ("split", "åˆ†æ‹†"),
        ("launch", "ç™¼å¸ƒæ–°å“"), ("unveil", "ç™¼è¡¨"), ("announce", "å®£å¸ƒ"),
        ("partnership", "åˆä½œ"), ("contract", "ç²å¾—åˆç´„"),
        # è©•ç´šè®Šå‹•
        ("upgrade", "ä¸Šèª¿è©•ç´š"), ("downgrade", "ä¸‹èª¿è©•ç´š"),
        ("price target", "ç›®æ¨™åƒ¹èª¿æ•´"),
        # AI/ç§‘æŠ€ç›¸é—œ
        ("ai spending", "AIæ”¯å‡º"), ("capex", "è³‡æœ¬æ”¯å‡º"),
        ("chip", "æ™¶ç‰‡"), ("semiconductor", "åŠå°é«”"),
        # æ”¿ç­–/ç›£ç®¡
        ("fda approv", "FDAæ ¸å‡†"), ("antitrust", "åå£Ÿæ–·"),
        ("tariff", "é—œç¨…"), ("sanction", "åˆ¶è£"), ("ban", "ç¦ä»¤"),
        # ç¶“æ¿Ÿç›¸é—œ
        ("rate cut", "é™æ¯"), ("rate hike", "å‡æ¯"),
        ("inflation", "é€šè†¨"), ("recession", "è¡°é€€"),
    ]

    for news in news_items[:5]:  # æª¢æŸ¥å‰5å‰‡
        text = (news["title"] + " " + (news["content"] or "")).lower()
        for keyword, event in event_keywords:
            if keyword in text:
                return event
    return ""


def generate_summary(category: str, news_items: list, sentiment: str) -> str:
    """æ ¹æ“šæ–°èå…§å®¹ç”Ÿæˆä¸€å¥è©±ç¸½çµï¼ŒåŒ…å«å…¬å¸åç¨±ã€äº‹ä»¶å’Œæ¼²è·Œå¹…"""
    if not news_items:
        return "ä»Šæ—¥ç„¡ç›¸é—œæ–°è"

    # å–å¾—æœ€é‡è¦çš„æ–°èæ¨™é¡Œï¼ˆç¬¬ä¸€å‰‡ï¼‰
    top_news = news_items[0]["title"]

    # åˆä½µæ‰€æœ‰æ–°èæ–‡å­—
    text_all = " ".join([(n["title"] + " " + (n["content"] or "")).lower() for n in news_items])

    # ç¸½ç¶“é¡åˆ¥ - ä¸é¡¯ç¤ºå…¬å¸åç¨±ï¼Œç›´æ¥ä½¿ç”¨æ¨¡æ¿
    MACRO_CATEGORIES = [
        "Fed/åˆ©ç‡", "é€šè†¨", "GDP/ç¶“æ¿Ÿæˆé•·", "å°±æ¥­", "ç¾å…ƒ/åŒ¯ç‡",
        "é»ƒé‡‘/é¿éšª", "å‚µåˆ¸/æ®–åˆ©ç‡", "è²¿æ˜“/é—œç¨…", "æ”¿åºœæ”¿ç­–"
    ]

    # å¦‚æœæ˜¯ç¸½ç¶“é¡åˆ¥ï¼Œè·³éå…¬å¸æå–ï¼Œç›´æ¥é€²å…¥æ¨¡æ¿é‚è¼¯
    if category not in MACRO_CATEGORIES:
        # æå–å…¬å¸åç¨±ï¼ˆåƒ…é™ç”¢æ¥­å’Œç§‘æŠ€ç”¢æ¥­éˆé¡åˆ¥ï¼‰
        companies = extract_companies(text_all, category)

        # æå–é—œéµäº‹ä»¶
        event = extract_key_event(news_items)

        # çµ„åˆç¸½çµ
        company_str = "ã€".join(companies[:2]) if companies else ""

        # åˆ¤æ–·æ¼²è·Œæ–¹å‘
        up_keywords = ["up", "rise", "gain", "jump", "surge", "soar", "climb", "higher", "æ¼²"]
        down_keywords = ["down", "fall", "drop", "decline", "plunge", "tumble", "sink", "lower", "è·Œ"]

        is_up = any(kw in text_all for kw in up_keywords)
        is_down = any(kw in text_all for kw in down_keywords)

        # ç”Ÿæˆæ™ºèƒ½ç¸½çµï¼ˆåƒ…ç”¢æ¥­é¡åˆ¥ï¼‰
        if company_str and event:
            if is_up and not is_down:
                return f"{company_str}ï¼š{event}ï¼Œè‚¡åƒ¹èµ°æš"
            elif is_down and not is_up:
                return f"{company_str}ï¼š{event}ï¼Œè‚¡åƒ¹æ‰¿å£“"
            else:
                return f"{company_str}ï¼š{event}"
        elif company_str:
            if is_up and not is_down:
                return f"{company_str} ç›¸é—œæ¶ˆæ¯æ­£é¢ï¼Œè‚¡åƒ¹ä¸Šæ¼²"
            elif is_down and not is_up:
                return f"{company_str} é¢è‡¨å£“åŠ›ï¼Œè‚¡åƒ¹ä¸‹è·Œ"
            else:
                return f"{company_str} å‹•æ…‹å—é—œæ³¨"
        elif event:
            return f"ç”¢æ¥­{event}æ¶ˆæ¯ï¼Œå½±éŸ¿å¸‚å ´æƒ…ç·’"

    # ç¸½ç¶“é¡åˆ¥ä½¿ç”¨å°ˆå±¬æ¨¡æ¿ - å€åˆ†ã€Œå·²å®£å¸ƒã€vsã€Œé æœŸã€

    # åˆ¤æ–·æ˜¯å¦ç‚ºå·²ç¢ºèªäº‹ä»¶ï¼ˆä½¿ç”¨éå»å¼æˆ–ç¢ºèªæ€§å‹•è©ï¼‰
    announced_words = ["holds", "held", "keeps", "kept", "announces", "announced",
                       "decides", "decided", "maintains", "maintained", "unchanged"]
    is_announced = any(w in text_all for w in announced_words)

    # Fed/åˆ©ç‡
    if category == "Fed/åˆ©ç‡":
        # åˆ©ç‡ç¶­æŒä¸è®Š
        if any(w in text_all for w in ["hold", "steady", "unchanged", "pause"]):
            if is_announced:
                # è£œå……ï¼šPowell ç¹¼ä»»è€…ç›¸é—œæ–°è
                if "successor" in text_all or "replace" in text_all or "candidate" in text_all:
                    return "Fed å®£å¸ƒç¶­æŒåˆ©ç‡ä¸è®Šï¼›å¸‚å ´é—œæ³¨ Powell ç¹¼ä»»è€…äººé¸"
                return "Fed å®£å¸ƒç¶­æŒåˆ©ç‡ä¸è®Šï¼Œæš«åœé™æ¯æ­¥èª¿"
            else:
                return "å¸‚å ´é æœŸ Fed å°‡ç¶­æŒåˆ©ç‡ä¸è®Š"
        # é™æ¯
        elif "cut" in text_all:
            if is_announced or "cuts" in text_all:
                return "Fed å®£å¸ƒé™æ¯ï¼Œå¯¬é¬†æ”¿ç­–å•Ÿå‹•"
            else:
                return "å¸‚å ´é æœŸ Fed å°‡é™æ¯ï¼Œé¢¨éšªè³‡ç”¢å¯èƒ½å—æƒ "
        # å‡æ¯
        elif "hike" in text_all or "raise" in text_all:
            if is_announced:
                return "Fed å®£å¸ƒå‡æ¯ï¼Œç·Šç¸®æ”¿ç­–å»¶çºŒ"
            else:
                return "å‡æ¯é æœŸå‡æº«ï¼Œå‚µåˆ¸æ®–åˆ©ç‡èµ°é«˜"
        else:
            return "Fed æ”¿ç­–å‹•æ…‹ï¼ŒæŒçºŒé—œæ³¨åˆ©ç‡èµ°å‘"

    # é€šè†¨
    elif category == "é€šè†¨":
        if "ease" in text_all or "cool" in text_all or "slow" in text_all or "fell" in text_all:
            return "é€šè†¨æ•¸æ“šé™æº«ï¼Œæœ‰åˆ©æ–¼å¯¬é¬†æ”¿ç­–é æœŸ"
        elif "rise" in text_all or "surge" in text_all or "hot" in text_all or "sticky" in text_all:
            return "é€šè†¨å£“åŠ›ä»å­˜ï¼Œå¯èƒ½å»¶å¾Œé™æ¯æ™‚ç¨‹"
        elif "cpi" in text_all or "pce" in text_all:
            return "é€šè†¨æ•¸æ“šå…¬å¸ƒï¼Œé—œæ³¨ç‰©åƒ¹è¶¨å‹¢"
        else:
            return "é€šè†¨ç›¸é—œæ¶ˆæ¯ï¼Œè§€å¯Ÿç‰©åƒ¹èµ°å‹¢"

    # å°±æ¥­
    elif category == "å°±æ¥­":
        if "strong" in text_all or "beats" in text_all or "added" in text_all:
            return "å°±æ¥­æ•¸æ“šå¼·å‹ï¼Œå‹å‹•å¸‚å ´ä»å…·éŸŒæ€§"
        elif "layoff" in text_all or "layoffs" in text_all:
            return "ä¼æ¥­è£å“¡æ¶ˆæ¯é »å‚³ï¼Œå°±æ¥­å¸‚å ´é¢è‡¨å£“åŠ›"
        elif "jobless" in text_all or "unemployment" in text_all:
            if "rise" in text_all or "higher" in text_all:
                return "å¤±æ¥­ç‡ä¸Šå‡ï¼Œå°±æ¥­å¸‚å ´é™æº«"
            elif "fall" in text_all or "low" in text_all:
                return "å¤±æ¥­ç‡ç¶­æŒä½æª”ï¼Œç¶“æ¿ŸåŸºæœ¬é¢ç©©å¥"
        else:
            return "å°±æ¥­å¸‚å ´æ¶ˆæ¯ï¼Œç•™æ„å‹å‹•æ•¸æ“š"

    # ç¾å…ƒ/åŒ¯ç‡
    elif category == "ç¾å…ƒ/åŒ¯ç‡":
        if "weak" in text_all or "fall" in text_all or "drop" in text_all or "slip" in text_all:
            return "ç¾å…ƒèµ°å¼±ï¼Œæ–°èˆˆå¸‚å ´èˆ‡å¤§å®—å•†å“å—æƒ "
        elif "strong" in text_all or "rise" in text_all or "surge" in text_all:
            return "ç¾å…ƒèµ°å¼·ï¼Œå‡ºå£ä¼æ¥­èˆ‡æ–°èˆˆå¸‚å ´æ‰¿å£“"
        elif "intervention" in text_all:
            return "åŒ¯å¸‚å¹²é æ¶ˆæ¯ï¼Œæ³¢å‹•åŠ åŠ‡"
        else:
            return "åŒ¯ç‡å¸‚å ´æ³¢å‹•ï¼Œé—œæ³¨ç¾å…ƒèµ°å‹¢"

    # é»ƒé‡‘/é¿éšª
    elif category == "é»ƒé‡‘/é¿éšª":
        if "record" in text_all or "all-time" in text_all:
            return "é»ƒé‡‘å‰µæ­·å²æ–°é«˜ï¼Œé¿éšªéœ€æ±‚å¼·å‹"
        elif "surge" in text_all or "jump" in text_all or "rally" in text_all:
            return "é»ƒé‡‘å¤§æ¼²ï¼Œé¿éšªæƒ…ç·’å‡æº«"
        elif "fall" in text_all or "drop" in text_all or "retreat" in text_all:
            return "é»ƒé‡‘å›è½ï¼Œé¢¨éšªåå¥½å›å‡"
        else:
            return "è²´é‡‘å±¬å¸‚å ´æ³¢å‹•ï¼Œè§€å¯Ÿé¿éšªæƒ…ç·’"

    # è²¿æ˜“/é—œç¨…
    elif category == "è²¿æ˜“/é—œç¨…":
        if "tariff" in text_all:
            if "impose" in text_all or "announces" in text_all or "slaps" in text_all:
                return "é—œç¨…æ”¿ç­–å¯¦æ–½ï¼Œè²¿æ˜“æ‘©æ“¦å‡ç´š"
            elif "threat" in text_all or "warns" in text_all or "considers" in text_all:
                return "é—œç¨…å¨è„…å‡æº«ï¼Œå¸‚å ´é—œæ³¨å¾ŒçºŒç™¼å±•"
            elif "delay" in text_all or "pause" in text_all:
                return "é—œç¨…æš«ç·©ï¼Œå¸‚å ´é¬†ä¸€å£æ°£"
        elif "deal" in text_all or "agreement" in text_all:
            return "è²¿æ˜“å”è­°é€²å±•ï¼Œå¸‚å ´æƒ…ç·’æ”¹å–„"
        else:
            return "è²¿æ˜“æ”¿ç­–å‹•æ…‹ï¼Œç•™æ„é—œç¨…ç™¼å±•"

    # æ”¿åºœæ”¿ç­–
    elif category == "æ”¿åºœæ”¿ç­–":
        if "shutdown" in text_all:
            return "æ”¿åºœé—œé–€é¢¨éšªå‡é«˜ï¼Œå¸‚å ´ä¸ç¢ºå®šæ€§å¢åŠ "
        elif "stimulus" in text_all or "spending" in text_all:
            return "è²¡æ”¿åˆºæ¿€æ”¿ç­–å‹•å‘ï¼Œé—œæ³¨ç¶“æ¿Ÿå½±éŸ¿"
        elif "debt ceiling" in text_all or "debt limit" in text_all:
            return "å‚µå‹™ä¸Šé™è­°é¡Œå—é—œæ³¨ï¼Œå¸‚å ´è§€æœ›"
        else:
            return "æ”¿åºœæ”¿ç­–å‹•æ…‹ï¼Œé—œæ³¨è²¡æ”¿èµ°å‘"

    # å‚µåˆ¸
    elif category == "å‚µåˆ¸/æ®–åˆ©ç‡":
        if "invert" in text_all or "inverted" in text_all:
            return "æ®–åˆ©ç‡æ›²ç·šå€’æ›ï¼Œè¡°é€€æ“”æ†‚å‡æº«"
        elif "rise" in text_all or "surge" in text_all or "climb" in text_all or "jump" in text_all:
            return "æ®–åˆ©ç‡ä¸Šå‡ï¼Œå‚µåˆ¸åƒ¹æ ¼æ‰¿å£“"
        elif "fall" in text_all or "drop" in text_all or "retreat" in text_all:
            return "æ®–åˆ©ç‡ä¸‹æ»‘ï¼Œè³‡é‡‘æµå‘é¿éšªè³‡ç”¢"
        else:
            return "å‚µåˆ¸å¸‚å ´æ¶ˆæ¯ï¼Œç•™æ„æ®–åˆ©ç‡è®ŠåŒ–"

    # GDP/ç¶“æ¿Ÿæˆé•·
    elif category == "GDP/ç¶“æ¿Ÿæˆé•·":
        if "recession" in text_all or "contract" in text_all:
            return "ç¶“æ¿Ÿè¡°é€€ç–‘æ…®å‡æº«ï¼Œé˜²ç¦¦æ€§è³‡ç”¢å—é’ç"
        elif "growth" in text_all or "expand" in text_all:
            return "ç¶“æ¿Ÿæˆé•·ç©©å¥ï¼Œæ”¯æ’ä¼æ¥­ç²åˆ©é æœŸ"
        else:
            return "ç¶“æ¿Ÿæ•¸æ“šæ›´æ–°ï¼Œè§€å¯Ÿæˆé•·å‹•èƒ½"

    # ç§‘æŠ€/AI
    elif category == "ç§‘æŠ€/AI":
        if "spend" in text_all or "invest" in text_all:
            return "AI æŠ•è³‡ç†±æ½®æŒçºŒï¼Œç§‘æŠ€è‚¡å—é—œæ³¨"
        elif "layoff" in text_all or "cut" in text_all:
            return "ç§‘æŠ€æ¥­è£å“¡æ¶ˆæ¯é »å‚³ï¼Œæˆæœ¬æ§ç®¡ç‚ºé‡é»"
        elif "earn" in text_all:
            return "ç§‘æŠ€å·¨é ­è²¡å ±é€±ï¼ŒAI æ”¯å‡ºæˆç„¦é»"
        else:
            return "ç§‘æŠ€ç”¢æ¥­æ¶ˆæ¯ï¼Œé—œæ³¨ AI èˆ‡é›²ç«¯ç™¼å±•"

    # é†«ç™‚ä¿å¥
    elif category == "é†«ç™‚ä¿å¥":
        if "plunge" in text_all or "drop" in text_all or "fall" in text_all:
            return "é†«ç™‚è‚¡é‡æŒ«ï¼Œæ”¿ç­–é¢¨éšªè¡æ“Šä¼°å€¼"
        elif "fda" in text_all or "approv" in text_all:
            return "FDA å¯©æ‰¹å‹•æ…‹ï¼Œè—¥å» è‚¡åƒ¹æ³¢å‹•"
        else:
            return "é†«ç™‚ç”¢æ¥­æ¶ˆæ¯ï¼Œé—œæ³¨æ”¿ç­–èˆ‡æ–°è—¥é€²å±•"

    # æ±½è»Š
    elif category == "æ±½è»Š":
        if "ev" in text_all or "electric" in text_all:
            if "slow" in text_all or "cut" in text_all or "pullback" in text_all:
                return "é›»å‹•è»Šéœ€æ±‚æ”¾ç·©ï¼Œè»Šå» èª¿æ•´ç­–ç•¥"
            else:
                return "é›»å‹•è»Šç”¢æ¥­å‹•æ…‹ï¼Œç«¶çˆ­æ ¼å±€è®ŠåŒ–"
        elif "tariff" in text_all:
            return "æ±½è»Šæ¥­é¢è‡¨é—œç¨…å£“åŠ›ï¼Œæˆæœ¬ä¸Šå‡"
        else:
            return "æ±½è»Šç”¢æ¥­æ¶ˆæ¯ï¼Œé—œæ³¨é›»å‹•è»Šç™¼å±•"

    # èˆªç©º/é‹è¼¸
    elif category == "èˆªç©º/é‹è¼¸":
        if "layoff" in text_all or "cut" in text_all:
            return "ç‰©æµæ¥­èª¿æ•´äººåŠ›ï¼Œåæ˜ éœ€æ±‚è®ŠåŒ–"
        elif "earn" in text_all:
            return "é‹è¼¸æ¥­è²¡å ±å…¬å¸ƒï¼Œé—œæ³¨ç‡Ÿé‹å±•æœ›"
        else:
            return "é‹è¼¸ç”¢æ¥­æ¶ˆæ¯ï¼Œç•™æ„ç‰©æµèˆ‡èˆªé‹è¶¨å‹¢"

    # é‡‘è/éŠ€è¡Œ
    elif category == "é‡‘è/éŠ€è¡Œ":
        if "earn" in text_all:
            return "éŠ€è¡Œè²¡å ±å­£ï¼Œé—œæ³¨æ·¨åˆ©å·®èˆ‡ä¿¡è²¸å“è³ª"
        else:
            return "é‡‘èç”¢æ¥­æ¶ˆæ¯ï¼Œé—œæ³¨éŠ€è¡Œè²¡å ±èˆ‡åˆ©å·®"

    # èƒ½æº
    elif category == "èƒ½æº":
        if "oil" in text_all and ("rise" in text_all or "surge" in text_all):
            return "æ²¹åƒ¹ä¸Šæ¼²ï¼Œèƒ½æºè‚¡å—æƒ "
        elif "oil" in text_all and ("fall" in text_all or "drop" in text_all):
            return "æ²¹åƒ¹ä¸‹è·Œï¼Œé€šè†¨å£“åŠ›ç·©è§£"
        else:
            return "èƒ½æºç”¢æ¥­æ¶ˆæ¯ï¼Œé—œæ³¨æ²¹åƒ¹èµ°å‹¢"

    # é›¶å”®/æ¶ˆè²»
    elif category == "é›¶å”®/æ¶ˆè²»":
        if "spend" in text_all and ("strong" in text_all or "rise" in text_all):
            return "æ¶ˆè²»æ”¯å‡ºå¼·å‹ï¼Œé›¶å”®è‚¡è¡¨ç¾å¯æœŸ"
        elif "weak" in text_all or "slow" in text_all:
            return "æ¶ˆè²»å‹•èƒ½æ”¾ç·©ï¼Œé›¶å”®æ¥­æ‰¿å£“"
        else:
            return "é›¶å”®æ¶ˆè²»æ¶ˆæ¯ï¼Œè§€å¯Ÿæ¶ˆè²»è€…ä¿¡å¿ƒ"

    # æˆ¿åœ°ç”¢
    elif category == "æˆ¿åœ°ç”¢":
        if "mortgage" in text_all and "rate" in text_all:
            return "æˆ¿è²¸åˆ©ç‡è®Šå‹•ï¼Œå½±éŸ¿è³¼å±‹éœ€æ±‚"
        else:
            return "æˆ¿åœ°ç”¢æ¶ˆæ¯ï¼Œé—œæ³¨æˆ¿è²¸åˆ©ç‡å½±éŸ¿"

    # åŠ å¯†è²¨å¹£
    elif category == "åŠ å¯†è²¨å¹£":
        if "surge" in text_all or "rally" in text_all or "rise" in text_all:
            return "åŠ å¯†è²¨å¹£ä¸Šæ¼²ï¼Œå¸‚å ´é¢¨éšªåå¥½å›å‡"
        elif "fall" in text_all or "drop" in text_all:
            return "åŠ å¯†è²¨å¹£å›è½ï¼ŒæŠ•è³‡äººè½‰è¶¨ä¿å®ˆ"
        else:
            return "åŠ å¯†è²¨å¹£å¸‚å ´æ³¢å‹•ï¼Œè§€å¯Ÿå¸‚å ´æƒ…ç·’"

    # é è¨­
    return "ç›¸é—œæ¶ˆæ¯æ›´æ–°ï¼ŒæŒçºŒé—œæ³¨å¾ŒçºŒç™¼å±•"


def extract_specific_details(text: str, news_items: list) -> dict:
    """
    å¾æ–°èæ–‡å­—ä¸­æå–å…·é«”ç´°ç¯€ï¼ˆäººåã€æ•¸å­—ã€æ—¥æœŸç­‰ï¼‰
    """
    import re
    details = {
        "people": [],
        "percentages": [],
        "countries": [],
        "companies": [],
        "dates": [],
        "amounts": [],
    }

    # æå–äººåï¼ˆå¸¸è¦‹é‡‘èäººç‰©ï¼‰
    people_patterns = [
        (r"(kevin\s+warsh|warsh)", "Kevin Warsh (è¯è¨±)"),
        (r"(jerome\s+powell|powell|é®‘çˆ¾)", "Jerome Powell (é®‘çˆ¾)"),
        (r"(jensen\s+huang|é»ƒä»å‹³)", "é»ƒä»å‹³ (Jensen Huang)"),
        (r"(trump|å·æ™®)", "å·æ™®"),
        (r"(elon\s+musk|é¦¬æ–¯å…‹)", "Elon Musk"),
        (r"(é­å“²å®¶|c\.c\.\s*wei)", "é­å“²å®¶"),
        (r"(åŠ‰å¾·éŸ³)", "åŠ‰å¾·éŸ³"),
        (r"(è˜‡å§¿ä¸°|lisa\s+su)", "è˜‡å§¿ä¸° (Lisa Su)"),
    ]
    for pattern, name in people_patterns:
        if re.search(pattern, text, re.IGNORECASE):
            if name not in details["people"]:
                details["people"].append(name)

    # æå–ç™¾åˆ†æ¯”
    pct_matches = re.findall(r'(\d+(?:\.\d+)?)\s*%', text)
    details["percentages"] = list(set(pct_matches))

    # æå–é‡‘é¡ï¼ˆå„„ã€å…†ï¼‰
    amount_patterns = [
        (r'(\d+(?:\.\d+)?)\s*å…†', "å…†"),
        (r'(\d+(?:\.\d+)?)\s*å„„', "å„„"),
        (r'\$(\d+(?:\.\d+)?)\s*(trillion|billion|million)', None),
    ]
    for pattern, unit in amount_patterns:
        matches = re.findall(pattern, text, re.IGNORECASE)
        if matches and unit:
            for m in matches:
                details["amounts"].append(f"{m}{unit}")

    # æå–åœ‹å®¶ï¼ˆé—œç¨…ç›¸é—œï¼‰
    country_patterns = [
        (r"(china|ä¸­åœ‹|å¤§é™¸)", "ä¸­åœ‹"),
        (r"(canada|åŠ æ‹¿å¤§)", "åŠ æ‹¿å¤§"),
        (r"(mexico|å¢¨è¥¿å“¥)", "å¢¨è¥¿å“¥"),
        (r"(taiwan|å°ç£)", "å°ç£"),
        (r"(japan|æ—¥æœ¬)", "æ—¥æœ¬"),
        (r"(eu|european|æ­ç›Ÿ|æ­æ´²)", "æ­ç›Ÿ"),
    ]
    for pattern, name in country_patterns:
        if re.search(pattern, text, re.IGNORECASE):
            if name not in details["countries"]:
                details["countries"].append(name)

    # æå–å…¬å¸åç¨±
    company_patterns = [
        (r"(nvidia|è¼é”)", "NVIDIA"),
        (r"(tsmc|å°ç©é›»)", "å°ç©é›»"),
        (r"(apple|è˜‹æœ)", "Apple"),
        (r"(microsoft|å¾®è»Ÿ)", "Microsoft"),
        (r"(google|alphabet|è°·æ­Œ)", "Google"),
        (r"(amazon|äºé¦¬éœ)", "Amazon"),
        (r"(meta|è‡‰æ›¸)", "Meta"),
        (r"(tesla|ç‰¹æ–¯æ‹‰)", "Tesla"),
        (r"(broadcom|åšé€š)", "Broadcom"),
        (r"(amd|è¶…å¾®)", "AMD"),
    ]
    for pattern, name in company_patterns:
        if re.search(pattern, text, re.IGNORECASE):
            if name not in details["companies"]:
                details["companies"].append(name)

    # æå–æœˆä»½/æ—¥æœŸ
    date_patterns = [
        (r"(january|ä¸€æœˆ|1æœˆ)", "1æœˆ"),
        (r"(february|äºŒæœˆ|2æœˆ)", "2æœˆ"),
        (r"(march|ä¸‰æœˆ|3æœˆ)", "3æœˆ"),
        (r"(april|å››æœˆ|4æœˆ)", "4æœˆ"),
        (r"(may|äº”æœˆ|5æœˆ)", "5æœˆ"),
        (r"(june|å…­æœˆ|6æœˆ)", "6æœˆ"),
        (r"(july|ä¸ƒæœˆ|7æœˆ)", "7æœˆ"),
        (r"(august|å…«æœˆ|8æœˆ)", "8æœˆ"),
        (r"(september|ä¹æœˆ|9æœˆ)", "9æœˆ"),
        (r"(october|åæœˆ|10æœˆ)", "10æœˆ"),
        (r"(november|åä¸€æœˆ|11æœˆ)", "11æœˆ"),
        (r"(december|åäºŒæœˆ|12æœˆ)", "12æœˆ"),
    ]
    for pattern, name in date_patterns:
        if re.search(pattern, text, re.IGNORECASE):
            if name not in details["dates"]:
                details["dates"].append(name)

    return details


def generate_dual_summary(category: str, news_items: list) -> dict:
    """
    ç”Ÿæˆé›™æ¬„ç¸½çµï¼šç¢ºèªäº‹å¯¦ + å¸‚å ´é æœŸ
    Returns: {"facts": str, "expectations": str}
    """
    if not news_items:
        return {"facts": "â€”", "expectations": "â€”"}

    # åˆä½µæ‰€æœ‰æ–°èæ–‡å­—
    text_all = " ".join([(n["title"] + " " + (n["content"] or "")).lower() for n in news_items])
    text_original = " ".join([(n["title"] + " " + (n["content"] or "")) for n in news_items])
    top_news = news_items[0]["title"]

    # æå–å…·é«”ç´°ç¯€
    details = extract_specific_details(text_original, news_items)

    # äº‹å¯¦åˆ¤æ–·è©ï¼ˆéå»å¼ã€å·²ç¢ºèªï¼‰
    fact_words = ["holds", "held", "keeps", "kept", "announces", "announced",
                  "decides", "decided", "maintains", "maintained", "unchanged",
                  "cuts", "raises", "raised", "rose", "fell", "dropped", "jumped",
                  "reported", "posted", "beat", "missed", "surged", "plunged"]

    # é æœŸåˆ¤æ–·è©
    expect_words = ["expected", "expects", "may", "might", "could", "likely",
                    "forecast", "predict", "anticipate", "outlook", "guidance",
                    "will", "would", "should", "plan", "plans", "consider"]

    has_facts = any(w in text_all for w in fact_words)
    has_expectations = any(w in text_all for w in expect_words)

    facts = "â€”"
    expectations = "â€”"

    # ===== Fed/åˆ©ç‡ =====
    if category == "Fed/åˆ©ç‡":
        fact_parts = []

        # äº‹å¯¦ï¼šåˆ©ç‡æ±ºç­–
        if any(w in text_all for w in ["holds", "held", "keeps", "kept", "maintains", "maintained"]):
            if "rate" in text_all or "interest" in text_all:
                fact_parts.append("Fed å®£å¸ƒç¶­æŒåˆ©ç‡ä¸è®Š")
        elif "cut" in text_all and any(w in text_all for w in ["announced", "cuts", "decided"]):
            fact_parts.append("Fed å®£å¸ƒé™æ¯")
        elif "hike" in text_all or "raise" in text_all:
            if any(w in text_all for w in ["announced", "raises", "raised"]):
                fact_parts.append("Fed å®£å¸ƒå‡æ¯")

        # é™„åŠ äº‹å¯¦ï¼šFed ä¸»å¸­ç¹¼ä»»è€…
        if "warsh" in text_all or "è¯è¨±" in text_all:
            if "nominate" in text_all or "æå" in text_all or "successor" in text_all:
                nominee_info = "å·æ™®æå Kevin Warsh (è¯è¨±) æ¥ä»» Fed ä¸»å¸­"
                if "5æœˆ" in text_original or "may" in text_all:
                    nominee_info += "ï¼Œé è¨ˆ5æœˆé®‘çˆ¾ä»»æœŸå±†æ»¿å¾Œæ¥ä»»"
                fact_parts.append(nominee_info)
        elif "successor" in text_all or "replace" in text_all or "candidate" in text_all or "ç¹¼ä»»" in text_all:
            fact_parts.append("Powell ç¹¼ä»»è€…è­°é¡Œæµ®ç¾")

        # çµ„åˆäº‹å¯¦
        if fact_parts:
            facts = "ï¼›".join(fact_parts)

        # é æœŸ
        if "pause" in text_all or "wait" in text_all:
            expectations = "å¸‚å ´é æœŸçŸ­æœŸç¶­æŒè§€æœ›"
        elif "cut" in text_all and any(w in text_all for w in expect_words):
            expectations = "å¸‚å ´é æœŸæœªä¾†å¯èƒ½é™æ¯"
        elif "hike" in text_all and any(w in text_all for w in expect_words):
            expectations = "å¸‚å ´é æœŸå¯èƒ½å†å‡æ¯"
        elif "data" in text_all or "inflation" in text_all:
            expectations = "é—œæ³¨å¾ŒçºŒç¶“æ¿Ÿæ•¸æ“šèµ°å‘"

    # ===== é€šè†¨ =====
    elif category == "é€šè†¨":
        if "cpi" in text_all or "pce" in text_all:
            if "fell" in text_all or "dropped" in text_all or "eased" in text_all:
                facts = "é€šè†¨æ•¸æ“šä¸‹æ»‘"
            elif "rose" in text_all or "jumped" in text_all or "higher" in text_all:
                facts = "é€šè†¨æ•¸æ“šä¸Šå‡"
            elif "reported" in text_all or "released" in text_all:
                facts = "é€šè†¨æ•¸æ“šå…¬å¸ƒ"

        if "sticky" in text_all or "persistent" in text_all:
            expectations = "é€šè†¨é»æ€§ä»é«˜ï¼Œé™æ¯æ™‚ç¨‹æå»¶å¾Œ"
        elif "ease" in text_all or "cool" in text_all:
            expectations = "é€šè†¨æœ‰æœ›æŒçºŒé™æº«"
        elif "target" in text_all:
            expectations = "é—œæ³¨æ˜¯å¦é”æˆ 2% ç›®æ¨™"

    # ===== å°±æ¥­ =====
    elif category == "å°±æ¥­":
        if "added" in text_all or "payroll" in text_all:
            if "beat" in text_all or "strong" in text_all:
                facts = "éè¾²å°±æ¥­æ•¸æ“šå„ªæ–¼é æœŸ"
            elif "miss" in text_all or "weak" in text_all:
                facts = "éè¾²å°±æ¥­æ•¸æ“šä¸å¦‚é æœŸ"
            else:
                facts = "éè¾²å°±æ¥­æ•¸æ“šå…¬å¸ƒ"
        elif "layoff" in text_all or "layoffs" in text_all:
            facts = "ä¼æ¥­è£å“¡æ¶ˆæ¯é »å‚³"
        elif "unemployment" in text_all:
            if "rose" in text_all or "higher" in text_all:
                facts = "å¤±æ¥­ç‡ä¸Šå‡"
            elif "fell" in text_all or "low" in text_all:
                facts = "å¤±æ¥­ç‡ç¶­æŒä½æª”"

        if "recession" in text_all:
            expectations = "å°±æ¥­æƒ¡åŒ–æåŠ æ·±è¡°é€€æ“”æ†‚"
        elif "soft landing" in text_all:
            expectations = "è»Ÿè‘—é™¸é æœŸä»å­˜"
        elif "labor" in text_all and "tight" in text_all:
            expectations = "å‹å‹•å¸‚å ´ä»åç·Šä¿"

    # ===== è²¿æ˜“/é—œç¨… =====
    elif category == "è²¿æ˜“/é—œç¨…":
        import re
        fact_parts = []

        # æå–åœ‹å®¶èˆ‡é—œç¨…ç™¾åˆ†æ¯”çš„é…å°
        tariff_details = []
        country_tariff_patterns = [
            (r"china|ä¸­åœ‹|å¤§é™¸", "ä¸­åœ‹"),
            (r"canada|åŠ æ‹¿å¤§", "åŠ æ‹¿å¤§"),
            (r"mexico|å¢¨è¥¿å“¥", "å¢¨è¥¿å“¥"),
            (r"eu|european|æ­ç›Ÿ|æ­æ´²", "æ­ç›Ÿ"),
            (r"japan|æ—¥æœ¬", "æ—¥æœ¬"),
            (r"taiwan|å°ç£", "å°ç£"),
        ]

        # å˜—è©¦æå–ã€Œåœ‹å®¶ + ç™¾åˆ†æ¯”ã€çš„é—œç¨…è³‡è¨Š
        for pattern, country_name in country_tariff_patterns:
            # æœå°‹è©²åœ‹å®¶é™„è¿‘çš„ç™¾åˆ†æ¯”
            country_match = re.search(pattern, text_all, re.IGNORECASE)
            if country_match:
                # åœ¨åœ‹å®¶åç¨±å‰å¾Œ 50 å­—å…ƒç¯„åœå…§æœå°‹ç™¾åˆ†æ¯”
                start = max(0, country_match.start() - 50)
                end = min(len(text_all), country_match.end() + 50)
                context = text_all[start:end]
                pct_match = re.search(r'(\d+(?:\.\d+)?)\s*%', context)
                if pct_match:
                    tariff_details.append(f"{country_name} {pct_match.group(1)}%")

        # çµ„åˆé—œç¨…ç´°ç¯€
        if tariff_details:
            fact_parts.append("é—œç¨…ç¾æ³ï¼š" + "ã€".join(tariff_details))
        elif "impose" in text_all or "slaps" in text_all or "enacted" in text_all:
            fact_parts.append("é—œç¨…æ”¿ç­–å·²å¯¦æ–½")
        elif "announced" in text_all and "tariff" in text_all:
            fact_parts.append("é—œç¨…æªæ–½å®£å¸ƒ")
        elif "delay" in text_all or "pause" in text_all:
            fact_parts.append("é—œç¨…æªæ–½æš«ç·©")

        # åŠ å…¥æ¶‰åŠçš„åœ‹å®¶åˆ—è¡¨ï¼ˆå¦‚æœæ²’æœ‰å…·é«”ç¨…ç‡ï¼‰
        if not tariff_details and details["countries"]:
            fact_parts.append(f"æ¶‰åŠåœ‹å®¶ï¼š{', '.join(details['countries'])}")

        if fact_parts:
            facts = "ï¼›".join(fact_parts)

        # é æœŸ
        if "threat" in text_all or "warns" in text_all:
            expectations = "æ›´å¤šé—œç¨…å¨è„…å¯èƒ½å‡ºç¾"
        elif "negotiat" in text_all or "talk" in text_all:
            expectations = "è²¿æ˜“è«‡åˆ¤æŒçºŒé€²è¡Œä¸­"
        elif "retaliat" in text_all:
            expectations = "ç•™æ„å°æ–¹å ±å¾©æªæ–½"
        elif "escalat" in text_all:
            expectations = "è²¿æ˜“æˆ°å¯èƒ½å‡ç´š"

    # ===== AI/ç§‘æŠ€ =====
    elif category == "AI/ç§‘æŠ€" or category == "ç§‘æŠ€" or category == "AI":
        fact_parts = []

        # é»ƒä»å‹³/NVIDIA ç›¸é—œ
        if "é»ƒä»å‹³" in text_original or "jensen huang" in text_all:
            event_desc = "é»ƒä»å‹³ (Jensen Huang)"
            if "å®´" in text_original or "dinner" in text_all or "banquet" in text_all:
                event_desc = "é»ƒä»å‹³èˆ‰è¾¦å…†å…ƒå®´"
                # æª¢æŸ¥èˆ‡æœƒè€…
                attendees = []
                if "é­å“²å®¶" in text_original or "c.c. wei" in text_all:
                    attendees.append("å°ç©é›»é­å“²å®¶")
                if "åŠ‰å¾·éŸ³" in text_original:
                    attendees.append("åŠ‰å¾·éŸ³")
                if "ä¾›æ‡‰éˆ" in text_original or "supply chain" in text_all:
                    event_desc += "ï¼Œä¾›æ‡‰éˆå¤§è€é½Šèš"
                elif attendees:
                    event_desc += f"ï¼Œ{', '.join(attendees)}ç­‰å‡ºå¸­"
            elif "å°åŒ—" in text_original or "taipei" in text_all:
                event_desc += " è¨ªå°"
            fact_parts.append(event_desc)

        # NVIDIA è²¡å ±/æ¥­ç¸¾
        if "nvidia" in text_all or "è¼é”" in text_original:
            if "earnings" in text_all or "è²¡å ±" in text_original:
                if "beat" in text_all or "è¶…é æœŸ" in text_original:
                    fact_parts.append("NVIDIA è²¡å ±å„ªæ–¼é æœŸ")
                elif "miss" in text_all:
                    fact_parts.append("NVIDIA è²¡å ±ä¸å¦‚é æœŸ")
                else:
                    fact_parts.append("NVIDIA è²¡å ±ç™¼å¸ƒ")
            if "guidance" in text_all or "è²¡æ¸¬" in text_original:
                fact_parts.append("NVIDIA ç™¼å¸ƒè²¡æ¸¬æŒ‡å¼•")

        # AI ç”¢æ¥­å‹•æ…‹
        if "ai chip" in text_all or "ai æ™¶ç‰‡" in text_original or "äººå·¥æ™ºæ…§æ™¶ç‰‡" in text_original:
            fact_parts.append("AI æ™¶ç‰‡éœ€æ±‚ç›¸é—œæ¶ˆæ¯")
        if "data center" in text_all or "è³‡æ–™ä¸­å¿ƒ" in text_original:
            fact_parts.append("è³‡æ–™ä¸­å¿ƒéœ€æ±‚æŒçºŒ")

        # å…¶ä»–ç§‘æŠ€å…¬å¸
        if details["companies"]:
            companies_mentioned = [c for c in details["companies"] if c != "NVIDIA"]
            if companies_mentioned and not fact_parts:
                fact_parts.append(f"ç›¸é—œå…¬å¸ï¼š{', '.join(companies_mentioned[:3])}")

        if fact_parts:
            facts = "ï¼›".join(fact_parts)
        else:
            facts = "AI/ç§‘æŠ€ç”¢æ¥­å‹•æ…‹æ›´æ–°"

        # é æœŸ
        if "demand" in text_all or "éœ€æ±‚" in text_original:
            expectations = "AI ç›¸é—œéœ€æ±‚æŒçºŒçœ‹å¥½"
        elif "competition" in text_all or "ç«¶çˆ­" in text_original:
            expectations = "é—œæ³¨ç”¢æ¥­ç«¶çˆ­æ…‹å‹¢"
        elif "supply" in text_all or "ä¾›æ‡‰" in text_original:
            expectations = "ä¾›æ‡‰éˆç‹€æ³å—é—œæ³¨"
        else:
            expectations = "æŒçºŒé—œæ³¨ AI ç”¢æ¥­ç™¼å±•"

    # ===== é»ƒé‡‘/é¿éšª =====
    elif category == "é»ƒé‡‘/é¿éšª":
        if "record" in text_all or "all-time" in text_all:
            facts = "é»ƒé‡‘å‰µæ­·å²æ–°é«˜"
        elif "surged" in text_all or "jumped" in text_all or "rallied" in text_all:
            facts = "é»ƒé‡‘å¤§å¹…ä¸Šæ¼²"
        elif "fell" in text_all or "dropped" in text_all:
            facts = "é»ƒé‡‘åƒ¹æ ¼å›è½"

        if "safe haven" in text_all or "geopolitical" in text_all:
            expectations = "é¿éšªéœ€æ±‚å¯èƒ½æŒçºŒ"
        elif "dollar" in text_all:
            expectations = "é—œæ³¨ç¾å…ƒèµ°å‹¢å½±éŸ¿"

    # ===== å‚µåˆ¸/æ®–åˆ©ç‡ =====
    elif category == "å‚µåˆ¸/æ®–åˆ©ç‡":
        if "invert" in text_all:
            facts = "æ®–åˆ©ç‡æ›²ç·šå€’æ›"
        elif "rose" in text_all or "jumped" in text_all or "climbed" in text_all:
            facts = "æ®–åˆ©ç‡ä¸Šå‡"
        elif "fell" in text_all or "dropped" in text_all:
            facts = "æ®–åˆ©ç‡ä¸‹æ»‘"

        if "recession" in text_all:
            expectations = "å€’æ›åŠ æ·±è¡°é€€æ“”æ†‚"
        elif "fed" in text_all:
            expectations = "é—œæ³¨ Fed æ”¿ç­–å½±éŸ¿"

    # ===== ç¾å…ƒ/åŒ¯ç‡ =====
    elif category == "ç¾å…ƒ/åŒ¯ç‡":
        if "rose" in text_all or "strengthened" in text_all or "surged" in text_all:
            facts = "ç¾å…ƒèµ°å¼·"
        elif "fell" in text_all or "weakened" in text_all or "dropped" in text_all:
            facts = "ç¾å…ƒèµ°å¼±"
        elif "intervention" in text_all:
            facts = "å¤®è¡Œå¹²é åŒ¯å¸‚"

        if "emerging" in text_all:
            expectations = "æ–°èˆˆå¸‚å ´å¯èƒ½æ‰¿å£“"
        elif "export" in text_all:
            expectations = "å‡ºå£ä¼æ¥­å—åŒ¯ç‡å½±éŸ¿"

    # ===== GDP/ç¶“æ¿Ÿæˆé•· =====
    elif category == "GDP/ç¶“æ¿Ÿæˆé•·":
        if "grew" in text_all or "expanded" in text_all:
            facts = "GDP æ­£æˆé•·"
        elif "contracted" in text_all or "shrank" in text_all:
            facts = "GDP è² æˆé•·"
        elif "reported" in text_all or "released" in text_all:
            facts = "GDP æ•¸æ“šå…¬å¸ƒ"

        if "recession" in text_all:
            expectations = "è¡°é€€é¢¨éšªå—é—œæ³¨"
        elif "soft landing" in text_all:
            expectations = "è»Ÿè‘—é™¸é æœŸ"
        elif "growth" in text_all and any(w in text_all for w in expect_words):
            expectations = "ç¶“æ¿Ÿæˆé•·å±•æœ›å¯©æ…"

    # ===== æ”¿åºœæ”¿ç­– =====
    elif category == "æ”¿åºœæ”¿ç­–":
        if "shutdown" in text_all:
            if "avoid" in text_all or "avert" in text_all:
                facts = "æ”¿åºœé—œé–€å±æ©Ÿæš«è§£"
            else:
                facts = "æ”¿åºœé—œé–€é¢¨éšªå‡é«˜"
        elif "pass" in text_all or "approved" in text_all:
            facts = "æ”¿ç­–æ³•æ¡ˆé€šé"

        if "debt" in text_all and "ceiling" in text_all:
            expectations = "å‚µå‹™ä¸Šé™è­°é¡Œå¾…è§£"
        elif "stimulus" in text_all:
            expectations = "è²¡æ”¿åˆºæ¿€æ”¿ç­–å‹•å‘"

    # ===== å€‹è‚¡/ä¼æ¥­ =====
    elif category == "å€‹è‚¡/ä¼æ¥­" or category == "ä¼æ¥­" or category == "å€‹è‚¡":
        fact_parts = []

        # è²¡å ±ç›¸é—œ
        if "earnings" in text_all or "è²¡å ±" in text_original:
            if "beat" in text_all or "è¶…é æœŸ" in text_original or "å„ªæ–¼" in text_original:
                fact_parts.append("è²¡å ±å„ªæ–¼é æœŸ")
            elif "miss" in text_all or "ä¸å¦‚é æœŸ" in text_original:
                fact_parts.append("è²¡å ±ä¸å¦‚é æœŸ")
            else:
                fact_parts.append("è²¡å ±å…¬å¸ƒ")

        # äººäº‹/æ´»å‹•
        if details["people"]:
            people_str = "ã€".join(details["people"][:3])
            if "å®´" in text_original or "dinner" in text_all or "banquet" in text_all:
                fact_parts.append(f"{people_str}èˆ‰è¾¦é¤æœƒæ´»å‹•")
            elif "è¨ª" in text_original or "visit" in text_all:
                fact_parts.append(f"{people_str}å‡ºè¨ªæ´»å‹•")
            elif "æœƒè­°" in text_original or "meeting" in text_all:
                fact_parts.append(f"{people_str}åƒèˆ‡æœƒè­°")

        # å…¬å¸å‹•æ…‹
        if details["companies"]:
            companies_str = "ã€".join(details["companies"][:3])
            if not fact_parts:
                fact_parts.append(f"æ¶‰åŠå…¬å¸ï¼š{companies_str}")

        # é‡‘é¡ç›¸é—œ
        if details["amounts"]:
            amounts_str = "ã€".join(details["amounts"][:2])
            fact_parts.append(f"æ¶‰åŠé‡‘é¡ï¼š{amounts_str}")

        if fact_parts:
            facts = "ï¼›".join(fact_parts)
        else:
            facts = "ä¼æ¥­å‹•æ…‹æ›´æ–°"

        # é æœŸ
        if "guidance" in text_all or "è²¡æ¸¬" in text_original:
            expectations = "é—œæ³¨å¾ŒçºŒè²¡æ¸¬æŒ‡å¼•"
        elif "merger" in text_all or "acquisition" in text_all or "ä½µè³¼" in text_original:
            expectations = "ä½µè³¼æ¡ˆå¾ŒçºŒç™¼å±•"
        elif "layoff" in text_all or "è£å“¡" in text_original:
            expectations = "é—œæ³¨ä¼æ¥­ç‡Ÿé‹ç‹€æ³"
        else:
            expectations = "æŒçºŒé—œæ³¨ä¼æ¥­å‹•æ…‹"

    # ===== é€šç”¨è™•ç†ï¼ˆç¢ºä¿è¼¸å‡ºä¸­æ–‡ï¼‰=====
    # å„é¡åˆ¥çš„é è¨­ä¸­æ–‡æè¿°
    category_default_facts = {
        "Fed/åˆ©ç‡": "Fed æ”¿ç­–å‹•æ…‹æ›´æ–°",
        "é€šè†¨": "é€šè†¨ç›¸é—œæ•¸æ“šç™¼å¸ƒ",
        "å°±æ¥­": "å°±æ¥­å¸‚å ´æ¶ˆæ¯æ›´æ–°",
        "è²¿æ˜“/é—œç¨…": "è²¿æ˜“æ”¿ç­–å‹•æ…‹",
        "é»ƒé‡‘/é¿éšª": "è²´é‡‘å±¬å¸‚å ´æ³¢å‹•",
        "å‚µåˆ¸/æ®–åˆ©ç‡": "å‚µå¸‚è¡Œæƒ…è®ŠåŒ–",
        "ç¾å…ƒ/åŒ¯ç‡": "åŒ¯ç‡å¸‚å ´å‹•æ…‹",
        "GDP/ç¶“æ¿Ÿæˆé•·": "ç¶“æ¿Ÿæ•¸æ“šæ›´æ–°",
        "æ”¿åºœæ”¿ç­–": "æ”¿åºœæ”¿ç­–å‹•æ…‹",
        "AI/ç§‘æŠ€": "AI/ç§‘æŠ€ç”¢æ¥­å‹•æ…‹",
        "ç§‘æŠ€": "ç§‘æŠ€ç”¢æ¥­å‹•æ…‹",
        "AI": "äººå·¥æ™ºæ…§ç”¢æ¥­å‹•æ…‹",
        "å€‹è‚¡/ä¼æ¥­": "ä¼æ¥­å‹•æ…‹æ›´æ–°",
        "ä¼æ¥­": "ä¼æ¥­å‹•æ…‹æ›´æ–°",
        "å€‹è‚¡": "å€‹è‚¡å‹•æ…‹æ›´æ–°",
    }

    category_default_expectations = {
        "Fed/åˆ©ç‡": "æŒçºŒé—œæ³¨åˆ©ç‡æ”¿ç­–èµ°å‘",
        "é€šè†¨": "è§€å¯Ÿé€šè†¨è¶¨å‹¢è®ŠåŒ–",
        "å°±æ¥­": "ç•™æ„å‹å‹•å¸‚å ´è¡¨ç¾",
        "è²¿æ˜“/é—œç¨…": "é—œæ³¨å¾ŒçºŒè²¿æ˜“ç™¼å±•",
        "é»ƒé‡‘/é¿éšª": "è§€å¯Ÿé¿éšªæƒ…ç·’è®ŠåŒ–",
        "å‚µåˆ¸/æ®–åˆ©ç‡": "ç•™æ„æ®–åˆ©ç‡èµ°å‹¢",
        "ç¾å…ƒ/åŒ¯ç‡": "é—œæ³¨åŒ¯ç‡æ³¢å‹•å½±éŸ¿",
        "GDP/ç¶“æ¿Ÿæˆé•·": "è§€å¯Ÿç¶“æ¿Ÿæˆé•·å‹•èƒ½",
        "æ”¿åºœæ”¿ç­–": "é—œæ³¨æ”¿ç­–å¾ŒçºŒç™¼å±•",
        "AI/ç§‘æŠ€": "æŒçºŒé—œæ³¨ AI ç”¢æ¥­ç™¼å±•",
        "ç§‘æŠ€": "æŒçºŒé—œæ³¨ç§‘æŠ€ç”¢æ¥­ç™¼å±•",
        "AI": "æŒçºŒé—œæ³¨ AI ç”¢æ¥­ç™¼å±•",
        "å€‹è‚¡/ä¼æ¥­": "æŒçºŒé—œæ³¨ä¼æ¥­å‹•æ…‹",
        "ä¼æ¥­": "æŒçºŒé—œæ³¨ä¼æ¥­å‹•æ…‹",
        "å€‹è‚¡": "æŒçºŒé—œæ³¨å€‹è‚¡è¡¨ç¾",
    }

    # å¦‚æœæ²’æœ‰åŒ¹é…åˆ°å…·é«”äº‹å¯¦ï¼Œä½¿ç”¨é¡åˆ¥é è¨­
    if facts == "â€”" and has_facts:
        facts = category_default_facts.get(category, "ç›¸é—œæ¶ˆæ¯æ›´æ–°")

    # å¦‚æœæ²’æœ‰åŒ¹é…åˆ°å…·é«”é æœŸï¼Œä½¿ç”¨é¡åˆ¥é è¨­
    if expectations == "â€”" and has_expectations:
        if "outlook" in text_all or "guidance" in text_all:
            expectations = "é—œæ³¨å¾ŒçºŒè²¡æ¸¬å±•æœ›"
        elif "earnings" in text_all:
            expectations = "è²¡å ±å­£æŒçºŒé—œæ³¨"
        else:
            expectations = category_default_expectations.get(category, "æŒçºŒè§€å¯Ÿå¾ŒçºŒç™¼å±•")

    # é™åˆ¶æ–‡å­—é•·åº¦é¿å…ç ´ç‰ˆï¼ˆæœ€å¤š 60 å€‹å­—å…ƒï¼‰
    max_len = 60
    if len(facts) > max_len:
        facts = facts[:max_len-1] + "â€¦"
    if len(expectations) > max_len:
        expectations = expectations[:max_len-1] + "â€¦"

    return {"facts": facts, "expectations": expectations}


@st.cache_resource
def get_connection():
    """å–å¾—æ–°èè³‡æ–™åº«é€£æ¥ (SQLite fallback)"""
    if DB_TYPE != "sqlite":
        return None  # é SQLite ä¸éœ€è¦é€£æ¥ç‰©ä»¶
    if not DB_PATH.exists():
        raise FileNotFoundError(f"æ–°èè³‡æ–™åº«ä¸å­˜åœ¨: {DB_PATH}")
    return sqlite3.connect(DB_PATH, check_same_thread=False)


@st.cache_resource
def get_finance_connection():
    """å–å¾—é‡‘èè³‡æ–™åº«é€£æ¥ (SQLite fallback)"""
    if DB_TYPE != "sqlite":
        return None  # é SQLite ä¸éœ€è¦é€£æ¥ç‰©ä»¶
    if not FINANCE_DB_PATH.exists():
        raise FileNotFoundError(f"é‡‘èè³‡æ–™åº«ä¸å­˜åœ¨: {FINANCE_DB_PATH}")
    return sqlite3.connect(FINANCE_DB_PATH, check_same_thread=False)


def get_watchlist():
    """å–å¾—è¿½è¹¤æ¸…å–® - ä½¿ç”¨çµ±ä¸€è³‡æ–™å±¤"""
    try:
        client = _get_data_client()
        data = client.get_watchlist()
        return [{"symbol": r["symbol"], "name": r.get("name", ""), "market": r.get("market", ""),
                 "sector": r.get("sector", ""), "industry": r.get("industry", ""), "description": ""} for r in data]
    except Exception as e:
        st.error(f"å–å¾—è¿½è¹¤æ¸…å–®å¤±æ•—: {e}")
        return []


def get_stock_info(symbol: str):
    """å–å¾—å–®ä¸€è‚¡ç¥¨çš„è©³ç´°è³‡è¨Š - ä½¿ç”¨çµ±ä¸€è³‡æ–™å±¤"""
    try:
        client = _get_data_client()
        watchlist = client.get_watchlist()
        for r in watchlist:
            if r.get("symbol") == symbol:
                return {"symbol": r["symbol"], "name": r.get("name", ""), "market": r.get("market", ""),
                        "sector": r.get("sector", ""), "industry": r.get("industry", ""), "description": ""}
        return None
    except Exception:
        return None


def get_stock_prices(symbol: str, start_date: date = None, end_date: date = None):
    """å–å¾—è‚¡ç¥¨åƒ¹æ ¼æ•¸æ“š - ä½¿ç”¨çµ±ä¸€è³‡æ–™å±¤"""
    try:
        client = _get_data_client()
        data = client.get_daily_prices(symbol, start_date=start_date, end_date=end_date)

        if data:
            df = pd.DataFrame(data)
            if "date" in df.columns:
                df["date"] = pd.to_datetime(df["date"])
                # æŒ‰æ—¥æœŸå‡åºæ’åˆ—
                df = df.sort_values("date").reset_index(drop=True)
            return df
        return pd.DataFrame()
    except Exception as e:
        st.error(f"å–å¾—åƒ¹æ ¼æ•¸æ“šå¤±æ•—: {e}")
        return pd.DataFrame()


def get_stock_fundamentals(symbol: str):
    """å–å¾—è‚¡ç¥¨åŸºæœ¬é¢æ•¸æ“š"""
    # ç›®å‰çµ±ä¸€è³‡æ–™å±¤å°šæœªæ”¯æ´ fundamentals æŸ¥è©¢
    # ç•¶ä½¿ç”¨ PostgreSQL æ™‚æš«æ™‚è¿”å› None
    if DB_TYPE != "sqlite":
        return None

    conn = get_finance_connection()
    if not conn:
        return None
    cursor = conn.cursor()
    cursor.execute("""
        SELECT * FROM fundamentals
        WHERE symbol = ?
        ORDER BY date DESC
        LIMIT 1
    """, (symbol,))

    row = cursor.fetchone()
    if row:
        columns = [desc[0] for desc in cursor.description]
        return dict(zip(columns, row))
    return None


def get_news_for_stock(symbol: str, selected_date: date):
    """å–å¾—èˆ‡è‚¡ç¥¨ç›¸é—œçš„æ–°è - ä½¿ç”¨çµ±ä¸€è³‡æ–™å±¤"""
    # å»ºç«‹æœå°‹é—œéµå­—
    symbol_clean = symbol.replace(".TW", "").replace("^", "")

    # è‚¡ç¥¨ä»£ç¢¼å°æ‡‰çš„å…¬å¸åç¨±
    stock_keywords = {
        "AAPL": ["apple", "iphone", "aapl"],
        "MSFT": ["microsoft", "msft", "azure", "windows"],
        "GOOGL": ["google", "alphabet", "googl", "android", "youtube"],
        "AMZN": ["amazon", "amzn", "aws"],
        "NVDA": ["nvidia", "nvda", "gpu", "chip"],
        "META": ["meta", "facebook", "instagram", "whatsapp"],
        "TSLA": ["tesla", "tsla", "elon musk", "ev"],
        "JPM": ["jpmorgan", "jp morgan", "jpm", "jamie dimon"],
        "V": ["visa"],
        "UNH": ["unitedhealth", "unh"],
        "2330": ["tsmc", "å°ç©é›»", "2330"],
        "2317": ["é´»æµ·", "foxconn", "hon hai", "2317"],
        "2454": ["è¯ç™¼ç§‘", "mediatek", "2454"],
        "SPY": ["s&p 500", "s&p500", "spy"],
        "QQQ": ["nasdaq", "qqq", "nasdaq 100"],
    }

    keywords = stock_keywords.get(symbol_clean, [symbol_clean.lower()])

    try:
        # å–å¾—ç•¶å¤©æ–°èï¼Œç„¶å¾Œåœ¨ Python ä¸­éæ¿¾é—œéµå­—
        news_list = get_news_by_date(selected_date)
        all_news = []

        for news in news_list:
            title_lower = (news.get("title") or "").lower()
            content_lower = (news.get("content") or "").lower()
            text = title_lower + " " + content_lower

            for keyword in keywords:
                if keyword.lower() in text:
                    all_news.append(news)
                    break

        # å»é‡
        seen_ids = set()
        unique_news = []
        for n in all_news:
            news_id = n.get("id")
            if news_id and news_id not in seen_ids:
                seen_ids.add(news_id)
                unique_news.append(n)

        return unique_news
    except Exception as e:
        return []


def get_news_in_date_range(start_date: date, end_date: date, keyword: str = None):
    """å–å¾—æ—¥æœŸç¯„åœå…§çš„æ–°èçµ±è¨ˆ - ä½¿ç”¨çµ±ä¸€è³‡æ–™å±¤"""
    try:
        client = _get_data_client()
        news_list = client.get_news(
            start_date=start_date,
            end_date=end_date,
            limit=5000
        )

        date_counts = {}
        for r in news_list:
            # éæ¿¾é—œéµå­—
            if keyword and keyword.lower() not in (r.get("title") or "").lower():
                continue

            # å–å¾—æ—¥æœŸï¼ˆå„ªå…ˆä½¿ç”¨ collected_atï¼Œfallback åˆ° published_atï¼‰
            date_val = r.get("collected_at") or r.get("published_at") or ""
            if date_val:
                d = str(date_val)[:10]
                date_counts[d] = date_counts.get(d, 0) + 1

        return date_counts
    except Exception as e:
        return {}


def get_available_dates():
    """å–å¾—æœ‰æ–°èçš„æ—¥æœŸåˆ—è¡¨ - ä½¿ç”¨çµ±ä¸€è³‡æ–™å±¤"""
    try:
        client = _get_data_client()
        # å–å¾—æœ€è¿‘ 90 å¤©çš„æ–°èä¾†æå–æ—¥æœŸ
        end_date = date.today()
        start_date = end_date - timedelta(days=90)
        news_list = client.get_news(start_date=start_date, end_date=end_date, limit=5000)

        dates_set = set()
        for r in news_list:
            date_val = r.get("collected_at") or r.get("published_at") or ""
            if date_val:
                dates_set.add(str(date_val)[:10])

        dates = sorted(dates_set, reverse=True)
        return [datetime.strptime(d, "%Y-%m-%d").date() for d in dates if d]
    except Exception as e:
        return []


def get_ptt_available_dates():
    """å–å¾— PTT æœ‰æ–‡ç« çš„æ—¥æœŸåˆ—è¡¨ - ä½¿ç”¨çµ±ä¸€è³‡æ–™å±¤"""
    try:
        client = _get_data_client()
        # å–å¾—æœ€è¿‘ 90 å¤©çš„ PTT æ–°è
        end_date = date.today()
        start_date = end_date - timedelta(days=90)
        news_list = client.get_news(
            start_date=start_date, end_date=end_date,
            source="ptt", limit=5000
        )

        dates_set = set()
        for r in news_list:
            if r.get("source_type") == "ptt" and r.get("published_at"):
                dates_set.add(str(r["published_at"])[:10])

        dates = sorted(dates_set, reverse=True)
        return [datetime.strptime(d, "%Y-%m-%d").date() for d in dates if d]
    except Exception as e:
        return []


def get_news_by_date(selected_date: date):
    """å–å¾—æŒ‡å®šæ—¥æœŸçš„æ–°è - ä½¿ç”¨çµ±ä¸€è³‡æ–™å±¤"""
    try:
        client = _get_data_client()
        # ä½¿ç”¨çµ±ä¸€è³‡æ–™å±¤çš„ get_news æ–¹æ³•
        news_list = client.get_news(
            start_date=selected_date,
            end_date=selected_date,
            limit=500
        )
        return news_list
    except Exception as e:
        st.error(f"å–å¾—æ–°èå¤±æ•—: {e}")
        return []


def get_news_stats_by_date(selected_date: date):
    """å–å¾—æŒ‡å®šæ—¥æœŸçš„æ–°èçµ±è¨ˆ - ä½¿ç”¨çµ±ä¸€è³‡æ–™å±¤"""
    try:
        # å–å¾—ç•¶æ—¥æ–°èä¸¦è¨ˆç®—çµ±è¨ˆ
        news_list = get_news_by_date(selected_date)

        by_source_type = {}
        by_source = {}
        for r in news_list:
            st = r.get("source_type") or "other"
            by_source_type[st] = by_source_type.get(st, 0) + 1
            s = r.get("source") or "unknown"
            by_source[s] = by_source.get(s, 0) + 1

        return {
            "total_count": len(news_list),
            "by_source_type": by_source_type,
            "by_source": dict(sorted(by_source.items(), key=lambda x: x[1], reverse=True)[:10]),
        }
    except Exception as e:
        return {"total_count": 0, "by_source_type": {}, "by_source": {}}


def get_weekly_news(end_date: date, days: int = 7) -> list:
    """å–å¾—éå»ä¸€é€±çš„æ–°è - ä½¿ç”¨çµ±ä¸€è³‡æ–™å±¤"""
    try:
        client = _get_data_client()
        start_date = end_date - timedelta(days=days)
        news_list = client.get_news(
            start_date=start_date,
            end_date=end_date,
            limit=2000
        )
        return news_list
    except Exception as e:
        st.error(f"å–å¾—é€±æ–°èå¤±æ•—: {e}")
        return []


def categorize_news(news_list: list) -> dict:
    """å°‡æ–°èåˆ†é¡ç‚ºç¸½ç¶“ã€ç”¢æ¥­å’Œç§‘æŠ€ç”¢æ¥­éˆ"""
    macro_news = defaultdict(list)
    industry_news = defaultdict(list)
    tech_supply_chain_news = defaultdict(list)

    for news in news_list:
        title_lower = news["title"].lower()
        content_lower = (news["content"] or "").lower()
        text = title_lower + " " + content_lower

        # ç¸½ç¶“åˆ†é¡
        for category, keywords in MACRO_KEYWORDS.items():
            if any(kw in text for kw in keywords):
                macro_news[category].append(news)
                break

        # ç”¢æ¥­åˆ†é¡
        for category, keywords in INDUSTRY_KEYWORDS.items():
            if any(kw in text for kw in keywords):
                industry_news[category].append(news)
                break

        # ç§‘æŠ€ç”¢æ¥­éˆåˆ†é¡ï¼ˆä¸€å‰‡æ–°èå¯æ­¸å…¥å¤šå€‹ç”¢æ¥­éˆé¡åˆ¥ï¼‰
        for category, keywords in TECH_SUPPLY_CHAIN_KEYWORDS.items():
            if any(kw in text for kw in keywords):
                tech_supply_chain_news[category].append(news)

    return {
        "macro": dict(macro_news),
        "industry": dict(industry_news),
        "tech_supply_chain": dict(tech_supply_chain_news),
    }


def generate_weekly_summary(category: str, weekly_news: list, daily_count: int) -> tuple:
    """
    æ ¹æ“šä¸€é€±æ–°èç”Ÿæˆç”¢æ¥­ç¸½çµ
    Returns: (ç‡ˆè™Ÿ, ç¸½çµæ–‡å­—, é€±è¶¨å‹¢)
    """
    if not weekly_news:
        return "âšª", "æœ¬é€±ç„¡ç›¸é—œæ–°è", "â€”"

    # åˆä½µæ‰€æœ‰æ–°èæ–‡å­—
    text_all = " ".join([(n["title"] + " " + (n["content"] or "")).lower() for n in weekly_news])

    # è¨ˆç®—æ­£è² é¢æƒ…ç·’
    positive_count = sum(1 for kw in POSITIVE_KEYWORDS if kw in text_all)
    negative_count = sum(1 for kw in NEGATIVE_KEYWORDS if kw in text_all)

    # åˆ¤æ–·é€±è¶¨å‹¢å’Œç‡ˆè™Ÿ
    if positive_count > negative_count * 1.5:
        trend = "ğŸ“ˆåå¤š"
        light = "ğŸŸ¢"
    elif negative_count > positive_count * 1.5:
        trend = "ğŸ“‰åç©º"
        light = "ğŸ”´"
    else:
        trend = "â¡ï¸ä¸­æ€§"
        light = "ğŸŸ¡"

    # å„ç”¢æ¥­çš„çµè«–æ¨¡æ¿
    category_conclusions = {
        # ç”¢æ¥­æ¿å¡Š
        "åŠå°é«”": {
            "ğŸŸ¢": "æ™¶ç‰‡éœ€æ±‚å›æº«ï¼Œåº«å­˜å»åŒ–é †åˆ©ï¼Œç”¢æ¥­æ™¯æ°£å›å‡",
            "ğŸ”´": "çµ‚ç«¯éœ€æ±‚ç–²è»Ÿï¼Œåº«å­˜å£“åŠ›ä»åœ¨ï¼ŒçŸ­æœŸæ‰¿å£“",
            "ğŸŸ¡": "æ™¯æ°£èƒ½è¦‹åº¦ä¸æ˜ï¼Œç­‰å¾…éœ€æ±‚å›å‡è¨Šè™Ÿ",
        },
        "è»Ÿé«”/é›²ç«¯": {
            "ğŸŸ¢": "ä¼æ¥­ITæ”¯å‡ºæˆé•·ï¼Œé›²ç«¯è½‰å‹è¶¨å‹¢å»¶çºŒ",
            "ğŸ”´": "ä¼æ¥­ç¸®æ¸›æ”¯å‡ºï¼Œæˆé•·å‹•èƒ½æ”¾ç·©",
            "ğŸŸ¡": "æ”¯å‡ºæ…‹åº¦ä¿å®ˆï¼Œèšç„¦AIç›¸é—œæŠ•è³‡",
        },
        "ç¶²è·¯/ç¤¾ç¾¤": {
            "ğŸŸ¢": "å»£å‘Šå¸‚å ´å¾©ç”¦ï¼Œç”¨æˆ¶æˆé•·ç©©å¥",
            "ğŸ”´": "å»£å‘Šæ”¯å‡ºæ”¶ç¸®ï¼Œç«¶çˆ­åŠ åŠ‡",
            "ğŸŸ¡": "å»£å‘Šå¸‚å ´åˆ†åŒ–ï¼Œå¹³å°è¡¨ç¾ä¸ä¸€",
        },
        "ç¡¬é«”/æ¶ˆè²»é›»å­": {
            "ğŸŸ¢": "æ¶ˆè²»éœ€æ±‚å›æº«ï¼Œæ–°å“å¸¶å‹•æ›æ©Ÿæ½®",
            "ğŸ”´": "æ¶ˆè²»åŠ›é“ç–²å¼±ï¼Œåº«å­˜èª¿æ•´ä¸­",
            "ğŸŸ¡": "éœ€æ±‚å¹³ç©©ï¼Œç­‰å¾…æ–°å“é€±æœŸå•Ÿå‹•",
        },
        "AIäººå·¥æ™ºæ…§": {
            "ğŸŸ¢": "AIæ‡‰ç”¨åŠ é€Ÿè½åœ°ï¼ŒæŠ•è³‡ç†±åº¦ä¸æ¸›",
            "ğŸ”´": "AIè®Šç¾ç–‘æ…®æµ®ç¾ï¼Œä¼°å€¼é¢è‡¨ä¿®æ­£",
            "ğŸŸ¡": "AIç™¼å±•æŒçºŒï¼Œä½†æŠ•è³‡å›å ±å¾…é©—è­‰",
        },
        "é‡‘è": {
            "ğŸŸ¢": "åˆ©å·®æ“´å¤§ã€è³‡ç”¢å“è³ªç©©å¥ï¼Œç²åˆ©æˆé•·",
            "ğŸ”´": "ä¿¡ç”¨é¢¨éšªå‡æº«ï¼Œæ·¨åˆ©å·®æ”¶çª„",
            "ğŸŸ¡": "åˆ©ç‡ç’°å¢ƒä¸ç¢ºå®šï¼Œé‡‘èè‚¡è§€æœ›",
        },
        "é†«ç™‚ä¿å¥": {
            "ğŸŸ¢": "æ–°è—¥é€²å±•é †åˆ©ï¼Œé†«ç™‚éœ€æ±‚ç©©å®šæˆé•·",
            "ğŸ”´": "è—¥åƒ¹å£“åŠ›ã€è‡¨åºŠå¤±æ•—ï¼Œç”¢æ¥­æ‰¿å£“",
            "ğŸŸ¡": "é˜²ç¦¦ç‰¹æ€§é¡¯ç¾ï¼Œè¡¨ç¾ç›¸å°ç©©å¥",
        },
        "èƒ½æº": {
            "ğŸŸ¢": "æ²¹åƒ¹èµ°å¼·ï¼Œèƒ½æºè‚¡ç²åˆ©æ”¹å–„",
            "ğŸ”´": "æ²¹åƒ¹èµ°å¼±ï¼Œç²åˆ©é¢è‡¨å£“ç¸®",
            "ğŸŸ¡": "æ²¹åƒ¹éœ‡ç›ªï¼Œé—œæ³¨OPECæ”¿ç­–å‹•å‘",
        },
        "æ±½è»Š": {
            "ğŸŸ¢": "è»Šå¸‚éœ€æ±‚å›å‡ï¼Œé›»å‹•è»Šæ»²é€ç‡æé«˜",
            "ğŸ”´": "éœ€æ±‚æ”¾ç·©ï¼Œåƒ¹æ ¼æˆ°å£“ç¸®åˆ©æ½¤",
            "ğŸŸ¡": "å‚³çµ±è»Šç©©å®šï¼Œé›»å‹•è»Šç«¶çˆ­åŠ åŠ‡",
        },
        "é›¶å”®/æ¶ˆè²»": {
            "ğŸŸ¢": "æ¶ˆè²»ä¿¡å¿ƒå›å‡ï¼Œé›¶å”®éŠ·å”®æˆé•·",
            "ğŸ”´": "æ¶ˆè²»åŠ›é“è½‰å¼±ï¼Œåº«å­˜å£“åŠ›ä¸Šå‡",
            "ğŸŸ¡": "æ¶ˆè²»åˆ†åŒ–ï¼Œå¿…éœ€å“å„ªæ–¼éå¿…éœ€å“",
        },
        "èˆªç©º/é‹è¼¸": {
            "ğŸŸ¢": "æ—…éŠéœ€æ±‚å¼·å‹ï¼Œé‹åƒ¹ç¶­æŒé«˜æª”",
            "ğŸ”´": "éœ€æ±‚æ”¾ç·©ï¼Œé‹åƒ¹èµ°è·Œ",
            "ğŸŸ¡": "é‹è¼¸éœ€æ±‚å¹³ç©©ï¼Œé—œæ³¨ç‡ƒæ²¹æˆæœ¬",
        },
        "é€šè¨Šæœå‹™": {
            "ğŸŸ¢": "5Gç”¨æˆ¶æˆé•·ï¼ŒARPUæå‡",
            "ğŸ”´": "ç«¶çˆ­æ¿€çƒˆï¼Œç”¨æˆ¶æˆé•·è¶¨ç·©",
            "ğŸŸ¡": "ç”¢æ¥­æˆç†Ÿï¼Œè‚¡åˆ©æ®–åˆ©ç‡å…·å¸å¼•åŠ›",
        },
        "å·¥æ¥­": {
            "ğŸŸ¢": "è£½é€ æ¥­å¾©ç”¦ï¼ŒåŸºå»ºæŠ•è³‡å¢åŠ ",
            "ğŸ”´": "è¨‚å–®ä¸‹æ»‘ï¼Œæ™¯æ°£å¾ªç’°å‘ä¸‹",
            "ğŸŸ¡": "è£½é€ æ¥­æŒå¹³ï¼Œç­‰å¾…æ”¿ç­–åˆºæ¿€",
        },
        "å…¬ç”¨äº‹æ¥­": {
            "ğŸŸ¢": "ç›£ç®¡ç’°å¢ƒå‹å–„ï¼Œé›»åƒ¹èª¿æ¼²åæ˜ æˆæœ¬",
            "ğŸ”´": "åˆ©ç‡ä¸Šå‡å¢åŠ èè³‡æˆæœ¬",
            "ğŸŸ¡": "é˜²ç¦¦ç‰¹æ€§é¡¯ç¾ï¼Œé©åˆé¿éšªé…ç½®",
        },
        "åŸºç¤ææ–™": {
            "ğŸŸ¢": "åŸç‰©æ–™åƒ¹æ ¼ä¸Šæ¼²ï¼Œç”¢æ¥­ç²åˆ©æ”¹å–„",
            "ğŸ”´": "éœ€æ±‚ç–²è»Ÿï¼ŒåŸç‰©æ–™åƒ¹æ ¼èµ°è·Œ",
            "ğŸŸ¡": "åŸç‰©æ–™åƒ¹æ ¼éœ‡ç›ªï¼Œé—œæ³¨ä¸­åœ‹éœ€æ±‚",
        },
        "é‹¼éµ/çŸ³åŒ–/æ°´æ³¥": {
            "ğŸŸ¢": "ç‡Ÿå»ºéœ€æ±‚å›å‡ï¼Œå ±åƒ¹èµ°æš",
            "ğŸ”´": "å…§éœ€ä¸æŒ¯ï¼Œå ±åƒ¹æŒçºŒèµ°è·Œ",
            "ğŸŸ¡": "å‚³ç”¢æ™¯æ°£å¹³æ·¡ï¼Œç­‰å¾…éœ€æ±‚å›æº«",
        },
        "æˆ¿åœ°ç”¢": {
            "ğŸŸ¢": "æˆ¿å¸‚å›æº«ï¼Œäº¤æ˜“é‡å¢åŠ ",
            "ğŸ”´": "é«˜åˆ©ç‡è¡æ“Šï¼Œæˆ¿å¸‚é™æº«",
            "ğŸŸ¡": "æˆ¿å¸‚è§€æœ›ï¼Œç­‰å¾…åˆ©ç‡æ–¹å‘æ˜æœ—",
        },
        "åŠ å¯†è²¨å¹£": {
            "ğŸŸ¢": "å¸‚å ´æƒ…ç·’æ¨‚è§€ï¼Œè³‡é‡‘æŒçºŒæµå…¥",
            "ğŸ”´": "ç›£ç®¡ç–‘æ…®ã€å¸‚å ´ææ…Œï¼Œåƒ¹æ ¼ä¸‹è·Œ",
            "ğŸŸ¡": "åƒ¹æ ¼ç›¤æ•´ï¼Œç­‰å¾…çªç ´æ–¹å‘",
        },
        # ç§‘æŠ€ç”¢æ¥­éˆ
        "AIæ™¶ç‰‡": {
            "ğŸŸ¢": "AIç®—åŠ›éœ€æ±‚çˆ†ç™¼ï¼Œä¾›ä¸æ‡‰æ±‚",
            "ğŸ”´": "éœ€æ±‚æˆé•·ç–‘æ…®ï¼Œåº«å­˜é¢¨éšªæµ®ç¾",
            "ğŸŸ¡": "éœ€æ±‚ç¶­æŒé«˜æª”ï¼Œä½†æˆé•·è¶¨ç·©",
        },
        "è¨˜æ†¶é«”": {
            "ğŸŸ¢": "HBMéœ€æ±‚å¼·å‹ï¼Œåƒ¹æ ¼æ­¢è·Œå›å‡",
            "ğŸ”´": "ä¾›éæ–¼æ±‚ï¼Œåƒ¹æ ¼æŒçºŒä¸‹è·Œ",
            "ğŸŸ¡": "å‚³çµ±è¨˜æ†¶é«”ç–²è»Ÿï¼ŒHBMç¨å¼·",
        },
        "æ™¶åœ“ä»£å·¥": {
            "ğŸŸ¢": "å…ˆé€²è£½ç¨‹æ»¿è¼‰ï¼Œç”¢èƒ½ä¾›ä¸æ‡‰æ±‚",
            "ğŸ”´": "ç¨¼å‹•ç‡ä¸‹æ»‘ï¼Œåƒ¹æ ¼é¢è‡¨å£“åŠ›",
            "ğŸŸ¡": "å…ˆé€²è£½ç¨‹ç©©å¥ï¼Œæˆç†Ÿè£½ç¨‹èª¿æ•´",
        },
        "å°æ¸¬": {
            "ğŸŸ¢": "å…ˆé€²å°è£éœ€æ±‚å¼·ï¼Œç”¢èƒ½åƒç·Š",
            "ğŸ”´": "å‚³çµ±å°æ¸¬éœ€æ±‚å¼±ï¼Œç¨¼å‹•ç‡ä¸‹æ»‘",
            "ğŸŸ¡": "CoWoSç”¢èƒ½æ“´å……ä¸­ï¼Œå‚³çµ±å°æ¸¬æŒå¹³",
        },
        "ICè¨­è¨ˆ": {
            "ğŸŸ¢": "æ–°å“æ‹‰è²¨å•Ÿå‹•ï¼Œç‡Ÿæ”¶å‹•èƒ½å›å‡",
            "ğŸ”´": "åº«å­˜èª¿æ•´æœªå®Œï¼Œéœ€æ±‚èƒ½è¦‹åº¦ä½",
            "ğŸŸ¡": "æ‰‹æ©Ÿéœ€æ±‚å¹³æ·¡ï¼Œç­‰å¾…æ—ºå­£æ‹‰è²¨",
        },
        "ä¼ºæœå™¨/è³‡æ–™ä¸­å¿ƒ": {
            "ğŸŸ¢": "AIä¼ºæœå™¨éœ€æ±‚çˆ†ç™¼ï¼Œè¨‚å–®èƒ½è¦‹åº¦é«˜",
            "ğŸ”´": "å‚³çµ±ä¼ºæœå™¨éœ€æ±‚ç–²å¼±",
            "ğŸŸ¡": "AIä¼ºæœå™¨ç¨å¼·ï¼Œå‚³çµ±ä¼ºæœå™¨å¹³æ·¡",
        },
        "ç¶²é€šè¨­å‚™": {
            "ğŸŸ¢": "è³‡æ–™ä¸­å¿ƒå‡ç´šå¸¶å‹•ç¶²é€šéœ€æ±‚",
            "ğŸ”´": "ä¼æ¥­æ”¯å‡ºç¸®æ¸›ï¼Œéœ€æ±‚æ”¾ç·©",
            "ğŸŸ¡": "400G/800Gå‡ç´šè¶¨å‹¢æŒçºŒ",
        },
        "PCB/æ•£ç†±": {
            "ğŸŸ¢": "AIä¼ºæœå™¨å¸¶å‹•é«˜éšPCB/æ•£ç†±éœ€æ±‚",
            "ğŸ”´": "æ¶ˆè²»æ€§é›»å­éœ€æ±‚ç–²å¼±",
            "ğŸŸ¡": "AIç›¸é—œå¼·å‹ï¼Œå‚³çµ±æ‡‰ç”¨å¹³æ·¡",
        },
        "é›»æºä¾›æ‡‰": {
            "ğŸŸ¢": "AIä¼ºæœå™¨é›»æºéœ€æ±‚å¤§å¢",
            "ğŸ”´": "å‚³çµ±PC/NBéœ€æ±‚ç–²è»Ÿ",
            "ğŸŸ¡": "é«˜ç“¦æ•¸é›»æºéœ€æ±‚æˆé•·ï¼Œä½ç“¦æ•¸å¹³æ·¡",
        },
        "é¢æ¿/é¡¯ç¤º": {
            "ğŸŸ¢": "é¢æ¿å ±åƒ¹æ­¢è·Œå›å‡ï¼Œåº«å­˜å¥åº·",
            "ğŸ”´": "ä¾›éæ–¼æ±‚ï¼Œé¢æ¿åƒ¹æ ¼æŒçºŒä¸‹è·Œ",
            "ğŸŸ¡": "å¤§å°ºå¯¸ç©©å®šï¼Œä¸­å°å°ºå¯¸ç«¶çˆ­æ¿€çƒˆ",
        },
        "æ‰‹æ©Ÿä¾›æ‡‰éˆ": {
            "ğŸŸ¢": "æ–°æ©Ÿå‚™è²¨å•Ÿå‹•ï¼Œä¾›æ‡‰éˆå—æƒ ",
            "ğŸ”´": "æ‰‹æ©ŸéŠ·å”®ä¸æŒ¯ï¼Œä¾›æ‡‰éˆæ‰¿å£“",
            "ğŸŸ¡": "æ——è‰¦æ©Ÿç©©å®šï¼Œä¸­ä½éšç«¶çˆ­æ¿€çƒˆ",
        },
        "AIæ‡‰ç”¨/å¹³å°": {
            "ğŸŸ¢": "ä¼æ¥­AIå°å…¥åŠ é€Ÿï¼Œæ‡‰ç”¨è®Šç¾å¯æœŸ",
            "ğŸ”´": "AIå•†æ¥­æ¨¡å¼å¾…é©—è­‰ï¼Œç²åˆ©ç–‘æ…®",
            "ğŸŸ¡": "AIç™¼å±•æŒçºŒï¼Œä½†ä¼°å€¼éœ€æ¶ˆåŒ–",
        },
        "SaaS/é›²æœå‹™": {
            "ğŸŸ¢": "ä¼æ¥­ä¸Šé›²è¶¨å‹¢å»¶çºŒï¼Œè¨‚é–±ç‡Ÿæ”¶æˆé•·",
            "ğŸ”´": "å®¢æˆ¶ç¸®æ¸›é›²ç«¯æ”¯å‡ºï¼Œæˆé•·æ”¾ç·©",
            "ğŸŸ¡": "é›²ç«¯æ”¯å‡ºå„ªåŒ–ï¼Œèšç„¦AIåŠŸèƒ½",
        },
        "ç§‘æŠ€å·¨é ­": {
            "ğŸŸ¢": "AIæŠ•è³‡å¸¶å‹•ç‡Ÿæ”¶æˆé•·ï¼Œç²åˆ©å„ªæ–¼é æœŸ",
            "ğŸ”´": "æˆé•·è¶¨ç·©ï¼ŒAIæŠ•è³‡å›å ±å—è³ªç–‘",
            "ğŸŸ¡": "è²¡å ±åˆ†åŒ–ï¼ŒAIè®Šç¾èƒ½åŠ›æˆé—œéµ",
        },
        "AIåŸºç¤è¨­æ–½": {
            "ğŸŸ¢": "è³‡æœ¬æ”¯å‡ºæŒçºŒæ“´å¼µï¼ŒåŸºå»ºéœ€æ±‚å¼·å‹",
            "ğŸ”´": "æŠ•è³‡æ”¾ç·©ç–‘æ…®ï¼Œè¨‚å–®èƒ½è¦‹åº¦ä¸‹é™",
            "ğŸŸ¡": "é•·æœŸéœ€æ±‚ç¢ºå®šï¼ŒçŸ­æœŸç¯€å¥èª¿æ•´",
        },
    }

    # å–å¾—è©²é¡åˆ¥çš„çµè«–ï¼Œè‹¥ç„¡å‰‡ä½¿ç”¨é€šç”¨æ¨¡æ¿
    if category in category_conclusions:
        summary = category_conclusions[category].get(light, "æœ¬é€±æ¶ˆæ¯ä¸­æ€§ï¼ŒæŒçºŒè§€å¯Ÿ")
    else:
        # é€šç”¨çµè«–
        if light == "ğŸŸ¢":
            summary = "æœ¬é€±æ¶ˆæ¯æ­£é¢ï¼Œç”¢æ¥­å‰æ™¯æ¨‚è§€"
        elif light == "ğŸ”´":
            summary = "æœ¬é€±é¢è‡¨å£“åŠ›ï¼ŒçŸ­æœŸé ˆè¬¹æ…"
        else:
            summary = "æœ¬é€±å¤šç©ºäº¤é›œï¼Œå»ºè­°è§€æœ›"

    return light, summary, trend


def render_category_card(category: str, news_items: list, expanded: bool = False):
    """æ¸²æŸ“åˆ†é¡å¡ç‰‡ï¼ŒåŒ…å«ç‡ˆè™Ÿå’Œä¸€å¥è©±ç¸½çµ"""
    light, score = analyze_sentiment(news_items)
    summary = generate_summary(category, news_items, light)

    # æ¨™é¡Œè¡Œï¼šç‡ˆè™Ÿ + åˆ†é¡ + æ•¸é‡
    header = f"{light} **{category}** ({len(news_items)} å‰‡)"

    with st.expander(header, expanded=expanded):
        # ä¸€å¥è©±ç¸½çµ
        st.markdown(f"**ğŸ“Œ {summary}**")
        st.divider()

        # æ–°èåˆ—è¡¨
        for news in news_items[:5]:
            title = news["title"]
            if len(title) > 80:
                title = title[:80] + "..."
            st.markdown(f"â€¢ {title}")
            if news["content"]:
                content_preview = news["content"][:100] + "..." if len(news["content"]) > 100 else news["content"]
                st.caption(f"  {content_preview}")

        if len(news_items) > 5:
            st.caption(f"... é‚„æœ‰ {len(news_items) - 5} å‰‡ç›¸é—œæ–°è")


def render_summary_page(selected_date: date):
    """æ¸²æŸ“ç¸½çµé é¢"""
    st.title("ğŸ“Š æ–°èç¸½çµ")
    st.markdown(f"**æ—¥æœŸ**: {selected_date.strftime('%Y-%m-%d')}")

    # å–å¾—æ–°èä¸¦å¥—ç”¨ç¯©é¸
    raw_news = get_news_by_date(selected_date)
    ptt_min = st.session_state.get("ptt_min_push", 30)
    exclude_ed = st.session_state.get("exclude_editorial", True)
    news_list = filter_news(raw_news, ptt_min_push=ptt_min, exclude_editorial=exclude_ed)

    # é¡¯ç¤ºç¯©é¸è³‡è¨Š
    filtered_count = len(raw_news) - len(news_list)
    if filtered_count > 0:
        st.caption(f"ğŸ” å·²ç¯©é¸: åŸ {len(raw_news)} ç¯‡ â†’ {len(news_list)} ç¯‡ (éæ¿¾ {filtered_count} ç¯‡)")

    stats = get_news_stats_by_date(selected_date)

    if stats["total_count"] == 0:
        st.warning(f"{selected_date} æ²’æœ‰æ”¶é›†åˆ°æ–°è")
        return

    # çµ±è¨ˆå¡ç‰‡
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("æ–°èç¸½æ•¸", stats["total_count"])
    with col2:
        st.metric("RSS ä¾†æº", stats["by_source_type"].get("rss", 0))
    with col3:
        st.metric("API ä¾†æº", stats["by_source_type"].get("api", 0))
    with col4:
        st.metric("çˆ¬èŸ²ä¾†æº", stats["by_source_type"].get("scraper", 0))

    st.divider()

    # åˆ†é¡æ–°è
    categorized = categorize_news(news_list)

    # ========== ç¸½ç¶“è¶¨å‹¢ ==========
    st.header("ğŸ“ˆ ç¸½ç¶“è¶¨å‹¢")

    # å›ºå®šåˆ†é¡é †åºé¡¯ç¤º
    macro_news = categorized["macro"]

    # ç¸½è¦½è¡¨æ ¼ - å›ºå®šé †åºï¼Œåˆ†æˆäº‹å¯¦èˆ‡é æœŸå…©æ¬„
    st.markdown("#### å¿«é€Ÿç¸½è¦½")
    overview_data = []
    for category in MACRO_KEYWORDS.keys():
        news_items = macro_news.get(category, [])
        if news_items:
            light, _ = analyze_sentiment(news_items)
            dual = generate_dual_summary(category, news_items)
        else:
            light = "âšª"  # ç„¡è³‡æ–™ç”¨ç°è‰²
            dual = {"facts": "â€”", "expectations": "â€”"}
        overview_data.append({
            "ç‡ˆè™Ÿ": light,
            "åˆ†é¡": category,
            "ğŸ“‹ ç¢ºèªäº‹å¯¦": dual["facts"],
            "ğŸ”® å¸‚å ´é æœŸ": dual["expectations"],
            "æ–°èæ•¸": len(news_items)
        })

    df_overview = pd.DataFrame(overview_data)
    # è¨­å®šæ¬„ä½å¯¬åº¦é¿å…ç ´ç‰ˆ
    st.dataframe(
        df_overview,
        use_container_width=True,
        hide_index=True,
        column_config={
            "ç‡ˆè™Ÿ": st.column_config.TextColumn("ç‡ˆè™Ÿ", width="small"),
            "åˆ†é¡": st.column_config.TextColumn("åˆ†é¡", width="small"),
            "ğŸ“‹ ç¢ºèªäº‹å¯¦": st.column_config.TextColumn("ğŸ“‹ ç¢ºèªäº‹å¯¦", width="large"),
            "ğŸ”® å¸‚å ´é æœŸ": st.column_config.TextColumn("ğŸ”® å¸‚å ´é æœŸ", width="medium"),
            "æ–°èæ•¸": st.column_config.NumberColumn("æ–°èæ•¸", width="small"),
        }
    )

    st.markdown("#### è©³ç´°å…§å®¹")
    for category in MACRO_KEYWORDS.keys():
        news_items = macro_news.get(category, [])
        if news_items:
            render_category_card(category, news_items, expanded=False)
        else:
            with st.expander(f"âšª **{category}** (0 å‰‡)", expanded=False):
                st.caption("ä»Šæ—¥ç„¡ç›¸é—œæ–°è")

    st.divider()

    # ========== ç”¢æ¥­æ¿å¡Š ==========
    st.header("ğŸ­ ç”¢æ¥­æ¿å¡Š")
    st.caption("ğŸ’¡ ç¸½çµåŸºæ–¼éå»ä¸€é€±æ–°èè¶¨å‹¢åˆ†æï¼Œé¿å…å–®æ—¥æ–°èå½±éŸ¿åˆ¤æ–·")

    # å–å¾—éå»ä¸€é€±æ–°èç”¨æ–¼è¶¨å‹¢åˆ†æ (å¥—ç”¨ç¯©é¸)
    raw_weekly = get_weekly_news(selected_date, days=7)
    weekly_news_list = filter_news(raw_weekly, ptt_min_push=ptt_min, exclude_editorial=exclude_ed)
    weekly_categorized = categorize_news(weekly_news_list)
    weekly_industry_news = weekly_categorized["industry"]

    industry_news = categorized["industry"]  # ä»Šæ—¥æ–°è

    # ç¸½è¦½è¡¨æ ¼ - å›ºå®šé †åºï¼Œä½¿ç”¨é€±è¶¨å‹¢
    st.markdown("#### å¿«é€Ÿç¸½è¦½ (é€±è¶¨å‹¢)")
    overview_data = []
    for category in INDUSTRY_KEYWORDS.keys():
        daily_items = industry_news.get(category, [])
        weekly_items = weekly_industry_news.get(category, [])

        if weekly_items:
            light, summary, trend = generate_weekly_summary(category, weekly_items, len(daily_items))
        else:
            light = "âšª"
            summary = "æœ¬é€±ç„¡ç›¸é—œæ–°è"
            trend = "â€”"

        overview_data.append({
            "ç‡ˆè™Ÿ": light,
            "åˆ†é¡": category,
            "é€±è¶¨å‹¢": trend,
            "ç¸½çµ": summary,
            "ä»Šæ—¥": len(daily_items),
            "æœ¬é€±": len(weekly_items)
        })

    df_overview = pd.DataFrame(overview_data)
    st.dataframe(
        df_overview,
        use_container_width=True,
        hide_index=True,
        column_config={
            "ç‡ˆè™Ÿ": st.column_config.TextColumn("ç‡ˆè™Ÿ", width="small"),
            "åˆ†é¡": st.column_config.TextColumn("åˆ†é¡", width="small"),
            "é€±è¶¨å‹¢": st.column_config.TextColumn("é€±è¶¨å‹¢", width="small"),
            "ç¸½çµ": st.column_config.TextColumn("ç¸½çµ", width="large"),
            "ä»Šæ—¥": st.column_config.NumberColumn("ä»Šæ—¥", width="small"),
            "æœ¬é€±": st.column_config.NumberColumn("æœ¬é€±", width="small"),
        }
    )

    st.markdown("#### è©³ç´°å…§å®¹ (ä»Šæ—¥æ–°è)")
    col_left, col_right = st.columns(2)
    for i, category in enumerate(INDUSTRY_KEYWORDS.keys()):
        news_items = industry_news.get(category, [])
        weekly_items = weekly_industry_news.get(category, [])
        with (col_left if i % 2 == 0 else col_right):
            if news_items:
                render_category_card(category, news_items, expanded=False)
            else:
                weekly_count = len(weekly_items)
                with st.expander(f"âšª **{category}** (ä»Šæ—¥ 0 å‰‡ / é€± {weekly_count} å‰‡)", expanded=False):
                    st.caption("ä»Šæ—¥ç„¡ç›¸é—œæ–°è" if weekly_count == 0 else f"ä»Šæ—¥ç„¡æ–°èï¼Œæœ¬é€±å…± {weekly_count} å‰‡")

    st.divider()

    # ========== ç§‘æŠ€ç”¢æ¥­éˆ ==========
    st.header("ğŸ”— ç§‘æŠ€ç”¢æ¥­éˆ")
    st.caption("ğŸ’¡ ç¸½çµåŸºæ–¼éå»ä¸€é€±æ–°èè¶¨å‹¢åˆ†æ")

    weekly_tech_news = weekly_categorized["tech_supply_chain"]
    tech_supply_chain_news = categorized["tech_supply_chain"]  # ä»Šæ—¥æ–°è

    # ç¸½è¦½è¡¨æ ¼ - å›ºå®šé †åºï¼Œä½¿ç”¨é€±è¶¨å‹¢
    st.markdown("#### å¿«é€Ÿç¸½è¦½ (é€±è¶¨å‹¢)")
    overview_data = []
    for category in TECH_SUPPLY_CHAIN_KEYWORDS.keys():
        daily_items = tech_supply_chain_news.get(category, [])
        weekly_items = weekly_tech_news.get(category, [])

        if weekly_items:
            light, summary, trend = generate_weekly_summary(category, weekly_items, len(daily_items))
        else:
            light = "âšª"
            summary = "æœ¬é€±ç„¡ç›¸é—œæ–°è"
            trend = "â€”"

        overview_data.append({
            "ç‡ˆè™Ÿ": light,
            "åˆ†é¡": category,
            "é€±è¶¨å‹¢": trend,
            "ç¸½çµ": summary,
            "ä»Šæ—¥": len(daily_items),
            "æœ¬é€±": len(weekly_items)
        })

    df_overview = pd.DataFrame(overview_data)
    st.dataframe(
        df_overview,
        use_container_width=True,
        hide_index=True,
        column_config={
            "ç‡ˆè™Ÿ": st.column_config.TextColumn("ç‡ˆè™Ÿ", width="small"),
            "åˆ†é¡": st.column_config.TextColumn("åˆ†é¡", width="small"),
            "é€±è¶¨å‹¢": st.column_config.TextColumn("é€±è¶¨å‹¢", width="small"),
            "ç¸½çµ": st.column_config.TextColumn("ç¸½çµ", width="large"),
            "ä»Šæ—¥": st.column_config.NumberColumn("ä»Šæ—¥", width="small"),
            "æœ¬é€±": st.column_config.NumberColumn("æœ¬é€±", width="small"),
        }
    )

    st.markdown("#### è©³ç´°å…§å®¹ (ä»Šæ—¥æ–°è)")
    # ä½¿ç”¨ä¸‰æ¬„é¡¯ç¤ºï¼ˆå› ç‚ºåˆ†é¡è¼ƒå¤šï¼‰
    col1, col2, col3 = st.columns(3)
    cols = [col1, col2, col3]
    for i, category in enumerate(TECH_SUPPLY_CHAIN_KEYWORDS.keys()):
        daily_items = tech_supply_chain_news.get(category, [])
        weekly_items = weekly_tech_news.get(category, [])
        with cols[i % 3]:
            if daily_items:
                render_category_card(category, daily_items, expanded=False)
            else:
                weekly_count = len(weekly_items)
                with st.expander(f"âšª **{category}** (ä»Šæ—¥ 0 å‰‡ / é€± {weekly_count} å‰‡)", expanded=False):
                    st.caption("ä»Šæ—¥ç„¡ç›¸é—œæ–°è" if weekly_count == 0 else f"ä»Šæ—¥ç„¡æ–°èï¼Œæœ¬é€±å…± {weekly_count} å‰‡")

    st.divider()

    # ========== æ•¸æ“šåœ–è¡¨ ==========
    st.header("ğŸ“Š æ•¸æ“šåˆ†æ")

    col_left, col_right = st.columns(2)

    with col_left:
        st.subheader("æ–°èä¾†æºåˆ†ä½ˆ")
        if stats["by_source"]:
            df = pd.DataFrame(
                list(stats["by_source"].items()),
                columns=["ä¾†æº", "æ•¸é‡"]
            )
            st.bar_chart(df.set_index("ä¾†æº"))

    with col_right:
        st.subheader("ç†±é–€é—œéµè©")
        all_titles = " ".join([n["title"] for n in news_list])

        keywords = {
            "AI": all_titles.lower().count("ai") + all_titles.lower().count("artificial intelligence"),
            "Fed": all_titles.lower().count("fed"),
            "Trump": all_titles.lower().count("trump"),
            "Gold": all_titles.lower().count("gold"),
            "Tesla": all_titles.lower().count("tesla"),
            "Earnings": all_titles.lower().count("earning"),
            "Tariff": all_titles.lower().count("tariff"),
            "Market": all_titles.lower().count("market"),
            "Economy": all_titles.lower().count("econom"),
            "Rate": all_titles.lower().count("rate"),
        }
        keywords = {k: v for k, v in keywords.items() if v > 0}

        if keywords:
            sorted_kw = sorted(keywords.items(), key=lambda x: x[1], reverse=True)[:8]
            df_kw = pd.DataFrame(sorted_kw, columns=["é—œéµè©", "å‡ºç¾æ¬¡æ•¸"])
            st.bar_chart(df_kw.set_index("é—œéµè©"))


def render_news_list_page(selected_date: date):
    """æ¸²æŸ“æ–°èåˆ—è¡¨é é¢"""
    st.title("ğŸ“° æ–°èåˆ—è¡¨")
    st.markdown(f"**æ—¥æœŸ**: {selected_date.strftime('%Y-%m-%d')}")

    # å–å¾—æ–°èä¸¦å¥—ç”¨ç¯©é¸
    raw_news = get_news_by_date(selected_date)
    ptt_min = st.session_state.get("ptt_min_push", 30)
    exclude_ed = st.session_state.get("exclude_editorial", True)
    news_list = filter_news(raw_news, ptt_min_push=ptt_min, exclude_editorial=exclude_ed)

    # é¡¯ç¤ºç¯©é¸è³‡è¨Š
    filtered_count = len(raw_news) - len(news_list)
    if filtered_count > 0:
        st.caption(f"ğŸ” å·²ç¯©é¸: åŸ {len(raw_news)} ç¯‡ â†’ {len(news_list)} ç¯‡ (éæ¿¾ {filtered_count} ç¯‡)")

    if not news_list:
        st.warning(f"{selected_date} æ²’æœ‰ç¬¦åˆç¯©é¸æ¢ä»¶çš„æ–°è")
        return

    col1, col2 = st.columns(2)
    with col1:
        sources = ["å…¨éƒ¨"] + sorted(list(set(n["source"] for n in news_list if n["source"])))
        selected_source = st.selectbox("ä¾†æºç¯©é¸", sources)
    with col2:
        source_types = ["å…¨éƒ¨"] + sorted(list(set(n["source_type"] for n in news_list if n["source_type"])))
        selected_type = st.selectbox("é¡å‹ç¯©é¸", source_types)

    search_term = st.text_input("ğŸ” æœå°‹æ¨™é¡Œ", "")

    filtered_news = news_list
    if selected_source != "å…¨éƒ¨":
        filtered_news = [n for n in filtered_news if n["source"] == selected_source]
    if selected_type != "å…¨éƒ¨":
        filtered_news = [n for n in filtered_news if n["source_type"] == selected_type]
    if search_term:
        filtered_news = [n for n in filtered_news if search_term.lower() in n["title"].lower()]

    st.markdown(f"å…± **{len(filtered_news)}** å‰‡æ–°è")
    st.divider()

    for news in filtered_news:
        with st.expander(f"**{news['title'][:80]}{'...' if len(news['title']) > 80 else ''}**", expanded=False):
            col1, col2 = st.columns([3, 1])
            with col1:
                st.markdown(f"**ä¾†æº**: {news['source']} ({news['source_type']})")
                if news["published_at"]:
                    st.markdown(f"**ç™¼å¸ƒæ™‚é–“**: {news['published_at']}")
            with col2:
                if news["url"]:
                    st.link_button("ğŸ”— é–±è®€åŸæ–‡", news["url"])

            if news["content"]:
                st.markdown("**æ‘˜è¦**:")
                st.write(news["content"])


def render_news_detail_page(selected_date: date):
    """æ¸²æŸ“æ–°èè©³æƒ…é é¢"""
    st.title("ğŸ“„ æ–°èè©³æƒ…")
    st.markdown(f"**æ—¥æœŸ**: {selected_date.strftime('%Y-%m-%d')}")

    news_list = get_news_by_date(selected_date)

    if not news_list:
        st.warning(f"{selected_date} æ²’æœ‰æ”¶é›†åˆ°æ–°è")
        return

    news_titles = [f"{n['source']}: {n['title'][:60]}..." for n in news_list]
    selected_idx = st.selectbox(
        "é¸æ“‡æ–°è",
        range(len(news_titles)),
        format_func=lambda x: news_titles[x]
    )

    if selected_idx is not None:
        news = news_list[selected_idx]

        st.divider()
        st.header(news["title"])

        col1, col2, col3 = st.columns(3)
        with col1:
            st.markdown(f"**ä¾†æº**: {news['source']}")
        with col2:
            st.markdown(f"**é¡å‹**: {news['source_type']}")
        with col3:
            st.markdown(f"**åˆ†é¡**: {news['category']}")

        if news["published_at"]:
            st.markdown(f"**ç™¼å¸ƒæ™‚é–“**: {news['published_at']}")

        st.divider()

        if news["content"]:
            st.subheader("å…§å®¹æ‘˜è¦")
            st.write(news["content"])
        else:
            st.info("æ­¤æ–°èæ²’æœ‰æ‘˜è¦å…§å®¹")

        if news["url"]:
            st.divider()
            st.link_button("ğŸ”— é»æ“Šé–±è®€åŸæ–‡", news["url"], use_container_width=True)


def get_ptt_news_by_date(selected_date: date):
    """å–å¾—æŒ‡å®šæ—¥æœŸçš„ PTT æ–‡ç«  - ä½¿ç”¨çµ±ä¸€è³‡æ–™å±¤"""
    try:
        client = _get_data_client()
        # å–å¾—ç•¶å¤©æ–°èä¸¦éæ¿¾ PTT
        news_list = client.get_news(
            start_date=selected_date,
            end_date=selected_date,
            limit=500
        )
        # éæ¿¾å‡º PTT æ–‡ç« 
        ptt_news = [n for n in news_list if n.get("source_type") == "ptt"]
        return ptt_news
    except Exception as e:
        return []


def render_ptt_page(selected_date: date):
    """æ¸²æŸ“ PTT Stock é é¢"""
    st.title("ğŸ‡¹ğŸ‡¼ PTT Stock ç‰ˆ")
    st.markdown(f"**æ—¥æœŸ**: {selected_date.strftime('%Y-%m-%d')}")

    raw_ptt = get_ptt_news_by_date(selected_date)

    if not raw_ptt:
        st.warning(f"{selected_date} æ²’æœ‰ PTT æ–‡ç« ")
        st.info("æç¤ºï¼šåŸ·è¡Œ `python main.py --once` ä¾†æ”¶é›† PTT æ–‡ç« ")
        return

    # å¥—ç”¨æ¨æ–‡æ•¸ç¯©é¸
    ptt_min = st.session_state.get("ptt_min_push", 30)
    ptt_news = filter_news(raw_ptt, ptt_min_push=ptt_min, exclude_editorial=False)

    # é¡¯ç¤ºç¯©é¸è³‡è¨Š
    filtered_count = len(raw_ptt) - len(ptt_news)
    if filtered_count > 0:
        st.caption(f"ğŸ” å·²ç¯©é¸: åŸ {len(raw_ptt)} ç¯‡ â†’ {len(ptt_news)} ç¯‡ (éæ¿¾æ¨æ–‡æ•¸ < {ptt_min} çš„ {filtered_count} ç¯‡)")

    if not ptt_news:
        st.warning(f"æ²’æœ‰ç¬¦åˆç¯©é¸æ¢ä»¶çš„æ–‡ç«  (æ¨æ–‡æ•¸ >= {ptt_min})")
        return

    # çµ±è¨ˆ
    categories = {}
    for news in ptt_news:
        cat = news["category"] or "å…¶ä»–"
        categories[cat] = categories.get(cat, 0) + 1

    # é¡¯ç¤ºçµ±è¨ˆ
    st.markdown(f"å…± **{len(ptt_news)}** å‰‡æ–‡ç«  (æ¨æ–‡æ•¸ >= {ptt_min})")

    cols = st.columns(len(categories))
    for i, (cat, count) in enumerate(sorted(categories.items(), key=lambda x: x[1], reverse=True)):
        with cols[i % len(cols)]:
            st.metric(cat, count)

    st.divider()

    # åˆ†é¡ç¯©é¸
    cat_options = ["å…¨éƒ¨"] + sorted(categories.keys())
    selected_cat = st.selectbox("åˆ†é¡ç¯©é¸", cat_options)

    # æœå°‹
    search_term = st.text_input("ğŸ” æœå°‹æ¨™é¡Œ", "", key="ptt_search")

    # ç¯©é¸
    filtered = ptt_news
    if selected_cat != "å…¨éƒ¨":
        filtered = [n for n in filtered if n["category"] == selected_cat]
    if search_term:
        filtered = [n for n in filtered if search_term.lower() in n["title"].lower()]

    st.markdown(f"é¡¯ç¤º **{len(filtered)}** å‰‡")
    st.divider()

    # æ–‡ç« åˆ—è¡¨
    for news in filtered:
        # å–å¾—æ¨æ–‡æ•¸
        push_info = news["content"] or ""
        push_match = push_info.split("]")[0].replace("[", "") if "]" in push_info else ""

        # é¡è‰²æ¨™è¨˜æ¨æ–‡æ•¸
        if "çˆ†" in push_match:
            push_badge = "ğŸ”¥"
        elif push_match.isdigit() and int(push_match) >= 50:
            push_badge = "ğŸ”¥"
        elif push_match.startswith("X"):
            push_badge = "ğŸ’©"
        else:
            push_badge = ""

        # å–å¾—ç™¼æ–‡æ™‚é–“
        pub_time = ""
        if news["published_at"]:
            try:
                pub_dt = datetime.strptime(news["published_at"], "%Y-%m-%d %H:%M:%S")
                pub_time = pub_dt.strftime("%H:%M")
            except:
                pub_time = ""

        title_display = f"{push_badge} [{news['category']}] {news['title']}"
        if pub_time:
            title_display = f"{pub_time} {title_display}"

        with st.expander(title_display, expanded=False):
            st.markdown(f"**{push_info}**")
            if news["published_at"]:
                st.markdown(f"**ç™¼æ–‡æ™‚é–“**: {news['published_at']}")

            if news["url"]:
                st.link_button("ğŸ”— å‰å¾€ PTT åŸæ–‡", news["url"])


# ========== AI è¶¨å‹¢é›·é”ç³»çµ± ==========
TECH_TRENDS = {
    # ===== AI é‹ç®—å±¤ =====
    "GPU/AIæ™¶ç‰‡": {
        "keywords": ["gpu", "nvidia", "h100", "h200", "b100", "b200", "blackwell", "rubin", "ai chip", "ai accelerator", "grace"],
        "stocks": ["NVDA", "AMD", "INTC", "AVGO", "MRVL"],
        "phase": "æˆç†ŸæœŸ",
        "detail": "Blackwell é‡ç”¢ä¸­ï¼ŒRubin 2026H2 è©¦ç”¢",
    },
    "å®¢è£½åŒ–AIæ™¶ç‰‡": {
        "keywords": ["custom chip", "asic", "tpu", "trainium", "inferentia", "dojo", "willow"],
        "stocks": ["AVGO", "MRVL", "GOOGL", "AMZN"],
        "phase": "æˆé•·æœŸ",
        "detail": "é›²ç«¯å¤§å» è‡ªç ”æ™¶ç‰‡ï¼ŒBroadcom/Marvell ä»£å·¥",
    },
    # ===== è¨˜æ†¶é«”å±¤ =====
    "HBMè¨˜æ†¶é«”": {
        "keywords": ["hbm", "hbm3", "hbm3e", "hbm4", "high bandwidth memory"],
        "stocks": ["MU", "SK Hynix", "Samsung"],
        "phase": "çˆ†ç™¼æœŸ",
        "detail": "HBM4 2026Q1é–‹å§‹å‡ºè²¨ï¼ŒSK Hynix å¸‚ä½”70%",
    },
    "DDR5/LPDDR5": {
        "keywords": ["ddr5", "lpddr5", "memory module", "dram"],
        "stocks": ["MU", "SK Hynix", "Samsung"],
        "phase": "æˆç†ŸæœŸ",
        "detail": "ä¼ºæœå™¨æ›æ©Ÿæ½®å¸¶å‹• DDR5 æ»²é€",
    },
    # ===== å°è£å±¤ =====
    "å…ˆé€²å°è£": {
        "keywords": ["cowos", "advanced packaging", "chiplet", "2.5d", "3d packaging", "interposer", "soic", "emib", "foveros"],
        "stocks": ["TSM", "ASX", "AMAT", "INTC"],
        "phase": "çˆ†ç™¼æœŸ",
        "detail": "CoWoS 2026å¹´åº•é” 13è¬ç‰‡/æœˆ",
    },
    # ===== äº’é€£å±¤ =====
    "çŸ½å…‰å­/CPO": {
        "keywords": ["silicon photonics", "optical interconnect", "co-packaged optics", "cpo", "photonic", "800g", "1.6t", "3.2t", "odin", "optical engine"],
        "stocks": ["LITE", "COHR", "MRVL", "AVGO", "FN"],
        "phase": "çˆ†ç™¼æœŸ",
        "detail": "1.6Té‡ç”¢ï¼ŒCPOå¾å¯¦é©—è½‰å‘å¿…å‚™",
    },
    "é«˜é€Ÿé€£æ¥å™¨": {
        "keywords": ["connector", "high speed", "pcie", "ubb", "nvlink", "ethernet switch"],
        "stocks": ["APH", "TEL", "AVGO"],
        "phase": "æˆé•·æœŸ",
        "detail": "PCIe 6.0/NVLink 5 æ¨å‹•æ›ä»£",
    },
    # ===== æ•£ç†±å±¤ =====
    "æ¶²å†·æ•£ç†±": {
        "keywords": ["liquid cooling", "immersion cooling", "direct liquid", "cold plate", "coolant distribution"],
        "stocks": ["VRT", "CARR", "JCI"],
        "phase": "çˆ†ç™¼æœŸ",
        "detail": "1GWç´šè³‡æ–™ä¸­å¿ƒæ¨™é…æ¶²å†·",
    },
    # ===== é›»åŠ›å±¤ =====
    "é›»åŠ›åŸºç¤è¨­æ–½": {
        "keywords": ["power infrastructure", "data center power", "electricity demand", "grid capacity", "power shortage", "ups", "pdu"],
        "stocks": ["VST", "CEG", "PWR", "ETN", "EMR"],
        "phase": "æˆé•·æœŸ",
        "detail": "800V HVDCæ¶æ§‹æ™®åŠï¼Œé›»åŠ›æˆç“¶é ¸",
    },
    "æ ¸èƒ½å¾©èˆˆ": {
        "keywords": ["nuclear power", "nuclear energy", "smr", "small modular reactor", "uranium", "nuclear renaissance"],
        "stocks": ["CEG", "VST", "CCJ", "NNE", "SMR"],
        "phase": "æ—©æœŸ",
        "detail": "å¾®è»Ÿ/Google/Amazon ç°½æ ¸é›»PPA",
    },
    # ===== é›²ç«¯/å¹³å°å±¤ =====
    "é›²ç«¯AIæœå‹™": {
        "keywords": ["azure ai", "aws", "google cloud", "openai", "anthropic", "cloud ai", "ai infrastructure", "ai spending", "capex"],
        "stocks": ["MSFT", "GOOGL", "AMZN", "ORCL", "META"],
        "phase": "çˆ†ç™¼æœŸ",
        "detail": "Hyperscaler AI CapEx æŒçºŒæ“´å¼µ",
    },
    "AIæ¨¡å‹/å¹³å°": {
        "keywords": ["chatgpt", "gpt-5", "gemini", "claude", "llama", "openai", "anthropic", "foundation model", "large language model", "llm"],
        "stocks": ["MSFT", "GOOGL", "META", "AMZN"],
        "phase": "æˆé•·æœŸ",
        "detail": "GPT-5/Gemini 2.0 ç«¶çˆ­ç™½ç†±åŒ–",
    },
    "AIè³‡æ–™ä¸­å¿ƒ": {
        "keywords": ["ai data center", "hyperscale", "colocation", "data center construction", "ai factory", "gpu cluster"],
        "stocks": ["EQIX", "DLR", "AMT", "MSFT", "GOOGL"],
        "phase": "çˆ†ç™¼æœŸ",
        "detail": "GWç´šAIè³‡æ–™ä¸­å¿ƒå¤§é‡èˆˆå»º",
    },
    # ===== è»Ÿé«”/æ‡‰ç”¨å±¤ =====
    "AI Agent": {
        "keywords": ["ai agent", "autonomous agent", "agentic ai", "copilot", "mcp", "tool use"],
        "stocks": ["MSFT", "GOOGL", "CRM", "NOW", "PATH"],
        "phase": "æˆé•·æœŸ",
        "detail": "2026å¹´ä¼æ¥­AI Agentå¤§è¦æ¨¡éƒ¨ç½²",
    },
    "ä¼æ¥­AIæ‡‰ç”¨": {
        "keywords": ["enterprise ai", "ai saas", "ai software", "ai automation", "workflow ai", "ai analytics"],
        "stocks": ["CRM", "NOW", "WDAY", "SNOW", "PLTR", "PATH"],
        "phase": "æˆé•·æœŸ",
        "detail": "ä¼æ¥­AIè»Ÿé«”è¨‚é–±å¿«é€Ÿæˆé•·",
    },
    "é‚Šç·£AI": {
        "keywords": ["edge ai", "on-device ai", "npu", "qualcomm ai", "apple intelligence", "ai pc", "ai phone"],
        "stocks": ["QCOM", "AAPL", "ARM", "INTC", "AMD"],
        "phase": "æˆé•·æœŸ",
        "detail": "AI PC/Phone æ›æ©Ÿæ½®å•Ÿå‹•",
    },
    # ===== è¨­å‚™å±¤ =====
    "åŠå°é«”è¨­å‚™": {
        "keywords": ["semiconductor equipment", "lithography", "euv", "high na", "etching", "deposition", "inspection"],
        "stocks": ["ASML", "AMAT", "LRCX", "KLAC", "TOELY"],
        "phase": "ç©©å®šæœŸ",
        "detail": "High-NA EUV 2026é‡ç”¢",
    },
    # ===== é¢¨éšª =====
    "åœ°ç·£æ”¿æ²»": {
        "keywords": ["chip ban", "export control", "sanction", "china chip", "huawei", "tariff", "trade war", "entity list"],
        "stocks": [],
        "phase": "é¢¨éšª",
        "detail": "ç¾ä¸­ç§‘æŠ€æˆ°æŒçºŒï¼Œé—œç¨…é¢¨éšª",
    },
}

# é—œéµè‚¡ç¥¨è©³ç´°å°ç…§è¡¨
STOCK_DETAILS = {
    # GPU/AIæ™¶ç‰‡
    "NVDA": {"name": "NVIDIA", "category": "GPU/AIæ™¶ç‰‡", "role": "AIæ™¶ç‰‡é¾é ­ï¼ŒBlackwell/Rubinæ¶æ§‹"},
    "AMD": {"name": "AMD", "category": "GPU/AIæ™¶ç‰‡", "role": "MI300Xç«¶çˆ­è€…ï¼ŒCPU+GPUæ•´åˆ"},
    "INTC": {"name": "Intel", "category": "GPU/AIæ™¶ç‰‡", "role": "GaudiåŠ é€Ÿå™¨ï¼Œæ™¶åœ“ä»£å·¥è½‰å‹"},
    "AVGO": {"name": "Broadcom", "category": "å®¢è£½åŒ–AIæ™¶ç‰‡", "role": "å®¢è£½åŒ–AIæ™¶ç‰‡é¾é ­ï¼ŒGoogle TPUè¨­è¨ˆ"},
    "MRVL": {"name": "Marvell", "category": "å®¢è£½åŒ–AIæ™¶ç‰‡", "role": "é›²ç«¯å®¢è£½æ™¶ç‰‡ï¼Œæ”¶è³¼Celestial AI"},
    # è¨˜æ†¶é«”
    "MU": {"name": "Micron", "category": "HBMè¨˜æ†¶é«”", "role": "HBM3Eä¾›æ‡‰å•†ï¼Œç¾ç³»å”¯ä¸€"},
    # å°è£
    "TSM": {"name": "TSMC", "category": "å…ˆé€²å°è£", "role": "CoWoS/SoICé¾é ­ï¼ŒAIå°è£å¸‚ä½”80%+"},
    "ASX": {"name": "ASE Technology", "category": "å…ˆé€²å°è£", "role": "OSATé¾é ­ï¼Œ2.5D/3Då°è£"},
    # çŸ½å…‰å­
    "LITE": {"name": "Lumentum", "category": "çŸ½å…‰å­/CPO", "role": "é›·å°„/å…‰å­¸å…ƒä»¶ï¼ŒCPOé—œéµä¾›æ‡‰å•†"},
    "COHR": {"name": "Coherent", "category": "çŸ½å…‰å­/CPO", "role": "å…‰å­¸æ¨¡çµ„ï¼Œ800G/1.6Tæ”¶ç™¼å™¨"},
    "FN": {"name": "Fabrinet", "category": "çŸ½å…‰å­/CPO", "role": "å…‰å­¸è¨­å‚™ä»£å·¥"},
    # é€£æ¥å™¨
    "APH": {"name": "Amphenol", "category": "é«˜é€Ÿé€£æ¥å™¨", "role": "é«˜é€Ÿé€£æ¥å™¨é¾é ­ï¼ŒAIä¼ºæœå™¨å¿…å‚™"},
    "TEL": {"name": "TE Connectivity", "category": "é«˜é€Ÿé€£æ¥å™¨", "role": "é€£æ¥å™¨/æ„Ÿæ¸¬å™¨"},
    # æ•£ç†±
    "VRT": {"name": "Vertiv", "category": "æ¶²å†·æ•£ç†±", "role": "è³‡æ–™ä¸­å¿ƒæ¶²å†·é¾é ­"},
    "CARR": {"name": "Carrier Global", "category": "æ¶²å†·æ•£ç†±", "role": "HVAC/æ•£ç†±ç³»çµ±"},
    # é›»åŠ›
    "VST": {"name": "Vistra", "category": "é›»åŠ›åŸºç¤è¨­æ–½", "role": "é›»åŠ›å…¬å¸ï¼Œæ ¸èƒ½è³‡ç”¢"},
    "CEG": {"name": "Constellation Energy", "category": "æ ¸èƒ½å¾©èˆˆ", "role": "ç¾åœ‹æœ€å¤§æ ¸é›»é‹ç‡Ÿå•†"},
    "PWR": {"name": "Quanta Services", "category": "é›»åŠ›åŸºç¤è¨­æ–½", "role": "é›»åŠ›åŸºå»ºå·¥ç¨‹"},
    "ETN": {"name": "Eaton", "category": "é›»åŠ›åŸºç¤è¨­æ–½", "role": "é›»åŠ›ç®¡ç†ï¼ŒUPS/PDU"},
    "CCJ": {"name": "Cameco", "category": "æ ¸èƒ½å¾©èˆˆ", "role": "éˆ¾ç¤¦é¾é ­"},
    "SMR": {"name": "NuScale Power", "category": "æ ¸èƒ½å¾©èˆˆ", "role": "SMRå°å‹æ¨¡çµ„æ ¸é›»"},
    # è¨­å‚™
    "ASML": {"name": "ASML", "category": "åŠå°é«”è¨­å‚™", "role": "EUVå…‰åˆ»æ©Ÿç¨ä½”"},
    "AMAT": {"name": "Applied Materials", "category": "åŠå°é«”è¨­å‚™", "role": "æ²‰ç©/è•åˆ»è¨­å‚™"},
    "LRCX": {"name": "Lam Research", "category": "åŠå°é«”è¨­å‚™", "role": "è•åˆ»è¨­å‚™"},
    "KLAC": {"name": "KLA", "category": "åŠå°é«”è¨­å‚™", "role": "æª¢æ¸¬è¨­å‚™"},
    # è»Ÿé«”
    "MSFT": {"name": "Microsoft", "category": "AI Agent", "role": "Copilotç”Ÿæ…‹ç³»ï¼ŒAzure AI"},
    "GOOGL": {"name": "Google", "category": "AI Agent", "role": "Geminiï¼ŒTPUè‡ªç ”"},
    "CRM": {"name": "Salesforce", "category": "AI Agent", "role": "Agentforceä¼æ¥­AI"},
    "NOW": {"name": "ServiceNow", "category": "AI Agent", "role": "ä¼æ¥­æµç¨‹AIè‡ªå‹•åŒ–"},
    "PATH": {"name": "UiPath", "category": "ä¼æ¥­AIæ‡‰ç”¨", "role": "RPA/æµç¨‹è‡ªå‹•åŒ–é¾é ­"},
    # é‚Šç·£
    "QCOM": {"name": "Qualcomm", "category": "é‚Šç·£AI", "role": "æ‰‹æ©Ÿ/PC NPUé¾é ­"},
    "AAPL": {"name": "Apple", "category": "é‚Šç·£AI", "role": "Apple Intelligenceç”Ÿæ…‹"},
    "ARM": {"name": "ARM Holdings", "category": "é‚Šç·£AI", "role": "CPUæ¶æ§‹æˆæ¬Š"},
    # é›²ç«¯/å¹³å°
    "AMZN": {"name": "Amazon", "category": "é›²ç«¯AIæœå‹™", "role": "AWSé›²ç«¯é¾é ­ï¼ŒBedrock AIå¹³å°"},
    "ORCL": {"name": "Oracle", "category": "é›²ç«¯AIæœå‹™", "role": "OCIé›²ç«¯ï¼Œä¼æ¥­AIè³‡æ–™åº«"},
    "META": {"name": "Meta", "category": "AIæ¨¡å‹/å¹³å°", "role": "Llamaé–‹æºæ¨¡å‹ï¼ŒAIå»£å‘Šæ‡‰ç”¨"},
    # è³‡æ–™ä¸­å¿ƒ
    "EQIX": {"name": "Equinix", "category": "AIè³‡æ–™ä¸­å¿ƒ", "role": "å…¨çƒæœ€å¤§è³‡æ–™ä¸­å¿ƒREIT"},
    "DLR": {"name": "Digital Realty", "category": "AIè³‡æ–™ä¸­å¿ƒ", "role": "è³‡æ–™ä¸­å¿ƒREITï¼ŒHyperscalerå®¢æˆ¶"},
    "AMT": {"name": "American Tower", "category": "AIè³‡æ–™ä¸­å¿ƒ", "role": "é€šè¨Šå¡”/é‚Šç·£è³‡æ–™ä¸­å¿ƒ"},
    # ä¼æ¥­è»Ÿé«”
    "WDAY": {"name": "Workday", "category": "ä¼æ¥­AIæ‡‰ç”¨", "role": "HR/è²¡å‹™SaaSï¼ŒAIåŠ©ç†"},
    "SNOW": {"name": "Snowflake", "category": "ä¼æ¥­AIæ‡‰ç”¨", "role": "é›²ç«¯è³‡æ–™å€‰å„²ï¼ŒAI/MLå¹³å°"},
    "PLTR": {"name": "Palantir", "category": "ä¼æ¥­AIæ‡‰ç”¨", "role": "AIæ•¸æ“šåˆ†æå¹³å°ï¼Œæ”¿åºœ/ä¼æ¥­"},
    # ETF (ç”¨æ–¼2022é˜²ç¦¦é…ç½®)
    "XLE": {"name": "Energy Select ETF", "category": "ETF", "role": "èƒ½æºæ¿å¡ŠETF"},
    "XLF": {"name": "Financial Select ETF", "category": "ETF", "role": "é‡‘èæ¿å¡ŠETF"},
    "XLV": {"name": "Health Care Select ETF", "category": "ETF", "role": "é†«ç™‚æ¿å¡ŠETF"},
    "XLU": {"name": "Utilities Select ETF", "category": "ETF", "role": "å…¬ç”¨äº‹æ¥­ETF"},
    "SHY": {"name": "iShares 1-3Y Treasury", "category": "ETF", "role": "çŸ­æœŸåœ‹å‚µETF"},
    # é˜²ç¦¦è‚¡
    "JPM": {"name": "JPMorgan Chase", "category": "é‡‘è", "role": "ç¾åœ‹æœ€å¤§éŠ€è¡Œ"},
    "JNJ": {"name": "Johnson & Johnson", "category": "é†«ç™‚", "role": "é†«ç™‚ä¿å¥é¾é ­"},
    "PG": {"name": "Procter & Gamble", "category": "å¿…éœ€æ¶ˆè²»", "role": "æ¶ˆè²»å“é¾é ­"},
    "COST": {"name": "Costco", "category": "å¿…éœ€æ¶ˆè²»", "role": "æœƒå“¡åˆ¶é›¶å”®"},
}

# 2026 Q1 æŠ€è¡“é æ¸¬
Q1_2026_FORECAST = {
    "GPU/AIæ™¶ç‰‡": {
        "status": "ğŸŸ¢ é‡ç”¢",
        "milestone": "Blackwell B200 å…¨é¢é‡ç”¢ï¼ŒRubin R100 é€²å…¥è©¦ç”¢",
        "bottleneck": "CoWoSå°è£ç”¢èƒ½ä»ç·Š",
        "catalyst": "NVIDIA GTC 2026 (3æœˆ)",
    },
    "HBMè¨˜æ†¶é«”": {
        "status": "ğŸ”¥ çˆ†ç™¼",
        "milestone": "HBM4 é–‹å§‹å‡ºè²¨ï¼Œé »å¯¬é” 2TB/s",
        "bottleneck": "HBM4 è‰¯ç‡çˆ¬å¡ä¸­",
        "catalyst": "SK Hynix HBM4 é‡ç”¢å®£å¸ƒ",
    },
    "å…ˆé€²å°è£": {
        "status": "ğŸ”¥ çˆ†ç™¼",
        "milestone": "CoWoSæœˆç”¢èƒ½é”10è¬ç‰‡ï¼ŒCoWoS-Lé‡ç”¢",
        "bottleneck": "ABFè¼‰æ¿ä¾›æ‡‰",
        "catalyst": "TSMCæ³•èªªæœƒ (1æœˆ)",
    },
    "çŸ½å…‰å­/CPO": {
        "status": "ğŸš€ è½‰æŠ˜é»",
        "milestone": "1.6Tæ¨¡çµ„é‡ç”¢ï¼ŒCPOå¾å¯¦é©—è½‰å¿…å‚™",
        "bottleneck": "InPé›·å°„ä¾›æ‡‰",
        "catalyst": "OFC 2026 (3æœˆ)",
    },
    "æ¶²å†·æ•£ç†±": {
        "status": "ğŸŸ¢ æˆé•·",
        "milestone": "æ¶²å†·æ»²é€ç‡é”40%+",
        "bottleneck": "å®¢è£½åŒ–è¨­è¨ˆé€±æœŸ",
        "catalyst": "æ–°è³‡æ–™ä¸­å¿ƒæ¨™æ¡ˆ",
    },
    "é›»åŠ›åŸºç¤è¨­æ–½": {
        "status": "âš ï¸ ç“¶é ¸",
        "milestone": "800V HVDCæˆæ–°æ¨™æº–",
        "bottleneck": "é›»ç¶²å®¹é‡ä¸è¶³",
        "catalyst": "æ ¸é›»PPAç°½ç´„æ¶ˆæ¯",
    },
    "AI Agent": {
        "status": "ğŸŒ± æ—©æœŸ",
        "milestone": "ä¼æ¥­Agentå¤§è¦æ¨¡POC",
        "bottleneck": "å¯é æ€§/å®‰å…¨æ€§",
        "catalyst": "å¾®è»Ÿ/Salesforceç”¢å“ç™¼å¸ƒ",
    },
    "é›²ç«¯AIæœå‹™": {
        "status": "ğŸ”¥ çˆ†ç™¼",
        "milestone": "AI CapEx é”GDPä½”æ¯”æ–°é«˜",
        "bottleneck": "GPUä¾›æ‡‰/é›»åŠ›å–å¾—",
        "catalyst": "Hyperscalerè²¡å ± (CapExæŒ‡å¼•)",
    },
    "AIæ¨¡å‹/å¹³å°": {
        "status": "ğŸŸ¢ æˆé•·",
        "milestone": "GPT-5/Gemini 2.0 ç™¼å¸ƒï¼Œå¤šæ¨¡æ…‹æ¨™é…",
        "bottleneck": "è¨“ç·´æˆæœ¬/ç®—åŠ›éœ€æ±‚",
        "catalyst": "OpenAI/Googleæ–°æ¨¡å‹ç™¼å¸ƒ",
    },
    "AIè³‡æ–™ä¸­å¿ƒ": {
        "status": "ğŸ”¥ çˆ†ç™¼",
        "milestone": "GWç´šAIåœ’å€å‹•å·¥ï¼Œæ¶²å†·æ¨™é…",
        "bottleneck": "é›»åŠ›/åœŸåœ°/è¨±å¯è­‰",
        "catalyst": "æ–°è³‡æ–™ä¸­å¿ƒå‹•å·¥æ¶ˆæ¯",
    },
    "ä¼æ¥­AIæ‡‰ç”¨": {
        "status": "ğŸŸ¢ æˆé•·",
        "milestone": "AI SaaSæ»²é€ç‡é”15%+",
        "bottleneck": "ä¼æ¥­è³‡æ–™æº–å‚™åº¦",
        "catalyst": "ä¼æ¥­è»Ÿé«”è²¡å ± (AIç‡Ÿæ”¶ä½”æ¯”)",
    },
}

SUPPLY_CHAIN_KEYWORDS = {
    "çŸ­ç¼ºè­¦ç¤º": ["shortage", "constraint", "bottleneck", "tight supply", "allocation", "lead time extend"],
    "ç”¢èƒ½å‹•æ…‹": ["capacity expansion", "new fab", "foundry", "utilization", "ramp up", "mass production"],
    "åƒ¹æ ¼è®Šå‹•": ["price hike", "price increase", "price cut", "asp", "margin pressure"],
    "éœ€æ±‚ä¿¡è™Ÿ": ["strong demand", "order", "backlog", "booking", "guidance raise", "beat estimate"],
}


def analyze_trend_from_news(news_list: list) -> dict:
    """åˆ†ææ–°èä¸­çš„æŠ€è¡“è¶¨å‹¢"""
    from collections import defaultdict

    daily_mentions = defaultdict(lambda: defaultdict(int))
    total_mentions = defaultdict(int)

    for news in news_list:
        title = (news.get("title") or "").lower()
        content = (news.get("content") or "").lower()
        text = title + " " + content

        pub_date = news.get("published_at") or news.get("collected_at") or ""
        if pub_date:
            date_str = pub_date[:10]
        else:
            continue

        for trend_name, trend_info in TECH_TRENDS.items():
            for keyword in trend_info["keywords"]:
                if keyword.lower() in text:
                    daily_mentions[date_str][trend_name] += 1
                    total_mentions[trend_name] += 1
                    break

    # è¨ˆç®—å‹•èƒ½
    today = date.today()
    recent_7d = set((today - timedelta(days=i)).strftime("%Y-%m-%d") for i in range(7))
    prev_7d = set((today - timedelta(days=i)).strftime("%Y-%m-%d") for i in range(7, 14))

    momentum = {}
    for trend_name in TECH_TRENDS.keys():
        recent = sum(daily_mentions[d][trend_name] for d in recent_7d)
        prev = sum(daily_mentions[d][trend_name] for d in prev_7d)
        change_pct = ((recent - prev) / prev * 100) if prev > 0 else (100 if recent > 0 else 0)
        momentum[trend_name] = {"recent": recent, "prev": prev, "change_pct": change_pct, "total": total_mentions[trend_name]}

    return {"daily_mentions": dict(daily_mentions), "momentum": momentum}


def detect_supply_chain_alerts(news_list: list) -> list:
    """åµæ¸¬ä¾›æ‡‰éˆè­¦ç¤º"""
    alerts = []
    seen_titles = set()

    for news in news_list:
        title = news.get("title") or ""
        if title in seen_titles:
            continue

        text = (title + " " + (news.get("content") or "")).lower()

        for alert_type, keywords in SUPPLY_CHAIN_KEYWORDS.items():
            for kw in keywords:
                if kw in text:
                    related = [t for t, info in TECH_TRENDS.items() if any(k in text for k in info["keywords"])]
                    if related:
                        seen_titles.add(title)
                        alerts.append({
                            "type": alert_type,
                            "title": title,
                            "date": (news.get("published_at") or "")[:10],
                            "related": related,
                            "url": news.get("url"),
                        })
                        break
                break

    return alerts[:30]


def render_trend_radar_page():
    """æ¸²æŸ“è¶¨å‹¢é›·é”é é¢"""
    st.title("ğŸ¯ AI è¶¨å‹¢é›·é”")
    st.markdown("**è¿½è¹¤ AI ç”¢æ¥­éˆæŠ€è¡“æ¼”é€²ã€ä¾›æ‡‰éˆç“¶é ¸èˆ‡æŠ•è³‡è¼ªå‹•**")

    # æ™‚é–“ç¯„åœé¸æ“‡
    col1, col2 = st.columns([1, 3])
    with col1:
        time_range = st.selectbox(
            "æ™‚é–“ç¯„åœ",
            ["1å€‹æœˆ", "3å€‹æœˆ", "6å€‹æœˆ"],
            index=2  # é è¨­6å€‹æœˆ
        )

    days_map = {"1å€‹æœˆ": 30, "3å€‹æœˆ": 90, "6å€‹æœˆ": 180}
    days = days_map[time_range]

    end_date = date.today()
    start_date = end_date - timedelta(days=days)

    # å–å¾—æ–°è - ä½¿ç”¨çµ±ä¸€è³‡æ–™å±¤
    @st.cache_data(ttl=1800)
    def get_trend_news(start_str: str):
        try:
            client = _get_data_client()
            start_dt = datetime.strptime(start_str, "%Y-%m-%d").date()
            end_dt = date.today()
            news_list = client.get_news(
                start_date=start_dt,
                end_date=end_dt,
                limit=10000
            )
            return news_list
        except Exception as e:
            return []

    raw_news = get_trend_news(start_date.isoformat())
    if not raw_news:
        st.warning("æ²’æœ‰è¶³å¤ çš„æ–°èæ•¸æ“š")
        return

    # å¥—ç”¨ç¯©é¸
    ptt_min = st.session_state.get("ptt_min_push", 30)
    exclude_ed = st.session_state.get("exclude_editorial", True)
    news_list = filter_news(raw_news, ptt_min_push=ptt_min, exclude_editorial=exclude_ed)

    filtered_count = len(raw_news) - len(news_list)
    filter_info = f" (å·²éæ¿¾ {filtered_count} ç¯‡)" if filtered_count > 0 else ""
    st.caption(f"ğŸ“° åˆ†æ {len(news_list)} ç¯‡æ–°è ({start_date} ~ {end_date}){filter_info}")

    trend_data = analyze_trend_from_news(news_list)
    momentum = trend_data["momentum"]

    # ========== ç†±åº¦æ’è¡Œ ==========
    st.header("ğŸ”¥ è¶¨å‹¢ç†±åº¦æ’è¡Œ (é€±è®ŠåŒ–)")

    sorted_trends = sorted(momentum.items(), key=lambda x: x[1]["change_pct"], reverse=True)

    cols = st.columns(4)
    for i, (name, data) in enumerate(sorted_trends[:8]):
        with cols[i % 4]:
            change = data["change_pct"]
            emoji = "ğŸš€" if change > 50 else ("ğŸ“ˆ" if change > 0 else ("â¡ï¸" if change > -20 else "ğŸ“‰"))
            phase = TECH_TRENDS[name]["phase"]
            stocks = ", ".join(TECH_TRENDS[name]["stocks"][:2]) or "â€”"

            st.metric(
                label=f"{emoji} {name}",
                value=f"{data['recent']} å‰‡",
                delta=f"{change:+.0f}% vs ä¸Šé€±",
                help=f"éšæ®µ: {phase} | è‚¡ç¥¨: {stocks}"
            )

    st.divider()

    # ========== è¶¨å‹¢æ™‚é–“ç·š ==========
    st.header("ğŸ“Š è¶¨å‹¢æ™‚é–“ç·š")

    selected = st.multiselect(
        "é¸æ“‡ä¸»é¡Œ", list(TECH_TRENDS.keys()),
        default=["GPU/AIæ™¶ç‰‡", "HBMè¨˜æ†¶é«”", "çŸ½å…‰å­/CPO", "é›»åŠ›åŸºç¤è¨­æ–½"]
    )

    if selected:
        daily = trend_data["daily_mentions"]
        dates = sorted(daily.keys())[-days:]  # ä½¿ç”¨é¸æ“‡çš„æ™‚é–“ç¯„åœ

        # æ ¹æ“šæ™‚é–“ç¯„åœèª¿æ•´ç§»å‹•å¹³å‡çª—å£
        window = 7 if days >= 90 else 3

        fig = go.Figure()
        for trend in selected:
            vals = [daily.get(d, {}).get(trend, 0) for d in dates]
            smoothed = pd.Series(vals).rolling(window, min_periods=1).mean()
            fig.add_trace(go.Scatter(x=dates, y=smoothed, mode='lines', name=trend, line=dict(width=2)))

        fig.update_layout(
            height=500,
            xaxis_title="æ—¥æœŸ",
            yaxis_title=f"æ–°èæåŠæ•¸ ({window}æ—¥å‡ç·š)",
            legend=dict(orientation="h", y=1.1),
            hovermode="x unified"
        )
        st.plotly_chart(fig, use_container_width=True)

        # è¶¨å‹¢æ‘˜è¦
        st.markdown("#### ğŸ“ˆ è¶¨å‹¢è®ŠåŒ–æ‘˜è¦")
        # è¨ˆç®—å„éšæ®µè®ŠåŒ–
        if len(dates) >= 60:
            mid_point = len(dates) // 2
            first_half = dates[:mid_point]
            second_half = dates[mid_point:]

            summary_data = []
            for trend in selected:
                first_count = sum(daily.get(d, {}).get(trend, 0) for d in first_half)
                second_count = sum(daily.get(d, {}).get(trend, 0) for d in second_half)
                if first_count > 0:
                    change = ((second_count - first_count) / first_count) * 100
                else:
                    change = 100 if second_count > 0 else 0

                trend_direction = "ğŸ“ˆ ä¸Šå‡" if change > 20 else ("ğŸ“‰ ä¸‹é™" if change < -20 else "â¡ï¸ æŒå¹³")
                summary_data.append({
                    "ä¸»é¡Œ": trend,
                    "å‰åŠæœŸ": first_count,
                    "å¾ŒåŠæœŸ": second_count,
                    "è®ŠåŒ–": f"{change:+.0f}%",
                    "è¶¨å‹¢": trend_direction,
                })

            st.dataframe(pd.DataFrame(summary_data), use_container_width=True, hide_index=True)

    st.divider()

    # ========== ä¾›æ‡‰éˆè­¦ç¤º ==========
    st.header("âš ï¸ ä¾›æ‡‰éˆè­¦ç¤º")

    alerts = detect_supply_chain_alerts(news_list)
    if alerts:
        alert_types = list(set(a["type"] for a in alerts))
        tabs = st.tabs(alert_types)

        for tab, atype in zip(tabs, alert_types):
            with tab:
                for a in [x for x in alerts if x["type"] == atype][:8]:
                    st.markdown(f"**{a['date']}** | {', '.join(a['related'])}")
                    if a.get("url"):
                        st.markdown(f"[{a['title']}]({a['url']})")
                    else:
                        st.markdown(a['title'])
                    st.markdown("---")
    else:
        st.info("æš«ç„¡é‡å¤§ä¾›æ‡‰éˆè­¦ç¤º")

    st.divider()

    # ========== æŠ•è³‡åœ°åœ– ==========
    st.header("ğŸ“‹ AI ç”¢æ¥­éˆæŠ•è³‡åœ°åœ–")

    table_data = []
    for name, info in TECH_TRENDS.items():
        m = momentum.get(name, {})
        table_data.append({
            "ä¸»é¡Œ": name,
            "éšæ®µ": info["phase"],
            "è¿‘7å¤©": m.get("recent", 0),
            "é€±è®ŠåŒ–": f"{m.get('change_pct', 0):+.0f}%",
            "ç›¸é—œè‚¡ç¥¨": ", ".join(info["stocks"][:3]) or "â€”",
        })

    st.dataframe(pd.DataFrame(table_data), use_container_width=True, hide_index=True)

    with st.expander("ğŸ“– æŠ•è³‡éšæ®µèªªæ˜"):
        st.markdown("""
        | éšæ®µ | ç‰¹å¾µ | ç­–ç•¥å»ºè­° |
        |------|------|----------|
        | ğŸŒ± **æ—©æœŸ** | æŠ€è¡“èŒèŠ½ï¼ŒæåŠæ•¸å°‘ä½†åœ¨ä¸Šå‡ | å°éƒ¨ä½ä½ˆå±€ï¼Œé«˜é¢¨éšªé«˜å ±é…¬ |
        | ğŸ“ˆ **æˆé•·æœŸ** | æŠ€è¡“é©—è­‰ï¼Œéœ€æ±‚å¿«é€Ÿå¢åŠ  | ç©æ¥µåŠ ç¢¼ï¼Œè¿½è¹¤é¾é ­ |
        | ğŸ”¥ **çˆ†ç™¼æœŸ** | ä¾›ä¸æ‡‰æ±‚ï¼Œè‚¡åƒ¹é£†æ¼² | æ ¸å¿ƒæŒè‚¡ï¼Œç•™æ„éç†± |
        | ğŸ“Š **æˆç†ŸæœŸ** | æŠ€è¡“æ™®åŠï¼Œç«¶çˆ­åŠ åŠ‡ | é¸é¾é ­ï¼Œç•™æ„æ¯›åˆ© |
        | â¸ï¸ **ç©©å®šæœŸ** | éœ€æ±‚ç©©å®šï¼Œæˆé•·æ”¾ç·© | åƒ¹å€¼æŠ•è³‡ï¼Œé ˜æ¯ |
        | âš ï¸ **é¢¨éšª** | åœ°ç·£æ”¿æ²»/ç›£ç®¡é¢¨éšª | é¿éšªæˆ–è§€æœ› |
        """)

    # ========== 2026 Q1 æŠ€è¡“é æ¸¬ ==========
    st.header("ğŸ”® 2026 Q1 æŠ€è¡“é æ¸¬")

    forecast_data = []
    for tech, info in Q1_2026_FORECAST.items():
        forecast_data.append({
            "æŠ€è¡“é ˜åŸŸ": tech,
            "ç‹€æ…‹": info["status"],
            "é‡Œç¨‹ç¢‘": info["milestone"],
            "ç“¶é ¸": info["bottleneck"],
            "å‚¬åŒ–åŠ‘": info["catalyst"],
        })

    st.dataframe(pd.DataFrame(forecast_data), use_container_width=True, hide_index=True)

    st.divider()

    # ========== é—œéµè‚¡ç¥¨å°ç…§è¡¨ ==========
    st.header("ğŸ“ˆ é—œéµè‚¡ç¥¨å°ç…§è¡¨")

    # æŒ‰é¡åˆ¥åˆ†çµ„
    categories = {}
    for symbol, info in STOCK_DETAILS.items():
        cat = info["category"]
        if cat not in categories:
            categories[cat] = []
        categories[cat].append({"ä»£ç¢¼": symbol, "å…¬å¸": info["name"], "è§’è‰²": info["role"]})

    # é¸æ“‡é¡åˆ¥
    selected_cat = st.selectbox("é¸æ“‡æŠ€è¡“é ˜åŸŸ", list(categories.keys()))

    if selected_cat:
        st.dataframe(pd.DataFrame(categories[selected_cat]), use_container_width=True, hide_index=True)

        # é¡¯ç¤ºç›¸é—œè¶¨å‹¢
        trend_info = TECH_TRENDS.get(selected_cat, {})
        if trend_info:
            col1, col2 = st.columns(2)
            with col1:
                st.metric("éšæ®µ", trend_info.get("phase", "â€”"))
            with col2:
                st.metric("é—œéµå­—", ", ".join(trend_info.get("keywords", [])[:5]))
            if trend_info.get("detail"):
                st.info(f"ğŸ“Œ {trend_info['detail']}")

    st.divider()

    # ========== æŠ€è¡“æ¼”é€²è·¯ç·šåœ– ==========
    with st.expander("ğŸ—ºï¸ AI æŠ€è¡“æ¼”é€²è·¯ç·šåœ– (2024-2027) - å«å›æ¸¬è‚¡ç¥¨"):
        st.markdown("""
        ### æŠ€è¡“æ¼”é€²èˆ‡å°æ‡‰è‚¡ç¥¨

        | é ˜åŸŸ | 2024 H1 | 2024 H2 | 2025 H1 | 2025 H2 | 2026 Q1 | 2026 H2 | 2027+ |
        |------|---------|---------|---------|---------|---------|---------|-------|
        | **GPUé‹ç®—** | H100 | H200 | B100/B200(è©¦ç”¢) | Blackwell(é‡ç”¢) | Blackwell(æ”¾é‡) | Rubin R100(è©¦ç”¢) | R200 |
        | è‚¡ç¥¨ | NVDA | NVDA, AMD | NVDA, AMD, AVGO | NVDA, AMD, AVGO | NVDA, AMD, MRVL | NVDA, AMD | NVDA |
        | **è¨˜æ†¶é«”** | HBM3 | HBM3E | HBM3E(ä¾›ä¸æ‡‰æ±‚) | HBM3E+HBM4è©¦ç”¢ | HBM4é‡ç”¢ | HBM4E | HBM4+ |
        | è‚¡ç¥¨ | MU | MU | MU | MU | MU | MU | MU |
        | **å…ˆé€²å°è£** | CoWoS-S(5è¬ç‰‡) | CoWoS-S(6è¬ç‰‡) | CoWoS-L(7.5è¬ç‰‡) | CoWoS-L(é‡ç”¢) | CoWoS-R(10è¬ç‰‡) | SoIC(13è¬ç‰‡) | 3D IC |
        | è‚¡ç¥¨ | TSM, ASX | TSM, ASX | TSM, ASX | TSM, ASX | TSM, ASX | TSM, ASX | TSM |
        | **å…‰äº’é€£** | 400G | 800G(é‡ç”¢) | 800G(ä¸»æµ) | 1.6T(è©¦ç”¢) | 1.6Té‡ç”¢+CPO | 3.2T | CPOæ¨™é… |
        | è‚¡ç¥¨ | COHR | COHR, LITE | LITE, COHR, FN | LITE, COHR, MRVL | LITE, COHR, MRVL | LITE, MRVL | MRVL |
        | **æ•£ç†±** | æ°£å†· | æ°£å†·+æ¶²å†· | æ¶²å†·(æˆé•·) | æ¶²å†·(çˆ†ç™¼) | æ¶²å†·40%+ | æµ¸æ²’å¼ | æ¶²å†·æ¨™é… |
        | è‚¡ç¥¨ | - | VRT | VRT, CARR | VRT, CARR | VRT | VRT | VRT |
        | **é›»åŠ›** | å‚³çµ±é›»ç¶² | é›»åŠ›ç·Šå¼µ | æ ¸é›»PPA | 800V HVDC | é›»ç¶²ç“¶é ¸ | SMRä½ˆå±€ | æ ¸èƒ½+å†ç”Ÿ |
        | è‚¡ç¥¨ | - | VST, ETN | CEG, CCJ, VST | CEG, CCJ, ETN, PWR | CEG, CCJ, SMR | SMR, CEG | SMR |
        | **é›²ç«¯AI** | ChatGPTçˆ†ç™¼ | AI CapExå•Ÿå‹• | CapExåŠ é€Ÿ | CapExé«˜å³° | CapExæŒçºŒ | æ•ˆç‡å„ªåŒ– | ä¸‹ä¸€æ³¢ |
        | è‚¡ç¥¨ | MSFT, GOOGL | MSFT, AMZN, GOOGL | MSFT, AMZN, GOOGL, META | MSFT, AMZN, ORCL | MSFT, AMZN, ORCL | MSFT, GOOGL | - |
        | **è³‡æ–™ä¸­å¿ƒ** | å‚³çµ±DC | AI DCè¦åŠƒ | AI DCå‹•å·¥ | GWç´šåœ’å€ | æ“´ç”¢åŠ é€Ÿ | é‚Šç·£DC | åˆ†æ•£å¼ |
        | è‚¡ç¥¨ | EQIX, DLR | EQIX, DLR | EQIX, DLR, AMT | EQIX, DLR | EQIX, DLR | AMT | - |
        | **ä¼æ¥­AI** | POCéšæ®µ | è©¦é»éƒ¨ç½² | è¦æ¨¡éƒ¨ç½² | ç‡Ÿæ”¶è²¢ç» | AIä½”æ¯”15%+ | æ¨™é… | AIåŸç”Ÿ |
        | è‚¡ç¥¨ | - | CRM, NOW | CRM, NOW, PLTR | CRM, NOW, PLTR, WDAY | CRM, NOW, SNOW, PATH | å…¨éƒ¨ | - |

        ---

        ### ğŸ“Š å›æ¸¬å»ºè­°ï¼šå„æ™‚æœŸæ ¸å¿ƒæŒè‚¡

        | æ™‚æœŸ | ä¸»é¡Œç„¦é» | æ ¸å¿ƒæŒè‚¡ | è¼”åŠ©æŒè‚¡ |
        |------|----------|----------|----------|
        | **2024 H1** | GPUéœ€æ±‚çˆ†ç™¼ | NVDA | MSFT, GOOGL, TSM |
        | **2024 H2** | è¨˜æ†¶é«”/å°è£ç·Šå¼µ | NVDA, MU, TSM | COHR, VRT |
        | **2025 H1** | å…‰äº’é€£å´›èµ· | NVDA, LITE, TSM | MU, VRT, CEG |
        | **2025 H2** | HBM4+CPOè½‰æŠ˜ | MU, LITE, MRVL | NVDA, TSM, CEG |
        | **2026 Q1** | å¤šå…ƒçˆ†ç™¼ | LITE, MU, CEG | NVDA, VRT, CRM |
        | **2026 H2** | æ¬¡ä¸–ä»£ä½ˆå±€ | SMR, MRVL, NVDA | VRT, TSM, NOW |
        """)

    # ========== æŠ•è³‡ä¸»é¡Œæ‘˜è¦ ==========
    with st.expander("ğŸ’¡ 2026 Q1 æŠ•è³‡ä¸»é¡Œæ‘˜è¦"):
        st.markdown("""
        ### ğŸ”¥ æœ€ç†±é–€ä¸»é¡Œ (çˆ†ç™¼æœŸ)

        | ä¸»é¡Œ | æ ¸å¿ƒé‚è¼¯ | é¦–é¸è‚¡ç¥¨ |
        |------|----------|----------|
        | **HBMè¨˜æ†¶é«”** | HBM4å‡ºè²¨å•Ÿå‹•ï¼ŒSK Hynixå¸‚ä½”70% | MU, SK Hynix |
        | **çŸ½å…‰å­/CPO** | 1.6Té‡ç”¢ï¼ŒCPOå¾å¯¦é©—è®Šå¿…å‚™ | LITE, COHR, MRVL |
        | **å…ˆé€²å°è£** | CoWoSæœˆç”¢èƒ½ç¿»å€ï¼Œä»ä¾›ä¸æ‡‰æ±‚ | TSM, ASX |
        | **æ¶²å†·æ•£ç†±** | 1GWç´šè³‡æ–™ä¸­å¿ƒæ¨™é…æ¶²å†· | VRT |

        ### ğŸŒ± æ—©æœŸä½ˆå±€ (é«˜é¢¨éšªé«˜å ±é…¬)

        | ä¸»é¡Œ | æ ¸å¿ƒé‚è¼¯ | é¦–é¸è‚¡ç¥¨ |
        |------|----------|----------|
        | **æ ¸èƒ½å¾©èˆˆ** | ç§‘æŠ€å·¨é ­ç°½æ ¸é›»PPA | CEG, CCJ, SMR |
        | **AI Agent** | ä¼æ¥­Agentå¤§è¦æ¨¡éƒ¨ç½²å…ƒå¹´ | CRM, NOW, MSFT |

        ### âš ï¸ é¢¨éšªé—œæ³¨

        | é¢¨éšª | å½±éŸ¿ | æ‡‰å° |
        |------|------|------|
        | **é›»åŠ›ç“¶é ¸** | è³‡æ–™ä¸­å¿ƒé¸å€å—é™ï¼Œé›»åƒ¹ä¸Šæ¼² | é—œæ³¨é›»åŠ›è‚¡ VST, PWR |
        | **é—œç¨…æˆ°** | åŠå°é«”è¨­å‚™/é›¶ä»¶æˆæœ¬ä¸Šå‡ | é¿å…é«˜ä¸­åœ‹æ›éšªè‚¡ |
        | **ä¼°å€¼éé«˜** | AIè‚¡æ•´é«”ä¼°å€¼åé«˜ | åˆ†æ‰¹ä½ˆå±€ï¼Œç•™æ„å›èª¿ |
        """)


# ========== å­£åº¦æŒè‚¡æ± å›æ¸¬ç³»çµ± ==========
# åŸºæ–¼ã€Œå­£åˆå¯å¾—è³‡è¨Šã€çš„ä¿¡è™Ÿç³»çµ±ï¼Œé¿å…å¾Œè¦‹ä¹‹æ˜

# å­£åˆä¿¡è™Ÿå®šç¾©ï¼ˆé€™äº›æ˜¯æ¯å­£é–‹å§‹æ™‚å°±èƒ½è§€å¯Ÿåˆ°çš„ï¼‰
QUARTER_SIGNALS = {
    "2022Q1": {
        "fed_stance": "å³å°‡å‡æ¯",      # 2021/12 Fedé»é™£åœ–é¡¯ç¤º2022å‡æ¯
        "yield_curve": "æ­£å¸¸ä½†è¶¨å¹³",    # 10Y-2Y ç´„ 0.8%
        "cpi_trend": "ä¸Šå‡ (7%)",       # 2021/12 CPI 7.0%
        "spy_vs_200ma": "ä¸Šæ–¹",         # SPY åœ¨ 200MA ä¸Šæ–¹
        "vix": "ä¸­ç­‰ (17)",
        "signal_score": 0.3,            # -1(æ¥µåº¦é˜²ç¦¦) åˆ° 1(æ¥µåº¦ç©æ¥µ)
    },
    "2022Q2": {
        "fed_stance": "æ¿€é€²å‡æ¯ä¸­",     # 3æœˆå‡æ¯1ç¢¼ï¼Œæš—ç¤ºåŠ é€Ÿ
        "yield_curve": "è¶¨å¹³",          # 10Y-2Y æ¥è¿‘ 0
        "cpi_trend": "åŠ é€Ÿ (8.5%)",     # 2022/03 CPI 8.5%
        "spy_vs_200ma": "è·Œç ´",         # SPY è·Œç ´ 200MA
        "vix": "åé«˜ (21)",
        "signal_score": -0.3,
    },
    "2022Q3": {
        "fed_stance": "æŒçºŒé·¹æ´¾",       # 6æœˆå‡æ¯3ç¢¼
        "yield_curve": "å€’æ›",          # 10Y-2Y è½‰è² 
        "cpi_trend": "é«˜å³° (9.1%)",     # 2022/06 CPI 9.1%
        "spy_vs_200ma": "ä¸‹æ–¹",
        "vix": "åé«˜ (26)",
        "signal_score": -0.5,
    },
    "2022Q4": {
        "fed_stance": "é·¹æ´¾ä½†æ”¾ç·©",     # æŒçºŒå‡æ¯ä½†å¹…åº¦å¯èƒ½æ¸›
        "yield_curve": "å€’æ›",
        "cpi_trend": "é–‹å§‹ä¸‹æ»‘ (8.2%)", # 2022/09 CPI 8.2%
        "spy_vs_200ma": "ä¸‹æ–¹",
        "vix": "é«˜ (31)",
        "signal_score": -0.2,
    },
    "2023Q1": {
        "fed_stance": "å‡æ¯å°¾è²",       # å¸‚å ´é æœŸæ¥è¿‘çµ‚é»
        "yield_curve": "æ·±åº¦å€’æ›",
        "cpi_trend": "ä¸‹æ»‘ (6.5%)",
        "spy_vs_200ma": "æ¥è¿‘",
        "vix": "ä¸‹é™ (21)",
        "signal_score": 0.2,
    },
    "2023Q2": {
        "fed_stance": "æ¥è¿‘æš«åœ",
        "yield_curve": "å€’æ›",
        "cpi_trend": "æŒçºŒä¸‹æ»‘ (5%)",
        "spy_vs_200ma": "ä¸Šæ–¹",         # çªç ´ 200MA
        "vix": "ä½ (17)",
        "ai_momentum": "ChatGPTç”¨æˆ¶ç ´å„„", # æ–°ä¿¡è™Ÿï¼šAIé¡Œæ
        "signal_score": 0.5,
    },
    "2023Q3": {
        "fed_stance": "æš«åœè§€æœ›",
        "yield_curve": "å€’æ›",
        "cpi_trend": "ä¸‹æ»‘ (3.2%)",
        "spy_vs_200ma": "ä¸Šæ–¹",
        "vix": "ä½ (14)",
        "ai_momentum": "NVDAè²¡å ±è¶…é æœŸ",
        "signal_score": 0.6,
    },
    "2023Q4": {
        "fed_stance": "æš«åœï¼Œé™æ¯é æœŸ",
        "yield_curve": "å€’æ›æ”¶çª„",
        "cpi_trend": "ç©©å®š (3.7%)",
        "spy_vs_200ma": "ä¸Šæ–¹",
        "vix": "ä½ (17)",
        "ai_momentum": "AI CapExç¢ºèªå¢åŠ ",
        "signal_score": 0.7,
    },
    "2024Q1": {
        "fed_stance": "ç¶­æŒï¼Œç­‰å¾…é™æ¯",
        "yield_curve": "å€’æ›æ”¶çª„",
        "cpi_trend": "ç©©å®š (3.4%)",
        "spy_vs_200ma": "ä¸Šæ–¹",
        "vix": "ä½ (13)",
        "ai_momentum": "Hyperscaler CapExæŒ‡å¼•å¼·å‹",
        "signal_score": 0.7,
    },
    "2024Q2": {
        "fed_stance": "ç¶­æŒè§€æœ›",
        "yield_curve": "å€’æ›",
        "cpi_trend": "ç•¥å‡ (3.5%)",
        "spy_vs_200ma": "ä¸Šæ–¹",
        "vix": "ä½ (13)",
        "ai_momentum": "HBMä¾›ä¸æ‡‰æ±‚",
        "signal_score": 0.6,
    },
    "2024Q3": {
        "fed_stance": "å³å°‡é™æ¯",
        "yield_curve": "å€’æ›æ”¶çª„",
        "cpi_trend": "ä¸‹æ»‘ (2.9%)",
        "spy_vs_200ma": "ä¸Šæ–¹",
        "vix": "ä¸­ç­‰ (15)",
        "ai_momentum": "800Gé‡ç”¢ï¼Œå…‰äº’é€£é¡Œæ",
        "signal_score": 0.6,
    },
    "2024Q4": {
        "fed_stance": "é™æ¯é–‹å§‹",
        "yield_curve": "æ­£å¸¸åŒ–",
        "cpi_trend": "ç©©å®š (2.6%)",
        "spy_vs_200ma": "ä¸Šæ–¹",
        "vix": "ä¸­ç­‰ (16)",
        "ai_momentum": "æ ¸é›»PPAç°½ç´„ï¼Œé›»åŠ›ç“¶é ¸",
        "signal_score": 0.5,
    },
    "2025Q1": {
        "fed_stance": "é™æ¯æš«åœ",       # 1æœˆFedç¶­æŒåˆ©ç‡
        "yield_curve": "æ­£å¸¸",
        "cpi_trend": "ç•¥å‡ (2.9%)",
        "spy_vs_200ma": "è·Œç ´å¾Œåå½ˆ",   # 1æœˆåº•è·Œç ´ï¼Œ2æœˆåå½ˆ
        "vix": "é£†å‡ (16â†’28)",          # DeepSeekå¾ŒVIXé£†å‡è‡³28
        "ai_momentum": "DeepSeekè¡æ“Šï¼ŒAIä¼°å€¼é‡ä¼°",
        "tariff_risk": "å·æ™®é—œç¨…å¨è„…å‡ç´š",
        "signal_score": -0.3,           # ç†Šå¸‚ä¿¡è™Ÿï¼
    },
    # ===== ä»¥ä¸‹ç‚ºæœªä¾†é æ¸¬ (å‡è¨­æƒ…å¢ƒ) =====
    "2025Q2": {
        "fed_stance": "è§€æœ›",
        "yield_curve": "æ­£å¸¸",
        "cpi_trend": "å¾…è§€å¯Ÿ",
        "spy_vs_200ma": "å¾…è§€å¯Ÿ",
        "vix": "å¾…è§€å¯Ÿ (é—œç¨…è«‡åˆ¤)",
        "ai_momentum": "é—œç¨…å½±éŸ¿å¾…é‡æ¸…",
        "tariff_risk": "é—œç¨…è«‡åˆ¤é€²è¡Œä¸­",
        "signal_score": -0.1,           # ä»åä¿å®ˆ
    },
    "2025Q3": {
        "fed_stance": "å¯èƒ½é™æ¯",
        "yield_curve": "æ­£å¸¸",
        "cpi_trend": "ç©©å®š",
        "spy_vs_200ma": "å¾…è§€å¯Ÿ",
        "vix": "å¾…è§€å¯Ÿ",
        "ai_momentum": "HBM4é‡ç”¢",
        "signal_score": 0.5,
    },
    "2025Q4": {
        "fed_stance": "å¯¬é¬†é€±æœŸ",
        "yield_curve": "æ­£å¸¸",
        "cpi_trend": "ç©©å®š",
        "spy_vs_200ma": "å¾…è§€å¯Ÿ",
        "vix": "å¾…è§€å¯Ÿ",
        "ai_momentum": "AIå…¨é¢æ»²é€",
        "signal_score": 0.5,
    },
    "2026Q1": {
        "fed_stance": "å¯¬é¬†",
        "yield_curve": "æ­£å¸¸",
        "cpi_trend": "ç©©å®š",
        "spy_vs_200ma": "å¾…è§€å¯Ÿ",
        "vix": "å¾…è§€å¯Ÿ",
        "ai_momentum": "Rubiné ç†±",
        "signal_score": 0.5,
    },
}

def get_allocation_from_signal(signal_score: float, ai_momentum: bool = False) -> dict:
    """æ ¹æ“šä¿¡è™Ÿåˆ†æ•¸æ±ºå®šé…ç½®é¢¨æ ¼

    signal_score: -1 (æ¥µåº¦é˜²ç¦¦) åˆ° 1 (æ¥µåº¦ç©æ¥µ)
    """
    if signal_score <= -0.5:
        # æ¥µåº¦é˜²ç¦¦
        return {"style": "æ¥µåº¦é˜²ç¦¦", "equity": 0.40, "defensive": 0.40, "bond": 0.20}
    elif signal_score <= -0.2:
        # é˜²ç¦¦
        return {"style": "é˜²ç¦¦", "equity": 0.55, "defensive": 0.30, "bond": 0.15}
    elif signal_score <= 0.2:
        # ä¸­æ€§
        return {"style": "ä¸­æ€§", "equity": 0.70, "defensive": 0.20, "bond": 0.10}
    elif signal_score <= 0.5:
        # ç©æ¥µ
        return {"style": "ç©æ¥µ", "equity": 0.85, "defensive": 0.10, "bond": 0.05}
    else:
        # æ¥µåº¦ç©æ¥µ
        return {"style": "æ¥µåº¦ç©æ¥µ", "equity": 0.95, "defensive": 0.05, "bond": 0.00}


# åŸºæ–¼å­£åˆä¿¡è™Ÿçš„æŒè‚¡é…ç½®
QUARTERLY_PORTFOLIOS = {
    # ===== 2022 å¹´ =====
    "2022Q1": {
        "name": "å‡æ¯é æœŸå•Ÿå‹•",
        "signal": "Fedé»é™£åœ–é¡¯ç¤ºå‡æ¯ï¼ŒCPI 7%ï¼Œä½†SPYä»åœ¨200MAä¸Š",
        "start": "2022-01-01",
        "end": "2022-03-31",
        "holdings": {
            # signal_score: 0.3 (ä¸­æ€§åç©æ¥µ)
            "XLE": 0.20,    # é€šè†¨å—æƒ 
            "XLF": 0.15,    # å‡æ¯å—æƒ 
            "MSFT": 0.15,
            "AAPL": 0.15,
            "GOOGL": 0.12,
            "XLV": 0.12,    # éƒ¨åˆ†é˜²ç¦¦
            "JPM": 0.11,
        },
    },
    "2022Q2": {
        "name": "SPYè·Œç ´200MA",
        "signal": "SPYè·Œç ´200MAï¼ŒCPIåŠ é€Ÿè‡³8.5%ï¼ŒFedå‡æ¯åŠ é€Ÿ",
        "start": "2022-04-01",
        "end": "2022-06-30",
        "holdings": {
            # signal_score: -0.3 (é˜²ç¦¦)
            "XLE": 0.25,    # èƒ½æºé€šè†¨å—æƒ 
            "XLV": 0.20,    # é˜²ç¦¦
            "XLU": 0.15,    # é˜²ç¦¦
            "XLF": 0.15,    # å‡æ¯å—æƒ 
            "COST": 0.13,   # å¿…éœ€æ¶ˆè²»
            "JNJ": 0.12,
        },
    },
    "2022Q3": {
        "name": "æ®–åˆ©ç‡å€’æ›ç¢ºèª",
        "signal": "10Y-2Yå€’æ›ï¼ŒCPIé”9.1%é«˜å³°ï¼ŒVIX 26",
        "start": "2022-07-01",
        "end": "2022-09-30",
        "holdings": {
            # signal_score: -0.5 (æ¥µåº¦é˜²ç¦¦)
            "XLE": 0.20,
            "XLV": 0.20,
            "XLU": 0.20,
            "SHY": 0.15,    # çŸ­å‚µé¿éšª
            "COST": 0.13,
            "PG": 0.12,
        },
    },
    "2022Q4": {
        "name": "CPIè¦‹é ‚ä¿¡è™Ÿ",
        "signal": "CPIå¾9.1%é™è‡³8.2%ï¼Œé€šè†¨å¯èƒ½è¦‹é ‚",
        "start": "2022-10-01",
        "end": "2022-12-31",
        "holdings": {
            # signal_score: -0.2 (é˜²ç¦¦ä½†é–‹å§‹è©¦æ¢)
            "XLE": 0.18,
            "XLV": 0.15,
            "XLF": 0.12,
            "MSFT": 0.12,
            "AAPL": 0.12,
            "GOOGL": 0.10,
            "XLU": 0.10,
            "AMZN": 0.11,
        },
    },
    # ===== 2023 å¹´ =====
    "2023Q1": {
        "name": "å‡æ¯å°¾è²é æœŸ",
        "signal": "CPIé™è‡³6.5%ï¼Œå¸‚å ´é æœŸFedæ¥è¿‘çµ‚é»",
        "start": "2023-01-01",
        "end": "2023-03-31",
        "holdings": {
            # signal_score: 0.2 (ä¸­æ€§)
            "MSFT": 0.18,   # ChatGPTé¡Œæ (2022/11ä¸Šç·š)
            "NVDA": 0.15,   # GPUéœ€æ±‚é æœŸ
            "GOOGL": 0.15,
            "META": 0.12,   # æ•ˆç‡å¹´é¡Œæ
            "AAPL": 0.12,
            "XLV": 0.15,    # ç¶­æŒé˜²ç¦¦
            "AMZN": 0.13,
        },
    },
    "2023Q2": {
        "name": "AIéœ€æ±‚ç¢ºèª",
        "signal": "SPYçªç ´200MAï¼ŒChatGPTç”¨æˆ¶ç ´å„„ï¼ŒNVDAæŒ‡å¼•è¶…é æœŸ",
        "start": "2023-04-01",
        "end": "2023-06-30",
        "holdings": {
            # signal_score: 0.5 (ç©æ¥µ)
            "NVDA": 0.28,
            "MSFT": 0.18,
            "AMD": 0.12,
            "GOOGL": 0.12,
            "META": 0.10,
            "TSM": 0.10,
            "XLV": 0.10,    # å°‘é‡é˜²ç¦¦
        },
    },
    "2023Q3": {
        "name": "AI CapExç¢ºèª",
        "signal": "Hyperscalerè²¡å ±ç¢ºèªAIæŠ•è³‡ï¼ŒVIX 14ä½æª”",
        "start": "2023-07-01",
        "end": "2023-09-30",
        "holdings": {
            # signal_score: 0.6 (ç©æ¥µ)
            "NVDA": 0.28,
            "MSFT": 0.18,
            "AMD": 0.10,
            "AVGO": 0.10,
            "TSM": 0.10,
            "GOOGL": 0.10,
            "AMZN": 0.09,
            "XLV": 0.05,
        },
    },
    "2023Q4": {
        "name": "é™æ¯é æœŸå‡æº«",
        "signal": "Fedæš«åœå‡æ¯ï¼Œå¸‚å ´é–‹å§‹å®šåƒ¹2024é™æ¯",
        "start": "2023-10-01",
        "end": "2023-12-31",
        "holdings": {
            # signal_score: 0.7 (æ¥µåº¦ç©æ¥µ)
            "NVDA": 0.25,
            "MSFT": 0.18,
            "AMD": 0.10,
            "AVGO": 0.10,
            "META": 0.10,
            "GOOGL": 0.10,
            "TSM": 0.10,
            "AMZN": 0.07,
        },
    },
    # ===== 2024 å¹´ =====
    "2024Q1": {
        "name": "AI CapExæŒ‡å¼•å¼·å‹",
        "signal": "Hyperscaler 2024 CapExæŒ‡å¼•å¤§å¢ï¼ŒVIXä½æª”",
        "start": "2024-01-01",
        "end": "2024-03-31",
        "holdings": {
            # signal_score: 0.7
            "NVDA": 0.30,
            "MSFT": 0.18,
            "TSM": 0.12,
            "GOOGL": 0.12,
            "AVGO": 0.12,
            "AMD": 0.08,
            "META": 0.08,
        },
    },
    "2024Q2": {
        "name": "HBMä¾›éœ€ç·Šå¼µ",
        "signal": "HBMä¾›ä¸æ‡‰æ±‚æ–°èå¢åŠ ï¼Œè¨˜æ†¶é«”è‚¡å—é—œæ³¨",
        "start": "2024-04-01",
        "end": "2024-06-30",
        "holdings": {
            # signal_score: 0.6
            "NVDA": 0.25,
            "MU": 0.15,
            "TSM": 0.15,
            "MSFT": 0.12,
            "AVGO": 0.12,
            "AMD": 0.08,
            "GOOGL": 0.08,
            "XLV": 0.05,
        },
    },
    "2024Q3": {
        "name": "å…‰äº’é€£é¡Œæ",
        "signal": "800Gé‡ç”¢æ–°èå¢åŠ ï¼ŒçŸ½å…‰å­/CPOé¡Œææµ®ç¾",
        "start": "2024-07-01",
        "end": "2024-09-30",
        "holdings": {
            # signal_score: 0.6
            "NVDA": 0.22,
            "LITE": 0.12,
            "COHR": 0.08,
            "MU": 0.12,
            "TSM": 0.12,
            "VRT": 0.10,
            "MSFT": 0.10,
            "AVGO": 0.08,
            "XLV": 0.06,
        },
    },
    "2024Q4": {
        "name": "é›»åŠ›ç“¶é ¸æµ®ç¾",
        "signal": "æ ¸é›»PPAæ–°èå¢åŠ ï¼Œè³‡æ–™ä¸­å¿ƒé›»åŠ›é¡Œæ",
        "start": "2024-10-01",
        "end": "2024-12-31",
        "holdings": {
            # signal_score: 0.5
            "NVDA": 0.18,
            "CEG": 0.12,
            "VST": 0.08,
            "LITE": 0.12,
            "MU": 0.10,
            "TSM": 0.12,
            "VRT": 0.10,
            "MSFT": 0.10,
            "XLV": 0.08,
        },
    },
    # ===== 2025 å¹´ =====
    "2025Q1": {
        "name": "DeepSeekè¡æ“Š+é—œç¨…é¢¨éšª",
        "signal": "VIXé£†å‡è‡³28ï¼ŒDeepSeekè¡æ“ŠAIä¼°å€¼ï¼Œé—œç¨…é¢¨éšªå‡ç´š",
        "start": "2025-01-01",
        "end": "2025-03-31",
        "holdings": {
            # signal_score: -0.3 (ç†Šå¸‚é˜²ç¦¦)
            "XLV": 0.25,    # é†«ç™‚é˜²ç¦¦
            "XLU": 0.20,    # å…¬ç”¨äº‹æ¥­é˜²ç¦¦
            "CEG": 0.15,    # é›»åŠ› (AIéœ€æ±‚ä¸è®Š)
            "MSFT": 0.12,   # è»Ÿé«”æŠ—é—œç¨…
            "GOOGL": 0.10,
            "SHY": 0.10,    # çŸ­å‚µé¿éšª
            "NVDA": 0.08,   # å¤§å¹…æ¸›ç¢¼
        },
    },
    # ===== 2025 Q2-Q4 (æœªä¾†é æ¸¬ï¼Œåƒ…ä¾›åƒè€ƒ) =====
    "2025Q2": {
        "name": "é—œç¨…è«‡åˆ¤è§€æœ›",
        "signal": "[é æ¸¬] é—œç¨…å½±éŸ¿å¾…é‡æ¸…ï¼Œç¶­æŒä¿å®ˆé…ç½®",
        "start": "2025-04-01",
        "end": "2025-06-30",
        "holdings": {
            # signal_score: -0.1 (ä¸­æ€§åä¿å®ˆ)
            "XLV": 0.15,    # ç¶­æŒé˜²ç¦¦
            "CEG": 0.15,    # é›»åŠ›éœ€æ±‚ç©©å®š
            "MSFT": 0.15,   # è»Ÿé«”æŠ—é—œç¨…
            "LITE": 0.12,   # çŸ½å…‰å­
            "NVDA": 0.12,   # é€æ­¥åŠ å›
            "GOOGL": 0.10,
            "MU": 0.10,
            "TSM": 0.06,
            "XLU": 0.05,
        },
    },
    "2025Q3": {
        "name": "HBM4é‡ç”¢é æœŸ",
        "signal": "[é æ¸¬] è‹¥HBM4å¦‚æœŸé‡ç”¢ï¼Œè¨˜æ†¶é«”è‚¡å¯èƒ½é ˜æ¼²",
        "start": "2025-07-01",
        "end": "2025-09-30",
        "holdings": {
            "MU": 0.25,
            "NVDA": 0.20,
            "LITE": 0.15,
            "MRVL": 0.10,
            "TSM": 0.10,
            "CEG": 0.10,
            "VRT": 0.10,
        },
    },
    "2025Q4": {
        "name": "AIå…¨é¢æ»²é€é æœŸ",
        "signal": "[é æ¸¬] è‹¥AIæ‡‰ç”¨æŒçºŒæ“´æ•£ï¼Œç”¢æ¥­éˆå…¨é¢å—æƒ ",
        "start": "2025-10-01",
        "end": "2025-12-31",
        "holdings": {
            "LITE": 0.15,
            "MU": 0.15,
            "NVDA": 0.15,
            "CEG": 0.15,
            "MRVL": 0.10,
            "VRT": 0.10,
            "TSM": 0.10,
            "SMR": 0.10,
        },
    },
    "2026Q1": {
        "name": "æ¬¡ä¸–ä»£ä½ˆå±€é æœŸ",
        "signal": "[é æ¸¬] Rubinæ¶æ§‹é ç†±ï¼ŒSMRæ ¸é›»ä½ˆå±€",
        "start": "2026-01-01",
        "end": "2026-03-31",
        "holdings": {
            "NVDA": 0.20,
            "LITE": 0.15,
            "MU": 0.15,
            "CEG": 0.10,
            "SMR": 0.10,
            "MRVL": 0.10,
            "TSM": 0.10,
            "VRT": 0.10,
        },
    },
}

# åŸºæº–æŒ‡æ•¸
BENCHMARK_SYMBOLS = {
    "SPY": "S&P 500",
    "QQQ": "Nasdaq 100",
    "SOXX": "åŠå°é«”ETF",
    "SMH": "VanEckåŠå°é«”",
}

# ========== è¦å‰‡åŒ–ä¿¡è™Ÿç³»çµ± ==========
# å®šç¾©ä¿¡è™Ÿè¦å‰‡ - æ¯æ¢è¦å‰‡åŸºæ–¼æœˆåˆå¯å¾—è³‡è¨Š
SIGNAL_RULES = {
    # SPY ç›¸å° 200MA ä½ç½® (æœ€é‡è¦çš„è¶¨å‹¢æŒ‡æ¨™)
    "spy_below_200ma": {"weight": -0.30, "description": "SPY ä½æ–¼ 200MA"},
    "spy_above_200ma": {"weight": +0.15, "description": "SPY é«˜æ–¼ 200MA"},
    "spy_far_below_200ma": {"weight": -0.20, "description": "SPY ä½æ–¼ 200MA è¶…é 5%"},

    # VIX ææ…ŒæŒ‡æ•¸
    "vix_extreme": {"weight": -0.25, "description": "VIX > 35 (æ¥µåº¦ææ…Œ)"},
    "vix_high": {"weight": -0.15, "description": "VIX 25-35 (ææ…Œ)"},
    "vix_elevated": {"weight": -0.05, "description": "VIX 20-25 (è­¦æˆ’)"},
    "vix_low": {"weight": +0.10, "description": "VIX < 15 (å¹³éœ)"},

    # SPY å‹•èƒ½ (è¿‘æœŸè¡¨ç¾)
    "spy_momentum_negative": {"weight": -0.15, "description": "SPY è¿‘æœˆè·Œå¹… > 5%"},
    "spy_momentum_positive": {"weight": +0.10, "description": "SPY è¿‘æœˆæ¼²å¹… > 3%"},

    # 200MA æ–œç‡
    "ma200_declining": {"weight": -0.10, "description": "200MA ä¸‹é™è¶¨å‹¢"},
    "ma200_rising": {"weight": +0.05, "description": "200MA ä¸Šå‡è¶¨å‹¢"},
}


@st.cache_data(ttl=86400)  # å¿«å–ä¸€å¤©
def fetch_market_indicators(start_date: str, end_date: str) -> pd.DataFrame:
    """å–å¾—å¸‚å ´æŒ‡æ¨™æ•¸æ“š (SPY, VIX)"""
    import yfinance as yf

    # å–å¾— SPY å’Œ VIX
    spy = yf.Ticker("SPY")
    vix = yf.Ticker("^VIX")

    spy_hist = spy.history(start=start_date, end=end_date)
    vix_hist = vix.history(start=start_date, end=end_date)

    if spy_hist.empty:
        return pd.DataFrame()

    # åˆä½µæ•¸æ“š
    df = pd.DataFrame()
    df["spy_close"] = spy_hist["Close"]
    df["spy_ma200"] = spy_hist["Close"].rolling(window=200, min_periods=50).mean()
    df["spy_ma50"] = spy_hist["Close"].rolling(window=50, min_periods=20).mean()
    df["vix"] = vix_hist["Close"].reindex(df.index, method="ffill")

    # è¨ˆç®—è¡ç”ŸæŒ‡æ¨™
    df["spy_vs_ma200_pct"] = (df["spy_close"] / df["spy_ma200"] - 1) * 100
    df["spy_momentum_1m"] = df["spy_close"].pct_change(periods=21) * 100  # ç´„ä¸€å€‹æœˆ
    df["ma200_slope"] = df["spy_ma200"].pct_change(periods=21) * 100

    return df


def calculate_signal_score(row: pd.Series) -> tuple[float, list[str]]:
    """æ ¹æ“šè¦å‰‡è¨ˆç®—å–®æ—¥ä¿¡è™Ÿåˆ†æ•¸"""
    score = 0.0
    triggered_rules = []

    spy_vs_ma200 = row.get("spy_vs_ma200_pct", 0)
    vix = row.get("vix", 20)
    momentum = row.get("spy_momentum_1m", 0)
    ma200_slope = row.get("ma200_slope", 0)

    # SPY vs 200MA
    if pd.notna(spy_vs_ma200):
        if spy_vs_ma200 < -5:
            score += SIGNAL_RULES["spy_far_below_200ma"]["weight"]
            score += SIGNAL_RULES["spy_below_200ma"]["weight"]
            triggered_rules.append("SPYé ä½æ–¼200MA")
        elif spy_vs_ma200 < 0:
            score += SIGNAL_RULES["spy_below_200ma"]["weight"]
            triggered_rules.append("SPYä½æ–¼200MA")
        else:
            score += SIGNAL_RULES["spy_above_200ma"]["weight"]
            triggered_rules.append("SPYé«˜æ–¼200MA")

    # VIX
    if pd.notna(vix):
        if vix > 35:
            score += SIGNAL_RULES["vix_extreme"]["weight"]
            triggered_rules.append(f"VIXæ¥µé«˜({vix:.0f})")
        elif vix > 25:
            score += SIGNAL_RULES["vix_high"]["weight"]
            triggered_rules.append(f"VIXåé«˜({vix:.0f})")
        elif vix > 20:
            score += SIGNAL_RULES["vix_elevated"]["weight"]
            triggered_rules.append(f"VIXè­¦æˆ’({vix:.0f})")
        elif vix < 15:
            score += SIGNAL_RULES["vix_low"]["weight"]
            triggered_rules.append(f"VIXä½æª”({vix:.0f})")

    # Momentum
    if pd.notna(momentum):
        if momentum < -5:
            score += SIGNAL_RULES["spy_momentum_negative"]["weight"]
            triggered_rules.append(f"å‹•èƒ½è² ({momentum:.1f}%)")
        elif momentum > 3:
            score += SIGNAL_RULES["spy_momentum_positive"]["weight"]
            triggered_rules.append(f"å‹•èƒ½æ­£({momentum:.1f}%)")

    # 200MA æ–œç‡
    if pd.notna(ma200_slope):
        if ma200_slope < -0.5:
            score += SIGNAL_RULES["ma200_declining"]["weight"]
            triggered_rules.append("200MAä¸‹é™")
        elif ma200_slope > 0.5:
            score += SIGNAL_RULES["ma200_rising"]["weight"]
            triggered_rules.append("200MAä¸Šå‡")

    # é™åˆ¶åœ¨ -1 åˆ° 1 ä¹‹é–“
    score = max(-1.0, min(1.0, score))

    return score, triggered_rules


@st.cache_data(ttl=86400)
def calculate_monthly_signals(start_year: int = 2022, end_year: int = 2026) -> dict:
    """è¨ˆç®—æ¯æœˆåˆçš„ä¿¡è™Ÿåˆ†æ•¸"""
    # å–å¾—è¶³å¤ çš„æ­·å²æ•¸æ“š (éœ€è¦200å¤©MA)
    start_date = f"{start_year - 1}-01-01"
    end_date = f"{end_year}-12-31"

    df = fetch_market_indicators(start_date, end_date)

    if df.empty:
        return {}

    monthly_signals = {}

    # å°æ¯å€‹æœˆï¼Œå–æœˆåˆç¬¬ä¸€å€‹äº¤æ˜“æ—¥çš„æ•¸æ“š
    for year in range(start_year, end_year + 1):
        for month in range(1, 13):
            month_key = f"{year}-{month:02d}"

            # æ‰¾è©²æœˆç¬¬ä¸€å€‹äº¤æ˜“æ—¥
            month_start = f"{year}-{month:02d}-01"
            if month == 12:
                month_end = f"{year + 1}-01-01"
            else:
                month_end = f"{year}-{month + 1:02d}-01"

            month_data = df[(df.index >= month_start) & (df.index < month_end)]

            if month_data.empty:
                continue

            # å–æœˆåˆç¬¬ä¸€å€‹äº¤æ˜“æ—¥
            first_day = month_data.iloc[0]
            score, rules = calculate_signal_score(first_day)

            monthly_signals[month_key] = {
                "date": month_data.index[0].strftime("%Y-%m-%d"),
                "score": round(score, 2),
                "rules": rules,
                "spy_close": round(first_day.get("spy_close", 0), 2),
                "spy_vs_ma200": round(first_day.get("spy_vs_ma200_pct", 0), 2),
                "vix": round(first_day.get("vix", 0), 1),
            }

    return monthly_signals


def get_rule_based_allocation(signal_score: float) -> dict:
    """æ ¹æ“šä¿¡è™Ÿåˆ†æ•¸æ±ºå®šé…ç½®é¢¨æ ¼"""
    if signal_score <= -0.4:
        return {
            "style": "æ¥µåº¦é˜²ç¦¦",
            "equity_pct": 20,
            "preferred": ["SHY", "XLV", "XLU", "COST", "JNJ", "PG"],
        }
    elif signal_score <= -0.2:
        return {
            "style": "é˜²ç¦¦",
            "equity_pct": 40,
            "preferred": ["XLV", "XLU", "CEG", "COST", "MSFT", "SHY"],
        }
    elif signal_score <= 0.1:
        return {
            "style": "ä¸­æ€§",
            "equity_pct": 60,
            "preferred": ["MSFT", "GOOGL", "XLV", "CEG", "NVDA", "AAPL"],
        }
    elif signal_score <= 0.3:
        return {
            "style": "åå¤š",
            "equity_pct": 75,
            "preferred": ["NVDA", "MSFT", "GOOGL", "META", "TSM", "AMD"],
        }
    else:
        return {
            "style": "ç©æ¥µ",
            "equity_pct": 90,
            "preferred": ["NVDA", "LITE", "MRVL", "TSM", "AMD", "MU"],
        }


# ========== æœˆåº¦æŒè‚¡æ±  (2022-2026) ==========
# åŸºæ–¼æ¯æœˆåˆå¯å¾—ä¿¡è™Ÿçš„é…ç½® (holdings ä»æ‰‹å‹•ç¶­è­·ï¼Œsignal_score å¯ç”±è¦å‰‡ç³»çµ±è¦†è“‹)
MONTHLY_PORTFOLIOS = {
    # ===== 2022 å¹´ =====
    "2022-01": {
        "signal": "Fedè½‰é·¹ï¼ŒCPI 7%ï¼ŒSPYä»åœ¨é«˜æª”",
        "signal_score": 0.2,
        "holdings": {"MSFT": 0.18, "AAPL": 0.15, "GOOGL": 0.12, "NVDA": 0.10, "XLV": 0.12, "XLF": 0.10, "JPM": 0.08, "XLE": 0.08, "COST": 0.07},
    },
    "2022-02": {
        "signal": "ä¿„çƒæˆ°çˆ­çˆ†ç™¼ï¼ŒVIXé£†å‡ï¼Œæ²¹åƒ¹å¤§æ¼²",
        "signal_score": -0.3,
        "holdings": {"XLE": 0.20, "XLV": 0.18, "XLU": 0.15, "COST": 0.12, "JNJ": 0.10, "PG": 0.10, "SHY": 0.08, "JPM": 0.07},
    },
    "2022-03": {
        "signal": "Fedé¦–æ¬¡å‡æ¯25bpï¼Œé€šè†¨æŒçºŒä¸Šå‡",
        "signal_score": -0.2,
        "holdings": {"XLE": 0.20, "XLV": 0.15, "XLU": 0.12, "XLF": 0.12, "COST": 0.10, "JNJ": 0.10, "PG": 0.08, "JPM": 0.08, "SHY": 0.05},
    },
    "2022-04": {
        "signal": "CPI 8.5%å‰µæ–°é«˜ï¼ŒFedæš—ç¤º50bp",
        "signal_score": -0.4,
        "holdings": {"XLE": 0.22, "XLV": 0.18, "XLU": 0.15, "SHY": 0.12, "COST": 0.10, "JNJ": 0.10, "PG": 0.08, "UNH": 0.05},
    },
    "2022-05": {
        "signal": "Fedå‡æ¯50bpï¼ŒQTé–‹å§‹ï¼ŒSPYè·Œç ´200MA",
        "signal_score": -0.5,
        "holdings": {"XLE": 0.20, "XLV": 0.18, "SHY": 0.18, "XLU": 0.15, "COST": 0.10, "JNJ": 0.10, "PG": 0.09},
    },
    "2022-06": {
        "signal": "CPI 9.1%å³°å€¼ï¼Fedå‡æ¯75bp",
        "signal_score": -0.6,
        "holdings": {"SHY": 0.25, "XLE": 0.18, "XLV": 0.18, "XLU": 0.15, "COST": 0.10, "JNJ": 0.08, "PG": 0.06},
    },
    "2022-07": {
        "signal": "æŠ€è¡“æ€§åå½ˆï¼ŒCPIä»é«˜",
        "signal_score": -0.3,
        "holdings": {"XLE": 0.18, "XLV": 0.18, "SHY": 0.15, "XLU": 0.12, "COST": 0.10, "MSFT": 0.08, "AAPL": 0.08, "JNJ": 0.06, "PG": 0.05},
    },
    "2022-08": {
        "signal": "Jackson Holeé·¹æ´¾ç™¼è¨€ï¼Œåå½ˆçµæŸ",
        "signal_score": -0.5,
        "holdings": {"SHY": 0.22, "XLV": 0.18, "XLE": 0.15, "XLU": 0.15, "COST": 0.10, "JNJ": 0.10, "PG": 0.10},
    },
    "2022-09": {
        "signal": "Fedå‡æ¯75bpï¼Œæš—ç¤ºæ›´é«˜åˆ©ç‡",
        "signal_score": -0.6,
        "holdings": {"SHY": 0.28, "XLV": 0.18, "XLU": 0.18, "XLE": 0.12, "COST": 0.10, "JNJ": 0.08, "PG": 0.06},
    },
    "2022-10": {
        "signal": "SPYæ¥è¿‘å¹´åº¦ä½é»ï¼ŒVIXé«˜æª”",
        "signal_score": -0.4,
        "holdings": {"SHY": 0.22, "XLV": 0.18, "XLU": 0.15, "XLE": 0.12, "COST": 0.10, "MSFT": 0.08, "AAPL": 0.08, "JNJ": 0.07},
    },
    "2022-11": {
        "signal": "CPIé¦–æ¬¡æ”¾ç·©è‡³7.7%ï¼Œå¸‚å ´åå½ˆ",
        "signal_score": -0.1,
        "holdings": {"XLV": 0.15, "MSFT": 0.12, "AAPL": 0.12, "XLE": 0.12, "XLU": 0.10, "COST": 0.10, "GOOGL": 0.08, "SHY": 0.08, "JPM": 0.08, "JNJ": 0.05},
    },
    "2022-12": {
        "signal": "Fedå‡æ¯50bpæ”¾ç·©ï¼Œå¹´åº•ç›¤æ•´",
        "signal_score": 0.0,
        "holdings": {"XLV": 0.15, "MSFT": 0.12, "AAPL": 0.12, "XLE": 0.10, "XLU": 0.10, "COST": 0.10, "GOOGL": 0.08, "JPM": 0.08, "META": 0.08, "JNJ": 0.07},
    },
    # ===== 2023 å¹´ =====
    "2023-01": {
        "signal": "æ–°å¹´æ¨‚è§€æƒ…ç·’ï¼ŒCPIæŒçºŒä¸‹é™",
        "signal_score": 0.2,
        "holdings": {"MSFT": 0.15, "AAPL": 0.12, "GOOGL": 0.12, "META": 0.10, "XLV": 0.12, "NVDA": 0.08, "XLE": 0.08, "JPM": 0.08, "COST": 0.08, "AMD": 0.07},
    },
    "2023-02": {
        "signal": "å°±æ¥­æ•¸æ“šå¼·å‹ï¼Œå‡æ¯æ“”æ†‚å›å‡",
        "signal_score": 0.0,
        "holdings": {"MSFT": 0.14, "AAPL": 0.12, "GOOGL": 0.10, "XLV": 0.12, "META": 0.10, "XLE": 0.10, "JPM": 0.08, "COST": 0.08, "NVDA": 0.08, "XLU": 0.08},
    },
    "2023-03": {
        "signal": "SVBå€’é–‰ï¼éŠ€è¡Œå±æ©Ÿçˆ†ç™¼",
        "signal_score": -0.4,
        "holdings": {"SHY": 0.20, "XLV": 0.18, "XLU": 0.15, "MSFT": 0.12, "AAPL": 0.10, "GOOGL": 0.08, "COST": 0.08, "JNJ": 0.05, "PG": 0.04},
    },
    "2023-04": {
        "signal": "éŠ€è¡Œå±æ©Ÿç·©å’Œï¼ŒFedæš«åœé æœŸ",
        "signal_score": 0.1,
        "holdings": {"MSFT": 0.15, "AAPL": 0.12, "GOOGL": 0.12, "XLV": 0.12, "META": 0.10, "NVDA": 0.10, "COST": 0.08, "XLU": 0.08, "AMD": 0.08, "SHY": 0.05},
    },
    "2023-05": {
        "signal": "AIç†±æ½®åˆç¾ï¼NVDAè²¡å ±é©šè‰·",
        "signal_score": 0.5,
        "holdings": {"NVDA": 0.22, "MSFT": 0.15, "GOOGL": 0.12, "META": 0.12, "AAPL": 0.10, "AMD": 0.10, "TSM": 0.08, "AVGO": 0.06, "XLV": 0.05},
    },
    "2023-06": {
        "signal": "AIé¡ŒææŒçºŒç™¼é…µï¼ŒFedè·³éå‡æ¯",
        "signal_score": 0.6,
        "holdings": {"NVDA": 0.25, "MSFT": 0.15, "GOOGL": 0.12, "META": 0.10, "AMD": 0.10, "TSM": 0.08, "AVGO": 0.08, "AAPL": 0.07, "MU": 0.05},
    },
    "2023-07": {
        "signal": "ç§‘æŠ€è‚¡çºŒå¼·ï¼ŒCPIé™è‡³3%",
        "signal_score": 0.6,
        "holdings": {"NVDA": 0.25, "MSFT": 0.15, "GOOGL": 0.12, "META": 0.10, "AMD": 0.10, "TSM": 0.08, "AVGO": 0.08, "MU": 0.07, "AAPL": 0.05},
    },
    "2023-08": {
        "signal": "ä¸­åœ‹ç¶“æ¿Ÿæ“”æ†‚ï¼Œç¾å‚µæ®–åˆ©ç‡é£†å‡",
        "signal_score": 0.2,
        "holdings": {"NVDA": 0.18, "MSFT": 0.15, "XLV": 0.12, "GOOGL": 0.10, "META": 0.10, "AAPL": 0.08, "AMD": 0.08, "TSM": 0.07, "XLU": 0.07, "AVGO": 0.05},
    },
    "2023-09": {
        "signal": "Higher for longerï¼Œ10Yç ´4.5%",
        "signal_score": -0.1,
        "holdings": {"XLV": 0.15, "NVDA": 0.15, "MSFT": 0.12, "XLU": 0.10, "GOOGL": 0.10, "META": 0.08, "AAPL": 0.08, "COST": 0.08, "SHY": 0.07, "AMD": 0.07},
    },
    "2023-10": {
        "signal": "ä»¥å·´è¡çªï¼Œ10Yè§¸4.9%é«˜é»",
        "signal_score": -0.2,
        "holdings": {"XLV": 0.18, "XLU": 0.12, "MSFT": 0.12, "NVDA": 0.12, "SHY": 0.10, "GOOGL": 0.10, "COST": 0.08, "META": 0.08, "AAPL": 0.05, "JNJ": 0.05},
    },
    "2023-11": {
        "signal": "Fedæš—ç¤ºåœæ­¢å‡æ¯ï¼Œæ®–åˆ©ç‡å›è½",
        "signal_score": 0.4,
        "holdings": {"NVDA": 0.20, "MSFT": 0.15, "GOOGL": 0.12, "META": 0.12, "AMD": 0.10, "TSM": 0.08, "AVGO": 0.08, "AAPL": 0.08, "MU": 0.07},
    },
    "2023-12": {
        "signal": "Fedæš—ç¤º2024é™æ¯ï¼Œè–èª•è¡Œæƒ…",
        "signal_score": 0.6,
        "holdings": {"NVDA": 0.22, "MSFT": 0.15, "META": 0.12, "GOOGL": 0.12, "AMD": 0.10, "TSM": 0.08, "AVGO": 0.08, "MU": 0.08, "AAPL": 0.05},
    },
    # ===== 2024 å¹´ =====
    "2024-01": {
        "signal": "AI CapExæŒ‡å¼•å¼·å‹ï¼ŒVIXä½æª”",
        "signal_score": 0.7,
        "holdings": {"NVDA": 0.30, "MSFT": 0.18, "TSM": 0.12, "AVGO": 0.12, "AMD": 0.10, "GOOGL": 0.10, "META": 0.08},
    },
    "2024-02": {
        "signal": "NVDAè²¡å ±è¶…é æœŸï¼ŒAIéœ€æ±‚ç¢ºèª",
        "signal_score": 0.7,
        "holdings": {"NVDA": 0.32, "MSFT": 0.16, "TSM": 0.12, "AVGO": 0.12, "AMD": 0.10, "GOOGL": 0.10, "META": 0.08},
    },
    "2024-03": {
        "signal": "GTCå¤§æœƒï¼ŒBlackwellç™¼å¸ƒ",
        "signal_score": 0.7,
        "holdings": {"NVDA": 0.30, "MSFT": 0.15, "TSM": 0.12, "AVGO": 0.12, "MU": 0.10, "AMD": 0.08, "GOOGL": 0.08, "META": 0.05},
    },
    "2024-04": {
        "signal": "HBMä¾›éœ€ç·Šå¼µæµ®ç¾",
        "signal_score": 0.6,
        "holdings": {"NVDA": 0.28, "MU": 0.15, "TSM": 0.12, "MSFT": 0.12, "AVGO": 0.10, "AMD": 0.08, "GOOGL": 0.08, "XLV": 0.07},
    },
    "2024-05": {
        "signal": "è¨˜æ†¶é«”é¡ŒææŒçºŒ",
        "signal_score": 0.6,
        "holdings": {"NVDA": 0.25, "MU": 0.18, "TSM": 0.12, "MSFT": 0.12, "AVGO": 0.10, "AMD": 0.08, "GOOGL": 0.08, "XLV": 0.07},
    },
    "2024-06": {
        "signal": "NVDAæˆå…¨çƒå¸‚å€¼æœ€å¤§",
        "signal_score": 0.6,
        "holdings": {"NVDA": 0.25, "MU": 0.15, "TSM": 0.12, "MSFT": 0.12, "AVGO": 0.10, "LITE": 0.08, "AMD": 0.08, "GOOGL": 0.05, "XLV": 0.05},
    },
    "2024-07": {
        "signal": "800Gé‡ç”¢ï¼Œå…‰äº’é€£é¡Œæ",
        "signal_score": 0.6,
        "holdings": {"NVDA": 0.22, "LITE": 0.12, "MU": 0.12, "TSM": 0.12, "MSFT": 0.10, "COHR": 0.08, "VRT": 0.08, "AVGO": 0.08, "XLV": 0.08},
    },
    "2024-08": {
        "signal": "çŸ½å…‰å­é¡Œææ“´æ•£",
        "signal_score": 0.5,
        "holdings": {"NVDA": 0.20, "LITE": 0.15, "MU": 0.12, "TSM": 0.10, "VRT": 0.10, "COHR": 0.08, "MSFT": 0.08, "AVGO": 0.07, "XLV": 0.10},
    },
    "2024-09": {
        "signal": "Fedé™æ¯é æœŸï¼Œå…‰äº’é€£æŒçºŒ",
        "signal_score": 0.5,
        "holdings": {"NVDA": 0.18, "LITE": 0.15, "CEG": 0.10, "MU": 0.10, "TSM": 0.10, "VRT": 0.10, "MSFT": 0.10, "COHR": 0.07, "XLV": 0.10},
    },
    "2024-10": {
        "signal": "æ ¸é›»PPAç°½ç´„æ¶ˆæ¯ï¼Œé›»åŠ›é¡Œæ",
        "signal_score": 0.5,
        "holdings": {"CEG": 0.15, "NVDA": 0.15, "LITE": 0.12, "VST": 0.08, "MU": 0.10, "TSM": 0.10, "VRT": 0.10, "MSFT": 0.10, "XLV": 0.10},
    },
    "2024-11": {
        "signal": "å·æ™®ç•¶é¸ï¼Œé—œç¨…æ“”æ†‚åˆç¾",
        "signal_score": 0.3,
        "holdings": {"CEG": 0.15, "NVDA": 0.12, "LITE": 0.12, "XLV": 0.15, "MSFT": 0.12, "MU": 0.08, "VRT": 0.08, "TSM": 0.08, "GOOGL": 0.10},
    },
    "2024-12": {
        "signal": "å¹´åº•ç²åˆ©äº†çµï¼Œä¼°å€¼æ“”æ†‚",
        "signal_score": 0.2,
        "holdings": {"XLV": 0.15, "CEG": 0.15, "MSFT": 0.12, "NVDA": 0.12, "LITE": 0.10, "GOOGL": 0.10, "XLU": 0.08, "VRT": 0.08, "MU": 0.05, "SHY": 0.05},
    },
    # ===== 2025 å¹´ =====
    "2025-01": {
        "signal": "DeepSeekè¡æ“Šï¼VIXé£†å‡è‡³28",
        "signal_score": -0.4,
        "holdings": {"XLV": 0.25, "XLU": 0.20, "SHY": 0.15, "CEG": 0.12, "MSFT": 0.10, "GOOGL": 0.08, "NVDA": 0.05, "LITE": 0.05},
    },
    "2025-02": {
        "signal": "é—œç¨…å¨è„…å‡ç´šï¼Œå¸‚å ´éœ‡ç›ª",
        "signal_score": -0.3,
        "holdings": {"XLV": 0.22, "XLU": 0.18, "CEG": 0.15, "SHY": 0.12, "MSFT": 0.12, "GOOGL": 0.08, "NVDA": 0.08, "LITE": 0.05},
    },
    "2025-03": {
        "signal": "é—œç¨…è«‡åˆ¤ä¸­ï¼Œç¶­æŒè§€æœ›",
        "signal_score": -0.2,
        "holdings": {"XLV": 0.18, "CEG": 0.15, "XLU": 0.15, "MSFT": 0.12, "NVDA": 0.10, "GOOGL": 0.10, "LITE": 0.08, "SHY": 0.07, "VRT": 0.05},
    },
    # ===== 2025 Q2 (é—œç¨…é¢¨æš´) =====
    "2025-04": {
        "signal": "4/2è§£æ”¾æ—¥é—œç¨…ï¼SPYæš´è·Œï¼ŒVIXé£†è‡³45",
        "signal_score": -0.6,
        "holdings": {"SHY": 0.30, "XLV": 0.20, "XLU": 0.18, "COST": 0.10, "JNJ": 0.08, "PG": 0.07, "CEG": 0.07},
    },
    "2025-05": {
        "signal": "é—œç¨…è«‡åˆ¤åè¦†ï¼Œå¸‚å ´åŠ‡çƒˆéœ‡ç›ª",
        "signal_score": -0.4,
        "holdings": {"SHY": 0.22, "XLV": 0.20, "XLU": 0.15, "CEG": 0.12, "COST": 0.10, "MSFT": 0.08, "JNJ": 0.08, "PG": 0.05},
    },
    "2025-06": {
        "signal": "éƒ¨åˆ†é—œç¨…æš«ç·©90å¤©ï¼Œå¸‚å ´å–˜æ¯",
        "signal_score": -0.2,
        "holdings": {"XLV": 0.18, "CEG": 0.15, "XLU": 0.12, "SHY": 0.12, "MSFT": 0.12, "NVDA": 0.08, "GOOGL": 0.08, "LITE": 0.08, "COST": 0.07},
    },
    # ===== 2025 Q3 (è¬¹æ…å¾©ç”¦) =====
    "2025-07": {
        "signal": "é—œç¨…ä¸ç¢ºå®šæ€§æŒçºŒï¼Œè§€æœ›Q2è²¡å ±",
        "signal_score": -0.1,
        "holdings": {"XLV": 0.15, "CEG": 0.15, "MSFT": 0.12, "NVDA": 0.10, "XLU": 0.10, "GOOGL": 0.10, "LITE": 0.08, "SHY": 0.08, "MU": 0.07, "VRT": 0.05},
    },
    "2025-08": {
        "signal": "AI CapExç¢ºèªæŒçºŒï¼Œç§‘æŠ€è‚¡å›ç©©",
        "signal_score": 0.1,
        "holdings": {"NVDA": 0.15, "MSFT": 0.12, "CEG": 0.12, "LITE": 0.12, "XLV": 0.10, "GOOGL": 0.10, "MU": 0.08, "MRVL": 0.08, "TSM": 0.08, "VRT": 0.05},
    },
    "2025-09": {
        "signal": "Fedç¶­æŒåˆ©ç‡ï¼Œé—œç¨…è«‡åˆ¤æœ‰é€²å±•",
        "signal_score": 0.2,
        "holdings": {"NVDA": 0.15, "LITE": 0.15, "MSFT": 0.12, "CEG": 0.10, "MRVL": 0.10, "MU": 0.10, "GOOGL": 0.08, "TSM": 0.08, "XLV": 0.07, "VRT": 0.05},
    },
    # ===== 2025 Q4 (é€æ­¥å›ç©©) =====
    "2025-10": {
        "signal": "Q3è²¡å ±å„ªæ–¼é æœŸï¼ŒCPOé¡Œæç™¼é…µ",
        "signal_score": 0.3,
        "holdings": {"LITE": 0.18, "NVDA": 0.15, "MRVL": 0.12, "CEG": 0.10, "MSFT": 0.10, "MU": 0.10, "COHR": 0.08, "TSM": 0.07, "GOOGL": 0.05, "VRT": 0.05},
    },
    "2025-11": {
        "signal": "å¸‚å ´å›ç©©ï¼Œå¹´åº•è¡Œæƒ…å•Ÿå‹•",
        "signal_score": 0.4,
        "holdings": {"LITE": 0.18, "NVDA": 0.15, "MRVL": 0.12, "MU": 0.10, "CEG": 0.10, "MSFT": 0.10, "COHR": 0.08, "TSM": 0.07, "GOOGL": 0.05, "META": 0.05},
    },
    "2025-12": {
        "signal": "è–èª•è¡Œæƒ…ï¼Œä½†é—œç¨…ä»æ˜¯è®Šæ•¸",
        "signal_score": 0.3,
        "holdings": {"NVDA": 0.15, "LITE": 0.15, "MRVL": 0.10, "MSFT": 0.10, "CEG": 0.10, "MU": 0.10, "XLV": 0.08, "TSM": 0.08, "COHR": 0.07, "GOOGL": 0.07},
    },
    # ===== 2026 å¹´ =====
    "2026-01": {
        "signal": "æ–°å¹´å±•æœ›ï¼Œé—œç¨…æ”¿ç­–å¾…è§€å¯Ÿ",
        "signal_score": 0.2,
        "holdings": {"NVDA": 0.15, "LITE": 0.12, "MSFT": 0.12, "CEG": 0.10, "XLV": 0.12, "MU": 0.10, "MRVL": 0.08, "TSM": 0.08, "GOOGL": 0.08, "XLU": 0.05},
    },
    "2026-02": {
        "signal": "SPYç©©ç«™200MAä¸Šæ–¹ï¼ŒVIXä½æª”ï¼Œåå¤šæ“ä½œ",
        "signal_score": 0.2,
        "holdings": {"NVDA": 0.18, "LITE": 0.15, "MRVL": 0.12, "MU": 0.10, "CEG": 0.10, "MSFT": 0.10, "TSM": 0.08, "COHR": 0.07, "GOOGL": 0.05, "XLV": 0.05},
    },
}

def get_monthly_periods(start_month: str, end_month: str) -> list:
    """å–å¾—æœˆä»½åˆ—è¡¨"""
    months = list(MONTHLY_PORTFOLIOS.keys())
    try:
        start_idx = months.index(start_month)
        end_idx = months.index(end_month)
        return months[start_idx:end_idx+1]
    except ValueError:
        return []


@st.cache_data(ttl=3600)
def fetch_stock_prices(symbols: list, start_date: str, end_date: str) -> pd.DataFrame:
    """å–å¾—è‚¡ç¥¨æ­·å²åƒ¹æ ¼"""
    import yfinance as yf

    all_data = {}
    for symbol in symbols:
        try:
            ticker = yf.Ticker(symbol)
            df = ticker.history(start=start_date, end=end_date)
            if not df.empty:
                all_data[symbol] = df['Close']
        except Exception as e:
            st.warning(f"ç„¡æ³•å–å¾— {symbol} æ•¸æ“š: {e}")

    if all_data:
        return pd.DataFrame(all_data)
    return pd.DataFrame()


def calculate_portfolio_returns(prices_df: pd.DataFrame, weights: dict) -> pd.Series:
    """è¨ˆç®—æŠ•è³‡çµ„åˆå ±é…¬"""
    # åªä½¿ç”¨æœ‰æ•¸æ“šçš„è‚¡ç¥¨
    available = [s for s in weights.keys() if s in prices_df.columns]
    if not available:
        return pd.Series()

    # é‡æ–°æ­£è¦åŒ–æ¬Šé‡
    total_weight = sum(weights[s] for s in available)
    norm_weights = {s: weights[s] / total_weight for s in available}

    # è¨ˆç®—æ—¥å ±é…¬
    returns = prices_df[available].pct_change()

    # åŠ æ¬Šå ±é…¬
    portfolio_returns = pd.Series(0, index=returns.index)
    for symbol, weight in norm_weights.items():
        portfolio_returns += returns[symbol] * weight

    return portfolio_returns


def calculate_metrics(returns: pd.Series) -> dict:
    """è¨ˆç®—ç¸¾æ•ˆæŒ‡æ¨™"""
    if returns.empty or len(returns) < 2:
        return {"total_return": 0, "annualized_return": 0, "volatility": 0,
                "sharpe": 0, "max_drawdown": 0, "win_rate": 0}

    returns = returns.dropna()
    if len(returns) < 2:
        return {"total_return": 0, "annualized_return": 0, "volatility": 0,
                "sharpe": 0, "max_drawdown": 0, "win_rate": 0}

    # ç¸½å ±é…¬
    cumulative = (1 + returns).cumprod()
    total_return = (cumulative.iloc[-1] - 1) * 100

    # å¹´åŒ–å ±é…¬ (å‡è¨­252äº¤æ˜“æ—¥)
    days = len(returns)
    annualized_return = ((1 + total_return/100) ** (252/days) - 1) * 100 if days > 0 else 0

    # æ³¢å‹•ç‡ (å¹´åŒ–)
    volatility = returns.std() * (252 ** 0.5) * 100

    # å¤æ™®æ¯”ç‡ (å‡è¨­ç„¡é¢¨éšªåˆ©ç‡ 4%)
    risk_free = 0.04 / 252
    excess_returns = returns - risk_free
    sharpe = (excess_returns.mean() / returns.std()) * (252 ** 0.5) if returns.std() > 0 else 0

    # æœ€å¤§å›æ’¤
    cummax = cumulative.cummax()
    drawdown = (cumulative - cummax) / cummax
    max_drawdown = drawdown.min() * 100

    # å‹ç‡
    win_rate = (returns > 0).sum() / len(returns) * 100

    return {
        "total_return": total_return,
        "annualized_return": annualized_return,
        "volatility": volatility,
        "sharpe": sharpe,
        "max_drawdown": max_drawdown,
        "win_rate": win_rate,
    }


def render_quarterly_backtest_page():
    """æ¸²æŸ“æŒè‚¡æ± å›æ¸¬é é¢"""
    st.title("ğŸ“Š æŒè‚¡æ± å›æ¸¬ç³»çµ±")
    st.markdown("**åŸºæ–¼è¶¨å‹¢é›·é”åˆ†æçš„æŠ•è³‡çµ„åˆå›æ¸¬**")

    # æ›è‚¡é »ç‡é¸æ“‡
    freq_tab1, freq_tab2, freq_tab3 = st.tabs(["ğŸ“… å­£åº¦æ›è‚¡", "ğŸ“† æœˆåº¦æ›è‚¡", "ğŸ“ è¦å‰‡ä¿¡è™Ÿ"])

    with freq_tab1:
        render_quarterly_backtest()

    with freq_tab2:
        render_monthly_backtest()

    with freq_tab3:
        render_rule_based_signals()


def render_quarterly_backtest():
    """å­£åº¦å›æ¸¬"""
    st.markdown("#### å­£åº¦æ›è‚¡å›æ¸¬ (2022-2026)")

    # é¸æ“‡å›æ¸¬ç¯„åœ
    col1, col2 = st.columns(2)
    with col1:
        quarters = list(QUARTERLY_PORTFOLIOS.keys())
        start_q = st.selectbox("èµ·å§‹å­£åº¦", quarters, index=0, key="q_start")
    with col2:
        start_idx = quarters.index(start_q)
        end_q = st.selectbox("çµæŸå­£åº¦", quarters[start_idx:], index=len(quarters[start_idx:])-1, key="q_end")

    # é¸æ“‡åŸºæº–
    benchmark = st.selectbox(
        "æ¯”è¼ƒåŸºæº–",
        list(BENCHMARK_SYMBOLS.keys()),
        format_func=lambda x: f"{x} - {BENCHMARK_SYMBOLS[x]}",
        key="q_bench"
    )

    # ç­–ç•¥é¸æ“‡
    st.markdown("---")
    st.markdown("#### ğŸ¯ ç†Šå¸‚ç­–ç•¥é¸æ“‡")

    strategy = st.radio(
        "ç•¶ä¿¡è™Ÿåˆ†æ•¸ â‰¤ -0.2 (ç†Šå¸‚ä¿¡è™Ÿ) æ™‚ï¼š",
        ["ğŸ›¡ï¸ ç†Šå¸‚é˜²ç¦¦", "ğŸ’µ ç†Šå¸‚ç©ºæ‰‹", "ğŸ“Š å…©è€…æ¯”è¼ƒ"],
        index=2,
        horizontal=True,
        help="ç†Šå¸‚é˜²ç¦¦ï¼šæŒæœ‰é˜²ç¦¦è‚¡(XLV/XLU/SHY)ï¼›ç†Šå¸‚ç©ºæ‰‹ï¼š100%ç¾é‡‘(SHY)",
        key="q_strategy"
    )

    # ç©ºæ‰‹é–¾å€¼
    if strategy in ["ğŸ’µ ç†Šå¸‚ç©ºæ‰‹", "ğŸ“Š å…©è€…æ¯”è¼ƒ"]:
        cash_threshold = st.slider(
            "ç©ºæ‰‹ä¿¡è™Ÿé–¾å€¼",
            min_value=-0.5,
            max_value=0.0,
            value=-0.2,
            step=0.1,
            help="ä¿¡è™Ÿåˆ†æ•¸ä½æ–¼æ­¤å€¼æ™‚ï¼Œè½‰ç‚º100%ç¾é‡‘",
            key="q_threshold"
        )
    else:
        cash_threshold = -0.2

    if st.button("ğŸš€ é–‹å§‹å­£åº¦å›æ¸¬", type="primary", use_container_width=True, key="q_run"):
        with st.spinner("æ­£åœ¨å–å¾—æ•¸æ“šä¸¦è¨ˆç®—..."):
            run_backtest(start_q, end_q, benchmark, strategy, cash_threshold)


def render_monthly_backtest():
    """æœˆåº¦å›æ¸¬"""
    st.markdown("#### æœˆåº¦æ›è‚¡å›æ¸¬ (2022-2026)")
    st.info("ğŸ’¡ æœˆåº¦æ›è‚¡æ›´éˆæ´»ï¼Œé©åˆä¸»å‹•ç®¡ç†ã€‚æ¯æœˆåˆæ ¹æ“šä¿¡è™Ÿèª¿æ•´æŒè‚¡ã€‚")

    # ä¿¡è™Ÿä¾†æºé¸æ“‡
    signal_source = st.radio(
        "ğŸ“¡ ä¿¡è™Ÿä¾†æº",
        ["ğŸ“ æ‰‹å‹•ä¿¡è™Ÿ (äººå·¥åˆ¤æ–·)", "ğŸ“ è¦å‰‡ä¿¡è™Ÿ (è‡ªå‹•è¨ˆç®—)"],
        index=0,
        horizontal=True,
        help="æ‰‹å‹•ä¿¡è™Ÿï¼šä½¿ç”¨é è¨­çš„ signal_scoreï¼›è¦å‰‡ä¿¡è™Ÿï¼šæ ¹æ“š SPY/VIX ç­‰æŒ‡æ¨™è‡ªå‹•è¨ˆç®—",
        key="m_signal_source"
    )
    use_rule_signals = signal_source == "ğŸ“ è¦å‰‡ä¿¡è™Ÿ (è‡ªå‹•è¨ˆç®—)"

    # é¸æ“‡å›æ¸¬ç¯„åœ
    col1, col2 = st.columns(2)
    months = list(MONTHLY_PORTFOLIOS.keys())
    with col1:
        start_m = st.selectbox("èµ·å§‹æœˆä»½", months, index=0, key="m_start")
    with col2:
        start_idx = months.index(start_m)
        end_m = st.selectbox("çµæŸæœˆä»½", months[start_idx:], index=len(months[start_idx:])-1, key="m_end")

    # é¸æ“‡åŸºæº–
    benchmark = st.selectbox(
        "æ¯”è¼ƒåŸºæº–",
        list(BENCHMARK_SYMBOLS.keys()),
        format_func=lambda x: f"{x} - {BENCHMARK_SYMBOLS[x]}",
        key="m_bench"
    )

    # ç†Šå¸‚ç­–ç•¥
    st.markdown("---")
    strategy = st.radio(
        "ç•¶ä¿¡è™Ÿåˆ†æ•¸ â‰¤ -0.2 (ç†Šå¸‚ä¿¡è™Ÿ) æ™‚ï¼š",
        ["ğŸ›¡ï¸ ç†Šå¸‚é˜²ç¦¦", "ğŸ’µ ç†Šå¸‚ç©ºæ‰‹", "ğŸ“Š å…©è€…æ¯”è¼ƒ"],
        index=2,
        horizontal=True,
        help="ç†Šå¸‚é˜²ç¦¦ï¼šæŒ‰é…ç½®æŒæœ‰é˜²ç¦¦è‚¡ï¼›ç†Šå¸‚ç©ºæ‰‹ï¼š100%ç¾é‡‘(SHY)",
        key="m_strategy"
    )

    # ç©ºæ‰‹é–¾å€¼
    if strategy in ["ğŸ’µ ç†Šå¸‚ç©ºæ‰‹", "ğŸ“Š å…©è€…æ¯”è¼ƒ"]:
        cash_threshold = st.slider(
            "ç©ºæ‰‹ä¿¡è™Ÿé–¾å€¼",
            min_value=-0.5,
            max_value=0.0,
            value=-0.2,
            step=0.1,
            help="ä¿¡è™Ÿåˆ†æ•¸ä½æ–¼æ­¤å€¼æ™‚ï¼Œè½‰ç‚º100%ç¾é‡‘",
            key="m_threshold"
        )
    else:
        cash_threshold = -0.2

    col1, col2 = st.columns(2)
    with col1:
        if st.button("ğŸš€ é–‹å§‹æœˆåº¦å›æ¸¬", type="primary", use_container_width=True, key="m_run"):
            with st.spinner("æ­£åœ¨å–å¾—æ•¸æ“šä¸¦è¨ˆç®—..."):
                run_monthly_backtest(start_m, end_m, benchmark, strategy, cash_threshold, use_rule_signals)

    with col2:
        if st.button("ğŸ“‹ æŸ¥çœ‹æœ¬æœˆå»ºè­°", use_container_width=True, key="m_suggest"):
            show_current_month_suggestion()


def show_current_month_suggestion():
    """é¡¯ç¤ºç•¶æœˆæ›è‚¡å»ºè­°"""
    from datetime import datetime
    current_month = datetime.now().strftime("%Y-%m")

    st.markdown("---")
    st.markdown("### ğŸ“‹ ç•¶æœˆæŒè‚¡å»ºè­°")

    if current_month in MONTHLY_PORTFOLIOS:
        m_info = MONTHLY_PORTFOLIOS[current_month]
        signal_score = m_info["signal_score"]

        # ä¿¡è™Ÿç‡ˆè™Ÿ
        if signal_score >= 0.5:
            light = "ğŸŸ¢ ç©æ¥µ"
        elif signal_score >= 0.2:
            light = "ğŸŸ¡ åå¤š"
        elif signal_score >= -0.2:
            light = "âšª ä¸­æ€§"
        elif signal_score >= -0.4:
            light = "ğŸŸ  åç©º"
        else:
            light = "ğŸ”´ é˜²ç¦¦"

        st.markdown(f"**æœˆä»½**: {current_month}")
        st.markdown(f"**ä¿¡è™Ÿ**: {m_info['signal']}")
        st.markdown(f"**ä¿¡è™Ÿåˆ†æ•¸**: {signal_score:+.1f} ({light})")

        st.markdown("**å»ºè­°æŒè‚¡é…ç½®:**")
        holdings_df = pd.DataFrame([
            {
                "è‚¡ç¥¨": s,
                "æ¬Šé‡": f"{w*100:.0f}%",
                "å…¬å¸": STOCK_DETAILS.get(s, {}).get("name", s)
            }
            for s, w in sorted(m_info["holdings"].items(), key=lambda x: x[1], reverse=True)
        ])
        st.dataframe(holdings_df, use_container_width=True, hide_index=True)

        # ä¸‹æœˆé è¦½
        next_month_candidates = [m for m in MONTHLY_PORTFOLIOS.keys() if m > current_month]
        if next_month_candidates:
            next_month = next_month_candidates[0]
            next_info = MONTHLY_PORTFOLIOS[next_month]
            with st.expander(f"ğŸ“… ä¸‹æœˆé è¦½ ({next_month})"):
                st.markdown(f"**ä¿¡è™Ÿ**: {next_info['signal']}")
                st.markdown(f"**ä¿¡è™Ÿåˆ†æ•¸**: {next_info['signal_score']:+.1f}")
    else:
        # æ‰¾æœ€è¿‘çš„æœˆä»½
        past_months = [m for m in MONTHLY_PORTFOLIOS.keys() if m <= current_month]
        if past_months:
            latest = past_months[-1]
            m_info = MONTHLY_PORTFOLIOS[latest]
            st.warning(f"ç•¶æœˆ ({current_month}) å°šç„¡é…ç½®ï¼Œé¡¯ç¤ºæœ€è¿‘é…ç½® ({latest})")
            st.markdown(f"**ä¿¡è™Ÿ**: {m_info['signal']}")

            holdings_df = pd.DataFrame([
                {"è‚¡ç¥¨": s, "æ¬Šé‡": f"{w*100:.0f}%", "å…¬å¸": STOCK_DETAILS.get(s, {}).get("name", s)}
                for s, w in sorted(m_info["holdings"].items(), key=lambda x: x[1], reverse=True)
            ])
            st.dataframe(holdings_df, use_container_width=True, hide_index=True)
        else:
            st.error("ç„¡å¯ç”¨é…ç½®")


def render_rule_based_signals():
    """é¡¯ç¤ºè¦å‰‡åŒ–ä¿¡è™Ÿç³»çµ±"""
    st.markdown("#### ğŸ“ è¦å‰‡åŒ–ä¿¡è™Ÿç³»çµ±")
    st.info("""
    **ç„¡å¾Œè¦‹ä¹‹æ˜çš„ä¿¡è™Ÿç³»çµ±** - æ‰€æœ‰ä¿¡è™ŸåŸºæ–¼æœˆåˆç¬¬ä¸€å€‹äº¤æ˜“æ—¥å¯å¾—çš„å¸‚å ´æ•¸æ“šè‡ªå‹•è¨ˆç®—ï¼Œ
    ä¸ä¾è³´äººå·¥åˆ¤æ–·ï¼Œé¿å…äº‹å¾Œè«¸è‘›çš„åèª¤ã€‚
    """)

    # é¡¯ç¤ºè¦å‰‡å®šç¾©
    with st.expander("ğŸ“‹ ä¿¡è™Ÿè¦å‰‡å®šç¾©", expanded=False):
        rules_data = []
        for rule_id, rule in SIGNAL_RULES.items():
            rules_data.append({
                "è¦å‰‡": rule["description"],
                "æ¬Šé‡": f"{rule['weight']:+.2f}",
                "é¡å‹": "ç©ºæ–¹" if rule["weight"] < 0 else "å¤šæ–¹"
            })
        st.dataframe(pd.DataFrame(rules_data), use_container_width=True, hide_index=True)

        st.markdown("""
        **è¨ˆç®—é‚è¼¯ï¼š**
        - æ¯æœˆåˆç¬¬ä¸€å€‹äº¤æ˜“æ—¥ï¼Œæª¢æŸ¥å„é …æŒ‡æ¨™
        - è§¸ç™¼çš„è¦å‰‡æ¬Šé‡åŠ ç¸½ = ä¿¡è™Ÿåˆ†æ•¸
        - åˆ†æ•¸ç¯„åœï¼š-1.0 (æ¥µåº¦çœ‹ç©º) åˆ° +1.0 (æ¥µåº¦çœ‹å¤š)
        - åˆ†æ•¸ â‰¤ -0.2 è¦–ç‚ºç†Šå¸‚ä¿¡è™Ÿ
        """)

    # è¨ˆç®—ä¿¡è™Ÿ
    st.markdown("---")
    col1, col2 = st.columns(2)
    with col1:
        start_year = st.selectbox("èµ·å§‹å¹´ä»½", [2022, 2023, 2024, 2025, 2026], index=0, key="rule_start_year")
    with col2:
        end_year = st.selectbox("çµæŸå¹´ä»½", [2022, 2023, 2024, 2025, 2026], index=4, key="rule_end_year")

    if st.button("ğŸ”„ è¨ˆç®—è¦å‰‡ä¿¡è™Ÿ", type="primary", use_container_width=True, key="calc_rules"):
        with st.spinner("æ­£åœ¨å–å¾—å¸‚å ´æ•¸æ“šä¸¦è¨ˆç®—ä¿¡è™Ÿ..."):
            signals = calculate_monthly_signals(start_year, end_year)

            if not signals:
                st.error("ç„¡æ³•å–å¾—å¸‚å ´æ•¸æ“š")
                return

            # é¡¯ç¤ºä¿¡è™Ÿçµæœ
            st.markdown("### ğŸ“Š è¨ˆç®—çµæœ")

            # èˆ‡æ‰‹å‹•ä¿¡è™Ÿæ¯”è¼ƒ
            st.markdown("#### è¦å‰‡ä¿¡è™Ÿ vs æ‰‹å‹•ä¿¡è™Ÿ")

            compare_data = []
            for month_key in sorted(signals.keys()):
                sig = signals[month_key]
                manual_score = MONTHLY_PORTFOLIOS.get(month_key, {}).get("signal_score", None)
                manual_signal = MONTHLY_PORTFOLIOS.get(month_key, {}).get("signal", "N/A")

                rule_score = sig["score"]

                # åˆ¤æ–·æ˜¯å¦ä¸€è‡´
                rule_bear = rule_score <= -0.2
                manual_bear = manual_score <= -0.2 if manual_score is not None else None

                if manual_bear is None:
                    match = "âšª"
                elif rule_bear == manual_bear:
                    match = "âœ…"
                else:
                    match = "âŒ"

                compare_data.append({
                    "æœˆä»½": month_key,
                    "è¦å‰‡åˆ†æ•¸": f"{rule_score:+.2f}",
                    "æ‰‹å‹•åˆ†æ•¸": f"{manual_score:+.2f}" if manual_score is not None else "N/A",
                    "ä¸€è‡´": match,
                    "è§¸ç™¼è¦å‰‡": ", ".join(sig["rules"][:3]),
                    "SPY": f"${sig['spy_close']:.0f}",
                    "vs200MA": f"{sig['spy_vs_ma200']:+.1f}%",
                    "VIX": f"{sig['vix']:.0f}",
                })

            df = pd.DataFrame(compare_data)
            st.dataframe(df, use_container_width=True, hide_index=True, height=400)

            # çµ±è¨ˆä¸€è‡´æ€§
            matches = [d["ä¸€è‡´"] for d in compare_data if d["ä¸€è‡´"] != "âšª"]
            if matches:
                match_rate = sum(1 for m in matches if m == "âœ…") / len(matches) * 100
                st.metric("ç†Šå¸‚/å¤šé ­åˆ¤æ–·ä¸€è‡´ç‡", f"{match_rate:.0f}%")

            # ç¹ªè£½ä¿¡è™Ÿèµ°å‹¢åœ–
            st.markdown("#### ğŸ“ˆ ä¿¡è™Ÿåˆ†æ•¸èµ°å‹¢")
            fig = go.Figure()

            months = sorted(signals.keys())
            rule_scores = [signals[m]["score"] for m in months]

            fig.add_trace(go.Scatter(
                x=months, y=rule_scores,
                name="è¦å‰‡ä¿¡è™Ÿ", mode="lines+markers",
                line=dict(color="#2196F3", width=2)
            ))

            # æ‰‹å‹•ä¿¡è™Ÿ
            manual_scores = [MONTHLY_PORTFOLIOS.get(m, {}).get("signal_score", None) for m in months]
            manual_scores_clean = [s if s is not None else 0 for s in manual_scores]
            fig.add_trace(go.Scatter(
                x=months, y=manual_scores_clean,
                name="æ‰‹å‹•ä¿¡è™Ÿ", mode="lines+markers",
                line=dict(color="#FF9800", width=2, dash="dot")
            ))

            # ç†Šå¸‚é–¾å€¼ç·š
            fig.add_hline(y=-0.2, line_dash="dash", line_color="red", annotation_text="ç†Šå¸‚é–¾å€¼")
            fig.add_hline(y=0, line_dash="solid", line_color="gray", opacity=0.3)

            fig.update_layout(
                xaxis_title="æœˆä»½",
                yaxis_title="ä¿¡è™Ÿåˆ†æ•¸",
                hovermode="x unified",
                height=400,
                yaxis=dict(range=[-1, 1])
            )
            st.plotly_chart(fig, use_container_width=True)

            # é¡¯ç¤ºä¸ä¸€è‡´çš„æœˆä»½
            disagreements = [d for d in compare_data if d["ä¸€è‡´"] == "âŒ"]
            if disagreements:
                st.markdown("#### âš ï¸ åˆ¤æ–·ä¸ä¸€è‡´çš„æœˆä»½")
                st.dataframe(pd.DataFrame(disagreements), use_container_width=True, hide_index=True)


def run_monthly_backtest(start_m: str, end_m: str, benchmark: str, strategy: str, cash_threshold: float, use_rule_signals: bool = False):
    """åŸ·è¡Œæœˆåº¦å›æ¸¬"""
    selected_months = get_monthly_periods(start_m, end_m)

    if not selected_months:
        st.error("ç„¡æ•ˆçš„æœˆä»½ç¯„åœ")
        return

    # å¦‚æœä½¿ç”¨è¦å‰‡ä¿¡è™Ÿï¼Œå…ˆè¨ˆç®—
    rule_signals = {}
    if use_rule_signals:
        with st.spinner("è¨ˆç®—è¦å‰‡ä¿¡è™Ÿä¸­..."):
            start_year = int(start_m.split("-")[0])
            end_year = int(end_m.split("-")[0])
            rule_signals = calculate_monthly_signals(start_year, end_year)
            if not rule_signals:
                st.error("ç„¡æ³•è¨ˆç®—è¦å‰‡ä¿¡è™Ÿï¼Œæ”¹ç”¨æ‰‹å‹•ä¿¡è™Ÿ")
                use_rule_signals = False
            else:
                st.success(f"âœ… å·²è¨ˆç®— {len(rule_signals)} å€‹æœˆçš„è¦å‰‡ä¿¡è™Ÿ")

    # æ”¶é›†æ‰€æœ‰éœ€è¦çš„è‚¡ç¥¨
    all_symbols = set([benchmark, "SHY"])
    for m in selected_months:
        all_symbols.update(MONTHLY_PORTFOLIOS[m]["holdings"].keys())

    # å–å¾—åƒ¹æ ¼æ•¸æ“š
    start_date = f"{start_m}-01"
    # è¨ˆç®—çµæŸæ—¥æœŸ (ä¸‹æœˆç¬¬ä¸€å¤©)
    end_year, end_month = map(int, end_m.split("-"))
    if end_month == 12:
        end_date = f"{end_year + 1}-01-01"
    else:
        end_date = f"{end_year}-{end_month + 1:02d}-01"

    st.info(f"ğŸ“… å›æ¸¬æœŸé–“: {start_date} ~ {end_date}")

    prices_df = fetch_stock_prices(list(all_symbols), start_date, end_date)

    if prices_df.empty:
        st.error("ç„¡æ³•å–å¾—è‚¡åƒ¹æ•¸æ“š")
        return

    cash_holdings = {"SHY": 1.0}

    # åˆ¤æ–·æ˜¯å¦æ¯”è¼ƒæ¨¡å¼
    is_compare = strategy == "ğŸ“Š å…©è€…æ¯”è¼ƒ"
    strategies_to_run = ["ğŸ›¡ï¸ ç†Šå¸‚é˜²ç¦¦", "ğŸ’µ ç†Šå¸‚ç©ºæ‰‹"] if is_compare else [strategy]

    # å„²å­˜å…©ç¨®ç­–ç•¥çµæœ
    strategy_results = {}

    for strat in strategies_to_run:
        monthly_results = []
        all_returns = pd.Series(dtype=float)
        bear_months = []

        for m in selected_months:
            m_info = MONTHLY_PORTFOLIOS[m]

            # æ ¹æ“šä¿¡è™Ÿä¾†æºæ±ºå®š signal_score
            if use_rule_signals and m in rule_signals:
                signal_score = rule_signals[m]["score"]
                signal_desc = f"[è¦å‰‡] {', '.join(rule_signals[m]['rules'][:2])}"
            else:
                signal_score = m_info["signal_score"]
                signal_desc = m_info["signal"]

            is_bear = signal_score <= cash_threshold

            if is_bear and m not in bear_months:
                bear_months.append(m)

            # è¨ˆç®—è©²æœˆæ—¥æœŸç¯„åœ
            year, month = map(int, m.split("-"))
            m_start = f"{m}-01"
            if month == 12:
                m_end = f"{year + 1}-01-01"
            else:
                m_end = f"{year}-{month + 1:02d}-01"

            mask = (prices_df.index >= m_start) & (prices_df.index < m_end)
            m_prices = prices_df[mask]

            if m_prices.empty:
                continue

            # æ ¹æ“šç­–ç•¥é¸æ“‡æŒè‚¡
            if is_bear and strat == "ğŸ’µ ç†Šå¸‚ç©ºæ‰‹":
                holdings = cash_holdings
                status = "ğŸ’µ ç©ºæ‰‹"
            else:
                holdings = m_info["holdings"]
                status = "ğŸ”´ é˜²ç¦¦" if is_bear else "ğŸ“ˆ æŒè‚¡"

            # è¨ˆç®—å ±é…¬
            port_returns = calculate_portfolio_returns(m_prices, holdings)
            bench_returns = m_prices[benchmark].pct_change() if benchmark in m_prices.columns else pd.Series()

            p_metrics = calculate_metrics(port_returns)
            b_metrics = calculate_metrics(bench_returns)

            monthly_results.append({
                "æœˆä»½": m,
                "ä¿¡è™Ÿ": signal_desc[:25] + "..." if len(signal_desc) > 25 else signal_desc,
                "åˆ†æ•¸": f"{signal_score:+.1f}",
                "ç‹€æ…‹": status,
                "æŠ•çµ„": f"{p_metrics['total_return']:.1f}%",
                benchmark: f"{b_metrics.get('total_return', 0):.1f}%",
                "Alpha": f"{p_metrics['total_return'] - b_metrics.get('total_return', 0):+.1f}%",
            })

            all_returns = pd.concat([all_returns, port_returns])

        strategy_results[strat] = {
            "monthly_results": monthly_results,
            "all_returns": all_returns,
            "bear_months": bear_months
        }

    # å–å¾—ç†Šå¸‚æœˆä»½ (å…©ç­–ç•¥ç›¸åŒ)
    bear_months = strategy_results[strategies_to_run[0]]["bear_months"]

    # é¡¯ç¤ºç†Šå¸‚æœˆä»½
    if bear_months:
        st.warning(f"ğŸ”´ **ç†Šå¸‚æœˆä»½** (ä¿¡è™Ÿ â‰¤ {cash_threshold}): {', '.join(bear_months)}")

    # å…¨æœŸåŸºæº–å ±é…¬
    full_bench_returns = prices_df[benchmark].pct_change().dropna() if benchmark in prices_df.columns else pd.Series()
    full_b_metrics = calculate_metrics(full_bench_returns)

    if is_compare:
        # ===== æ¯”è¼ƒæ¨¡å¼ =====
        st.markdown("### ğŸ“Š ç­–ç•¥æ¯”è¼ƒ")

        # ç¸¾æ•ˆå°æ¯”è¡¨
        compare_data = []
        for strat in strategies_to_run:
            all_returns = strategy_results[strat]["all_returns"]
            p_metrics = calculate_metrics(all_returns)
            compare_data.append({
                "ç­–ç•¥": strat,
                "ç¸½å ±é…¬": f"{p_metrics['total_return']:.1f}%",
                "Alpha": f"{p_metrics['total_return'] - full_b_metrics['total_return']:+.1f}%",
                "å¤æ™®æ¯”ç‡": f"{p_metrics['sharpe']:.2f}",
                "æœ€å¤§å›æ’¤": f"{p_metrics['max_drawdown']:.1f}%",
                "æ³¢å‹•ç‡": f"{p_metrics['volatility']:.1f}%",
                "å‹ç‡": f"{p_metrics['win_rate']:.0f}%",
            })

        st.dataframe(pd.DataFrame(compare_data), use_container_width=True, hide_index=True)

        # ç¹ªè£½æ¯”è¼ƒåœ–è¡¨
        st.markdown("### ğŸ“‰ ç´¯ç©å ±é…¬èµ°å‹¢æ¯”è¼ƒ")
        fig = go.Figure()

        colors = {"ğŸ›¡ï¸ ç†Šå¸‚é˜²ç¦¦": "#2196F3", "ğŸ’µ ç†Šå¸‚ç©ºæ‰‹": "#4CAF50"}
        for strat in strategies_to_run:
            all_returns = strategy_results[strat]["all_returns"]
            if not all_returns.empty:
                port_cum = (1 + all_returns).cumprod()
                fig.add_trace(go.Scatter(
                    x=port_cum.index, y=(port_cum - 1) * 100,
                    name=strat, line=dict(color=colors.get(strat, "#999"), width=2)
                ))

        if not full_bench_returns.empty:
            bench_cum = (1 + full_bench_returns).cumprod()
            fig.add_trace(go.Scatter(
                x=bench_cum.index, y=(bench_cum - 1) * 100,
                name=benchmark, line=dict(color="#FF9800", width=2, dash="dot")
            ))

        # æ¨™è¨˜ç†Šå¸‚æœˆä»½
        for m in bear_months:
            year, month = map(int, m.split("-"))
            m_start = f"{m}-01"
            if month == 12:
                m_end = f"{year + 1}-01-01"
            else:
                m_end = f"{year}-{month + 1:02d}-01"
            fig.add_vrect(x0=m_start, x1=m_end, fillcolor="red", opacity=0.1, line_width=0)

        fig.update_layout(
            xaxis_title="æ—¥æœŸ", yaxis_title="ç´¯ç©å ±é…¬ (%)",
            hovermode="x unified", height=400,
            legend=dict(orientation="h", yanchor="bottom", y=1.02)
        )
        st.plotly_chart(fig, use_container_width=True)

        # æœˆåº¦ç¸¾æ•ˆæ¯”è¼ƒ
        st.markdown("### ğŸ“ˆ æœˆåº¦ç¸¾æ•ˆæ¯”è¼ƒ")
        tab1, tab2 = st.tabs(["ğŸ›¡ï¸ ç†Šå¸‚é˜²ç¦¦", "ğŸ’µ ç†Šå¸‚ç©ºæ‰‹"])
        with tab1:
            st.dataframe(pd.DataFrame(strategy_results["ğŸ›¡ï¸ ç†Šå¸‚é˜²ç¦¦"]["monthly_results"]), use_container_width=True, hide_index=True)
        with tab2:
            st.dataframe(pd.DataFrame(strategy_results["ğŸ’µ ç†Šå¸‚ç©ºæ‰‹"]["monthly_results"]), use_container_width=True, hide_index=True)

    else:
        # ===== å–®ä¸€ç­–ç•¥æ¨¡å¼ =====
        monthly_results = strategy_results[strategy]["monthly_results"]
        all_returns = strategy_results[strategy]["all_returns"]

        # é¡¯ç¤ºçµæœ
        st.markdown("### ğŸ“ˆ æœˆåº¦ç¸¾æ•ˆ")
        st.dataframe(pd.DataFrame(monthly_results), use_container_width=True, hide_index=True)

        # å…¨æœŸç¸¾æ•ˆ
        full_p_metrics = calculate_metrics(all_returns)

        st.markdown("### ğŸ“Š å…¨æœŸç¸¾æ•ˆæ‘˜è¦")
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("æŠ•çµ„ç¸½å ±é…¬", f"{full_p_metrics['total_return']:.1f}%")
        with col2:
            alpha = full_p_metrics['total_return'] - full_b_metrics['total_return']
            st.metric("Alpha", f"{alpha:+.1f}%")
        with col3:
            st.metric("å¤æ™®æ¯”ç‡", f"{full_p_metrics['sharpe']:.2f}")
        with col4:
            st.metric("æœ€å¤§å›æ’¤", f"{full_p_metrics['max_drawdown']:.1f}%")

        # ç¹ªè£½åœ–è¡¨
        st.markdown("### ğŸ“‰ ç´¯ç©å ±é…¬èµ°å‹¢")
        fig = go.Figure()

        port_cum = (1 + all_returns).cumprod()
        fig.add_trace(go.Scatter(
            x=port_cum.index, y=(port_cum - 1) * 100,
            name="æœˆåº¦æ›è‚¡", line=dict(color="#2196F3", width=2)
        ))

        if not full_bench_returns.empty:
            bench_cum = (1 + full_bench_returns).cumprod()
            fig.add_trace(go.Scatter(
                x=bench_cum.index, y=(bench_cum - 1) * 100,
                name=benchmark, line=dict(color="#FF9800", width=2)
            ))

        # æ¨™è¨˜ç†Šå¸‚æœˆä»½
        for m in bear_months:
            year, month = map(int, m.split("-"))
            m_start = f"{m}-01"
            if month == 12:
                m_end = f"{year + 1}-01-01"
            else:
                m_end = f"{year}-{month + 1:02d}-01"
            fig.add_vrect(x0=m_start, x1=m_end, fillcolor="red", opacity=0.1, line_width=0)

        fig.update_layout(
            xaxis_title="æ—¥æœŸ", yaxis_title="ç´¯ç©å ±é…¬ (%)",
            hovermode="x unified", height=400
        )
        st.plotly_chart(fig, use_container_width=True)

    # æŒè‚¡è®ŠåŒ–è¡¨
    st.markdown("### ğŸ“‹ æœˆåº¦æŒè‚¡è®ŠåŒ–")
    with st.expander("æŸ¥çœ‹è©³ç´°æŒè‚¡é…ç½®"):
        for m in selected_months:
            m_info = MONTHLY_PORTFOLIOS[m]
            st.markdown(f"**{m}** - {m_info['signal']}")
            holdings_df = pd.DataFrame([
                {"è‚¡ç¥¨": s, "æ¬Šé‡": f"{w*100:.0f}%"}
                for s, w in sorted(m_info["holdings"].items(), key=lambda x: x[1], reverse=True)
            ])
            st.dataframe(holdings_df, use_container_width=True, hide_index=True)
            st.markdown("---")


def run_backtest(start_q: str, end_q: str, benchmark: str, strategy: str = "ğŸ›¡ï¸ ç†Šå¸‚é˜²ç¦¦", cash_threshold: float = -0.2):
    """åŸ·è¡Œå›æ¸¬"""
    quarters = list(QUARTERLY_PORTFOLIOS.keys())
    start_idx = quarters.index(start_q)
    end_idx = quarters.index(end_q)
    selected_quarters = quarters[start_idx:end_idx+1]

    # æ”¶é›†æ‰€æœ‰éœ€è¦çš„è‚¡ç¥¨
    all_symbols = set([benchmark, "SHY"])  # åŠ å…¥SHYä½œç‚ºç¾é‡‘æ›¿ä»£
    for q in selected_quarters:
        all_symbols.update(QUARTERLY_PORTFOLIOS[q]["holdings"].keys())

    # å–å¾—åƒ¹æ ¼æ•¸æ“š
    start_date = QUARTERLY_PORTFOLIOS[start_q]["start"]
    end_date = QUARTERLY_PORTFOLIOS[end_q]["end"]

    st.info(f"ğŸ“… å›æ¸¬æœŸé–“: {start_date} ~ {end_date}")

    prices_df = fetch_stock_prices(list(all_symbols), start_date, end_date)

    if prices_df.empty:
        st.error("ç„¡æ³•å–å¾—è‚¡åƒ¹æ•¸æ“š")
        return

    # ç¾é‡‘é…ç½® (ç”¨SHYä»£æ›¿)
    cash_holdings = {"SHY": 1.0}

    def calculate_quarter_returns(q, holdings_override=None):
        """è¨ˆç®—å–®å­£å ±é…¬"""
        q_info = QUARTERLY_PORTFOLIOS[q]
        q_start = q_info["start"]
        q_end = q_info["end"]

        mask = (prices_df.index >= q_start) & (prices_df.index <= q_end)
        q_prices = prices_df[mask]

        if q_prices.empty:
            return None, None, None

        holdings = holdings_override if holdings_override else q_info["holdings"]
        portfolio_returns = calculate_portfolio_returns(q_prices, holdings)
        benchmark_returns = q_prices[benchmark].pct_change() if benchmark in q_prices.columns else pd.Series()

        return portfolio_returns, benchmark_returns, q_prices

    # è¨ˆç®—æ¯å­£å ±é…¬ - æ”¯æ´å¤šç­–ç•¥
    quarterly_results = []
    all_defensive_returns = pd.Series(dtype=float)  # ç†Šå¸‚é˜²ç¦¦ç­–ç•¥
    all_cash_returns = pd.Series(dtype=float)       # ç†Šå¸‚ç©ºæ‰‹ç­–ç•¥
    bear_quarters = []  # è¨˜éŒ„å“ªäº›å­£åº¦æ˜¯ç†Šå¸‚

    for q in selected_quarters:
        q_info = QUARTERLY_PORTFOLIOS[q]
        q_signal = QUARTER_SIGNALS.get(q, {})
        signal_score = q_signal.get("signal_score", 0)

        is_bear = signal_score <= cash_threshold
        if is_bear:
            bear_quarters.append(q)

        # è¨ˆç®—é˜²ç¦¦ç­–ç•¥å ±é…¬ (åŸå§‹é…ç½®)
        def_returns, bench_returns, q_prices = calculate_quarter_returns(q)
        if def_returns is None:
            st.warning(f"{q} ç„¡æ•¸æ“šï¼Œè·³é")
            continue

        # è¨ˆç®—ç©ºæ‰‹ç­–ç•¥å ±é…¬ (ç†Šå¸‚æ™‚100%ç¾é‡‘)
        if is_bear:
            cash_returns, _, _ = calculate_quarter_returns(q, cash_holdings)
        else:
            cash_returns = def_returns.copy()

        # è¨ˆç®—ç¸¾æ•ˆ
        def_metrics = calculate_metrics(def_returns)
        cash_metrics = calculate_metrics(cash_returns) if cash_returns is not None else {}
        bench_metrics = calculate_metrics(bench_returns) if bench_returns is not None else {}

        # æ ¹æ“šç­–ç•¥é¸æ“‡é¡¯ç¤ºå…§å®¹
        if strategy == "ğŸ“Š å…©è€…æ¯”è¼ƒ":
            quarterly_results.append({
                "å­£åº¦": q,
                "ä¿¡è™Ÿ": q_info["name"],
                "åˆ†æ•¸": f"{signal_score:+.1f}",
                "ç‹€æ…‹": "ğŸ”´ ç†Šå¸‚" if is_bear else "ğŸŸ¢ æ­£å¸¸",
                "é˜²ç¦¦ç­–ç•¥": f"{def_metrics['total_return']:.1f}%",
                "ç©ºæ‰‹ç­–ç•¥": f"{cash_metrics.get('total_return', 0):.1f}%",
                f"{benchmark}": f"{bench_metrics.get('total_return', 0):.1f}%",
                "é˜²ç¦¦Alpha": f"{def_metrics['total_return'] - bench_metrics.get('total_return', 0):+.1f}%",
                "ç©ºæ‰‹Alpha": f"{cash_metrics.get('total_return', 0) - bench_metrics.get('total_return', 0):+.1f}%",
            })
        else:
            # å–®ä¸€ç­–ç•¥
            if strategy == "ğŸ’µ ç†Šå¸‚ç©ºæ‰‹":
                p_metrics = cash_metrics
                p_returns = cash_returns
            else:
                p_metrics = def_metrics
                p_returns = def_returns

            quarterly_results.append({
                "å­£åº¦": q,
                "æ±ºç­–ä¿¡è™Ÿ": q_info["name"],
                "ä¿¡è™Ÿåˆ†æ•¸": f"{signal_score:+.1f}",
                "ç‹€æ…‹": "ğŸ”´ ç©ºæ‰‹" if (is_bear and strategy == "ğŸ’µ ç†Šå¸‚ç©ºæ‰‹") else ("ğŸ›¡ï¸ é˜²ç¦¦" if is_bear else "ğŸ“ˆ æŒè‚¡"),
                "æŠ•çµ„å ±é…¬": f"{p_metrics['total_return']:.1f}%",
                f"{benchmark}": f"{bench_metrics.get('total_return', 0):.1f}%",
                "Alpha": f"{p_metrics['total_return'] - bench_metrics.get('total_return', 0):+.1f}%",
                "æœ€å¤§å›æ’¤": f"{p_metrics['max_drawdown']:.1f}%",
            })

        # ç´¯ç©å ±é…¬
        all_defensive_returns = pd.concat([all_defensive_returns, def_returns])
        all_cash_returns = pd.concat([all_cash_returns, cash_returns])

    # é¡¯ç¤ºç†Šå¸‚å­£åº¦
    if bear_quarters:
        st.warning(f"ğŸ”´ **ç†Šå¸‚å­£åº¦** (ä¿¡è™Ÿ â‰¤ {cash_threshold}): {', '.join(bear_quarters)}")

    # é¡¯ç¤ºå­£åº¦çµæœ
    st.header("ğŸ“ˆ å­£åº¦ç¸¾æ•ˆæ¯”è¼ƒ")
    st.dataframe(pd.DataFrame(quarterly_results), use_container_width=True, hide_index=True)

    # è¨ˆç®—å…¨æœŸç¸¾æ•ˆ
    st.divider()
    st.header("ğŸ“Š å…¨æœŸç¸¾æ•ˆæ‘˜è¦")

    # å–å¾—å…¨æœŸåŸºæº–å ±é…¬
    full_benchmark_returns = prices_df[benchmark].pct_change().dropna() if benchmark in prices_df.columns else pd.Series()

    full_def_metrics = calculate_metrics(all_defensive_returns)
    full_cash_metrics = calculate_metrics(all_cash_returns)
    full_b_metrics = calculate_metrics(full_benchmark_returns)

    if strategy == "ğŸ“Š å…©è€…æ¯”è¼ƒ":
        # æ¯”è¼ƒæ¨¡å¼ - é¡¯ç¤ºå…©ç¨®ç­–ç•¥
        st.markdown("### ç­–ç•¥ç¸¾æ•ˆæ¯”è¼ƒ")

        comparison_data = {
            "æŒ‡æ¨™": ["ç¸½å ±é…¬", "å¹´åŒ–å ±é…¬", "å¤æ™®æ¯”ç‡", "æ³¢å‹•ç‡", "æœ€å¤§å›æ’¤", "å‹ç‡"],
            "ğŸ›¡ï¸ ç†Šå¸‚é˜²ç¦¦": [
                f"{full_def_metrics['total_return']:.1f}%",
                f"{full_def_metrics['annualized_return']:.1f}%",
                f"{full_def_metrics['sharpe']:.2f}",
                f"{full_def_metrics['volatility']:.1f}%",
                f"{full_def_metrics['max_drawdown']:.1f}%",
                f"{full_def_metrics['win_rate']:.1f}%",
            ],
            "ğŸ’µ ç†Šå¸‚ç©ºæ‰‹": [
                f"{full_cash_metrics['total_return']:.1f}%",
                f"{full_cash_metrics['annualized_return']:.1f}%",
                f"{full_cash_metrics['sharpe']:.2f}",
                f"{full_cash_metrics['volatility']:.1f}%",
                f"{full_cash_metrics['max_drawdown']:.1f}%",
                f"{full_cash_metrics['win_rate']:.1f}%",
            ],
            f"{benchmark}": [
                f"{full_b_metrics['total_return']:.1f}%",
                f"{full_b_metrics['annualized_return']:.1f}%",
                f"{full_b_metrics['sharpe']:.2f}",
                f"{full_b_metrics['volatility']:.1f}%",
                f"{full_b_metrics['max_drawdown']:.1f}%",
                f"{full_b_metrics['win_rate']:.1f}%",
            ],
        }
        st.dataframe(pd.DataFrame(comparison_data), use_container_width=True, hide_index=True)

        # å‹è€…åˆ¤å®š
        def_alpha = full_def_metrics['total_return'] - full_b_metrics['total_return']
        cash_alpha = full_cash_metrics['total_return'] - full_b_metrics['total_return']

        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("ğŸ›¡ï¸ é˜²ç¦¦ Alpha", f"{def_alpha:+.1f}%")
        with col2:
            st.metric("ğŸ’µ ç©ºæ‰‹ Alpha", f"{cash_alpha:+.1f}%")
        with col3:
            winner = "ğŸ’µ ç†Šå¸‚ç©ºæ‰‹" if cash_alpha > def_alpha else "ğŸ›¡ï¸ ç†Šå¸‚é˜²ç¦¦"
            diff = abs(cash_alpha - def_alpha)
            st.metric("å‹è€…", winner, delta=f"+{diff:.1f}%")

    else:
        # å–®ä¸€ç­–ç•¥æ¨¡å¼
        if strategy == "ğŸ’µ ç†Šå¸‚ç©ºæ‰‹":
            full_p_metrics = full_cash_metrics
        else:
            full_p_metrics = full_def_metrics

        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("æŠ•çµ„ç¸½å ±é…¬", f"{full_p_metrics['total_return']:.1f}%")
            st.metric(f"{benchmark}ç¸½å ±é…¬", f"{full_b_metrics['total_return']:.1f}%")
        with col2:
            alpha = full_p_metrics['total_return'] - full_b_metrics['total_return']
            st.metric("è¶…é¡å ±é…¬ (Alpha)", f"{alpha:.1f}%",
                      delta=f"{'å‹' if alpha > 0 else 'è² '}")
            st.metric("å¹´åŒ–å ±é…¬", f"{full_p_metrics['annualized_return']:.1f}%")
        with col3:
            st.metric("å¤æ™®æ¯”ç‡", f"{full_p_metrics['sharpe']:.2f}")
            st.metric("æ³¢å‹•ç‡", f"{full_p_metrics['volatility']:.1f}%")
        with col4:
            st.metric("æœ€å¤§å›æ’¤", f"{full_p_metrics['max_drawdown']:.1f}%")
            st.metric("å‹ç‡", f"{full_p_metrics['win_rate']:.1f}%")

    # ç¹ªè£½ç´¯ç©å ±é…¬åœ–
    st.divider()
    st.header("ğŸ“‰ ç´¯ç©å ±é…¬èµ°å‹¢")

    fig = go.Figure()

    # é˜²ç¦¦ç­–ç•¥ç´¯ç©
    if strategy in ["ğŸ›¡ï¸ ç†Šå¸‚é˜²ç¦¦", "ğŸ“Š å…©è€…æ¯”è¼ƒ"]:
        defensive_cum = (1 + all_defensive_returns).cumprod()
        fig.add_trace(go.Scatter(
            x=defensive_cum.index,
            y=(defensive_cum - 1) * 100,
            name="ğŸ›¡ï¸ ç†Šå¸‚é˜²ç¦¦",
            line=dict(color="#2196F3", width=2),
        ))

    # ç©ºæ‰‹ç­–ç•¥ç´¯ç©
    if strategy in ["ğŸ’µ ç†Šå¸‚ç©ºæ‰‹", "ğŸ“Š å…©è€…æ¯”è¼ƒ"]:
        cash_cum = (1 + all_cash_returns).cumprod()
        fig.add_trace(go.Scatter(
            x=cash_cum.index,
            y=(cash_cum - 1) * 100,
            name="ğŸ’µ ç†Šå¸‚ç©ºæ‰‹",
            line=dict(color="#4CAF50", width=2),
        ))

    # åŸºæº–ç´¯ç©
    if not full_benchmark_returns.empty:
        benchmark_cum = (1 + full_benchmark_returns).cumprod()
        fig.add_trace(go.Scatter(
            x=benchmark_cum.index,
            y=(benchmark_cum - 1) * 100,
            name=f"{benchmark}",
            line=dict(color="#FF9800", width=2),
        ))

    # æ¨™è¨˜ç†Šå¸‚å­£åº¦
    for q in bear_quarters:
        q_start = QUARTERLY_PORTFOLIOS[q]["start"]
        q_end = QUARTERLY_PORTFOLIOS[q]["end"]
        fig.add_vrect(x0=q_start, x1=q_end, fillcolor="red", opacity=0.1, line_width=0)

    # æ¨™è¨˜å­£åº¦åˆ†ç•Œ
    for q in selected_quarters:
        q_start = QUARTERLY_PORTFOLIOS[q]["start"]
        fig.add_vline(x=q_start, line_dash="dash", line_color="gray", opacity=0.5)
        fig.add_annotation(x=q_start, y=1.05, yref="paper", text=q, showarrow=False, font=dict(size=10))

    fig.update_layout(
        title="ç´¯ç©å ±é…¬ç‡ (%)",
        xaxis_title="æ—¥æœŸ",
        yaxis_title="ç´¯ç©å ±é…¬ (%)",
        hovermode="x unified",
        height=500,
    )
    st.plotly_chart(fig, use_container_width=True)

    # é¡¯ç¤ºæŒè‚¡é…ç½®èˆ‡æ±ºç­–ä¿¡è™Ÿ
    st.divider()
    st.header("ğŸ“‹ å„å­£æ±ºç­–ä¿¡è™Ÿèˆ‡æŒè‚¡é…ç½®")

    for q in selected_quarters:
        q_info = QUARTERLY_PORTFOLIOS[q]
        q_signal = QUARTER_SIGNALS.get(q, {})
        signal_score = q_signal.get("signal_score", 0)

        # æ ¹æ“šä¿¡è™Ÿåˆ†æ•¸æ±ºå®šé¡è‰²
        if signal_score >= 0.5:
            signal_color = "ğŸŸ¢"
        elif signal_score >= 0.2:
            signal_color = "ğŸŸ¡"
        elif signal_score >= -0.2:
            signal_color = "âšª"
        elif signal_score >= -0.5:
            signal_color = "ğŸŸ "
        else:
            signal_color = "ğŸ”´"

        with st.expander(f"{signal_color} **{q}** - {q_info['name']} (ä¿¡è™Ÿ: {signal_score:+.1f})"):
            # é¡¯ç¤ºå­£åˆå¯å¾—ä¿¡è™Ÿ
            st.markdown("##### ğŸ“¡ å­£åˆæ±ºç­–ä¿¡è™Ÿ (å­£åˆç¬¬ä¸€å¤©å¯å¾—è³‡è¨Š)")
            signal_text = q_info.get("signal", "")
            st.info(f"**{signal_text}**")

            if q_signal:
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.markdown(f"**Fedæ…‹åº¦**: {q_signal.get('fed_stance', 'â€”')}")
                    st.markdown(f"**æ®–åˆ©ç‡æ›²ç·š**: {q_signal.get('yield_curve', 'â€”')}")
                with col2:
                    st.markdown(f"**CPIè¶¨å‹¢**: {q_signal.get('cpi_trend', 'â€”')}")
                    st.markdown(f"**SPY vs 200MA**: {q_signal.get('spy_vs_200ma', 'â€”')}")
                with col3:
                    st.markdown(f"**VIX**: {q_signal.get('vix', 'â€”')}")
                    if q_signal.get('ai_momentum'):
                        st.markdown(f"**AIå‹•èƒ½**: {q_signal.get('ai_momentum')}")

            st.markdown("##### ğŸ’¼ æŒè‚¡é…ç½®")
            st.markdown(f"**æœŸé–“**: {q_info['start']} ~ {q_info['end']}")

            holdings_df = pd.DataFrame([
                {"è‚¡ç¥¨": s, "æ¬Šé‡": f"{w*100:.0f}%", "å…¬å¸": STOCK_DETAILS.get(s, {}).get("name", s)}
                for s, w in sorted(q_info["holdings"].items(), key=lambda x: x[1], reverse=True)
            ])
            st.dataframe(holdings_df, use_container_width=True, hide_index=True)

    # é¢¨éšªæç¤º
    st.divider()
    st.warning("""
    âš ï¸ **å…è²¬è²æ˜**

    æ­¤å›æ¸¬çµæœåƒ…ä¾›åƒè€ƒï¼Œä¸æ§‹æˆæŠ•è³‡å»ºè­°ã€‚
    - éå»ç¸¾æ•ˆä¸ä»£è¡¨æœªä¾†è¡¨ç¾
    - å›æ¸¬æœªè€ƒæ…®äº¤æ˜“æˆæœ¬ã€æ»‘é»ã€ç¨…è²»
    - å¯¦éš›åŸ·è¡Œå¯èƒ½æœ‰æµå‹•æ€§å•é¡Œ
    - è«‹è¬¹æ…è©•ä¼°è‡ªèº«é¢¨éšªæ‰¿å—åº¦
    """)


# ========== å€‹è‚¡æ·±åº¦åˆ†æé é¢ ==========
FOCUS_STOCKS = {
    "AAPL": {"name": "Apple", "sector": "ç§‘æŠ€", "description": "æ¶ˆè²»é›»å­èˆ‡æœå‹™å·¨é ­ï¼ŒiPhoneã€Macã€æœå‹™ç”Ÿæ…‹ç³»"},
    "INTC": {"name": "Intel", "sector": "åŠå°é«”", "description": "CPU è£½é€ å•†ï¼Œæ­£åœ¨è½‰å‹æ™¶åœ“ä»£å·¥"},
    "PLTR": {"name": "Palantir", "sector": "è»Ÿé«”/AI", "description": "å¤§æ•¸æ“šåˆ†æå¹³å°ï¼Œæ”¿åºœèˆ‡ä¼æ¥­ AI è§£æ±ºæ–¹æ¡ˆ"},
    "LITE": {"name": "Lumentum", "sector": "å…‰å­¸/å…‰å­", "description": "å…‰å­¸èˆ‡å…‰å­ç”¢å“ï¼Œ3D æ„Ÿæ¸¬ã€å…‰é€šè¨Š"},
}


def render_individual_stock_page(selected_date: date):
    """æ¸²æŸ“å€‹è‚¡æ·±åº¦åˆ†æé é¢"""
    st.title("ğŸ”¬ å€‹è‚¡æ·±åº¦åˆ†æ")

    # è‚¡ç¥¨é¸æ“‡
    col1, col2 = st.columns([1, 3])
    with col1:
        selected_symbol = st.selectbox(
            "é¸æ“‡è‚¡ç¥¨",
            list(FOCUS_STOCKS.keys()),
            format_func=lambda x: f"{x} - {FOCUS_STOCKS[x]['name']}"
        )

    stock_info = FOCUS_STOCKS[selected_symbol]

    with col2:
        st.markdown(f"""
        ### {selected_symbol} - {stock_info['name']}
        **ç”¢æ¥­**: {stock_info['sector']} | {stock_info['description']}
        """)

    st.divider()

    # ========== å–å¾—è‚¡åƒ¹æ•¸æ“š ==========
    end_date = date.today()
    start_date = end_date - timedelta(days=365)

    # ä½¿ç”¨ yfinance å–å¾—æœ€æ–°æ•¸æ“š
    try:
        import yfinance as yf
        ticker = yf.Ticker(selected_symbol)
        df = ticker.history(start=start_date, end=end_date)
        df = df.reset_index()
        df.columns = [c.lower() for c in df.columns]

        if df.empty:
            st.warning(f"ç„¡æ³•å–å¾— {selected_symbol} çš„è‚¡åƒ¹æ•¸æ“š")
            return

        # è¨ˆç®—æŠ€è¡“æŒ‡æ¨™
        df['ma20'] = df['close'].rolling(window=20).mean()
        df['ma50'] = df['close'].rolling(window=50).mean()
        df['ma200'] = df['close'].rolling(window=200).mean()

        # RSI
        delta = df['close'].diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
        rs = gain / loss
        df['rsi'] = 100 - (100 / (1 + rs))

        # MACD
        exp1 = df['close'].ewm(span=12, adjust=False).mean()
        exp2 = df['close'].ewm(span=26, adjust=False).mean()
        df['macd'] = exp1 - exp2
        df['signal'] = df['macd'].ewm(span=9, adjust=False).mean()
        df['macd_hist'] = df['macd'] - df['signal']

        # å¸ƒæ—é€šé“
        df['bb_mid'] = df['close'].rolling(window=20).mean()
        df['bb_std'] = df['close'].rolling(window=20).std()
        df['bb_upper'] = df['bb_mid'] + 2 * df['bb_std']
        df['bb_lower'] = df['bb_mid'] - 2 * df['bb_std']

    except Exception as e:
        st.error(f"å–å¾—è‚¡åƒ¹æ•¸æ“šæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return

    # ========== é—œéµæŒ‡æ¨™å¡ç‰‡ ==========
    latest = df.iloc[-1]
    prev = df.iloc[-2] if len(df) > 1 else latest

    price_change = latest['close'] - prev['close']
    price_change_pct = (price_change / prev['close']) * 100

    col1, col2, col3, col4, col5 = st.columns(5)
    with col1:
        st.metric(
            "æ”¶ç›¤åƒ¹",
            f"${latest['close']:.2f}",
            f"{price_change:+.2f} ({price_change_pct:+.2f}%)"
        )
    with col2:
        st.metric("æˆäº¤é‡", f"{latest['volume']/1e6:.1f}M")
    with col3:
        rsi_val = latest['rsi']
        rsi_status = "è¶…è²·" if rsi_val > 70 else ("è¶…è³£" if rsi_val < 30 else "ä¸­æ€§")
        st.metric("RSI (14)", f"{rsi_val:.1f}", rsi_status)
    with col4:
        macd_val = latest['macd']
        macd_signal = "å¤šé ­" if macd_val > latest['signal'] else "ç©ºé ­"
        st.metric("MACD", f"{macd_val:.2f}", macd_signal)
    with col5:
        # è¨ˆç®—è·é›¢ 52 é€±é«˜ä½
        high_52w = df['high'].tail(252).max()
        low_52w = df['low'].tail(252).min()
        pct_from_high = ((latest['close'] - high_52w) / high_52w) * 100
        st.metric("è·52é€±é«˜", f"{pct_from_high:.1f}%", f"${high_52w:.2f}")

    st.divider()

    # ========== åœ–è¡¨å€åŸŸ ==========
    tab1, tab2, tab3, tab4 = st.tabs(["ğŸ“ˆ åƒ¹æ ¼èµ°å‹¢", "ğŸ“Š æŠ€è¡“æŒ‡æ¨™", "ğŸ“° ç›¸é—œæ–°è", "ğŸ¯ åˆ†æç¸½çµ"])

    with tab1:
        # Kç·šåœ– + å‡ç·š
        fig = make_subplots(
            rows=2, cols=1,
            shared_xaxes=True,
            vertical_spacing=0.05,
            row_heights=[0.7, 0.3],
            subplot_titles=("åƒ¹æ ¼èˆ‡å‡ç·š", "æˆäº¤é‡")
        )

        # Kç·š
        fig.add_trace(go.Candlestick(
            x=df['date'],
            open=df['open'],
            high=df['high'],
            low=df['low'],
            close=df['close'],
            name='Kç·š'
        ), row=1, col=1)

        # å‡ç·š
        fig.add_trace(go.Scatter(x=df['date'], y=df['ma20'], name='MA20', line=dict(color='orange', width=1)), row=1, col=1)
        fig.add_trace(go.Scatter(x=df['date'], y=df['ma50'], name='MA50', line=dict(color='blue', width=1)), row=1, col=1)
        fig.add_trace(go.Scatter(x=df['date'], y=df['ma200'], name='MA200', line=dict(color='red', width=1)), row=1, col=1)

        # å¸ƒæ—é€šé“
        fig.add_trace(go.Scatter(x=df['date'], y=df['bb_upper'], name='BB Upper', line=dict(color='gray', dash='dash', width=0.5)), row=1, col=1)
        fig.add_trace(go.Scatter(x=df['date'], y=df['bb_lower'], name='BB Lower', line=dict(color='gray', dash='dash', width=0.5), fill='tonexty', fillcolor='rgba(128,128,128,0.1)'), row=1, col=1)

        # æˆäº¤é‡
        colors = ['red' if row['close'] < row['open'] else 'green' for _, row in df.iterrows()]
        fig.add_trace(go.Bar(x=df['date'], y=df['volume'], name='æˆäº¤é‡', marker_color=colors), row=2, col=1)

        fig.update_layout(
            height=600,
            xaxis_rangeslider_visible=False,
            showlegend=True,
            legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1)
        )
        st.plotly_chart(fig, use_container_width=True)

    with tab2:
        # RSI + MACD åœ–
        fig2 = make_subplots(
            rows=2, cols=1,
            shared_xaxes=True,
            vertical_spacing=0.1,
            subplot_titles=("RSI (14)", "MACD")
        )

        # RSI
        fig2.add_trace(go.Scatter(x=df['date'], y=df['rsi'], name='RSI', line=dict(color='purple')), row=1, col=1)
        fig2.add_hline(y=70, line_dash="dash", line_color="red", row=1, col=1)
        fig2.add_hline(y=30, line_dash="dash", line_color="green", row=1, col=1)

        # MACD
        fig2.add_trace(go.Scatter(x=df['date'], y=df['macd'], name='MACD', line=dict(color='blue')), row=2, col=1)
        fig2.add_trace(go.Scatter(x=df['date'], y=df['signal'], name='Signal', line=dict(color='orange')), row=2, col=1)
        colors_macd = ['green' if v > 0 else 'red' for v in df['macd_hist']]
        fig2.add_trace(go.Bar(x=df['date'], y=df['macd_hist'], name='Histogram', marker_color=colors_macd), row=2, col=1)

        fig2.update_layout(height=500, showlegend=True)
        st.plotly_chart(fig2, use_container_width=True)

    with tab3:
        # ç›¸é—œæ–°è
        st.markdown("#### ğŸ“° è¿‘æœŸç›¸é—œæ–°è")

        # æœå°‹é—œéµå­—
        search_keywords = {
            "AAPL": ["apple", "iphone", "aapl", "tim cook"],
            "INTC": ["intel", "intc", "pat gelsinger", "foundry"],
            "PLTR": ["palantir", "pltr", "alex karp"],
            "LITE": ["lumentum", "lite", "optical", "photonics"],
        }

        keywords = search_keywords.get(selected_symbol, [selected_symbol.lower()])

        # å–å¾—ç›¸é—œæ–°è - ä½¿ç”¨çµ±ä¸€è³‡æ–™å±¤
        related_news = []
        try:
            client = _get_data_client()
            for kw in keywords[:2]:  # é™åˆ¶é—œéµå­—æ•¸é‡
                results = client.search_news(kw, limit=10)
                related_news.extend(results)
        except Exception as e:
            pass

        # å»é‡
        seen_titles = set()
        unique_news = []
        for n in related_news:
            if n["title"] not in seen_titles:
                seen_titles.add(n["title"])
                unique_news.append(n)

        if unique_news:
            for news in unique_news[:15]:
                pub_date = news.get("published_at", "")[:10] if news.get("published_at") else ""
                source = news.get("source", "")
                st.markdown(f"**{pub_date}** | {source}")
                st.markdown(f"[{news['title']}]({news.get('url', '#')})")
                st.markdown("---")
        else:
            st.info("æš«ç„¡ç›¸é—œæ–°è")

    with tab4:
        # æŠ€è¡“åˆ†æç¸½çµ
        st.markdown("#### ğŸ¯ æŠ€è¡“åˆ†æç¸½çµ")

        # è¶¨å‹¢åˆ¤æ–·
        trend_signals = []
        if latest['close'] > latest['ma20']:
            trend_signals.append("âœ… è‚¡åƒ¹åœ¨ MA20 ä¹‹ä¸Š (çŸ­æœŸå¤šé ­)")
        else:
            trend_signals.append("âŒ è‚¡åƒ¹åœ¨ MA20 ä¹‹ä¸‹ (çŸ­æœŸç©ºé ­)")

        if latest['close'] > latest['ma50']:
            trend_signals.append("âœ… è‚¡åƒ¹åœ¨ MA50 ä¹‹ä¸Š (ä¸­æœŸå¤šé ­)")
        else:
            trend_signals.append("âŒ è‚¡åƒ¹åœ¨ MA50 ä¹‹ä¸‹ (ä¸­æœŸç©ºé ­)")

        if latest['close'] > latest['ma200']:
            trend_signals.append("âœ… è‚¡åƒ¹åœ¨ MA200 ä¹‹ä¸Š (é•·æœŸå¤šé ­)")
        else:
            trend_signals.append("âŒ è‚¡åƒ¹åœ¨ MA200 ä¹‹ä¸‹ (é•·æœŸç©ºé ­)")

        if latest['ma20'] > latest['ma50']:
            trend_signals.append("âœ… MA20 > MA50 (é»ƒé‡‘äº¤å‰å½¢æ…‹)")
        else:
            trend_signals.append("âš ï¸ MA20 < MA50 (æ­»äº¡äº¤å‰å½¢æ…‹)")

        # RSI åˆ¤æ–·
        if rsi_val > 70:
            trend_signals.append("âš ï¸ RSI > 70 (è¶…è²·å€ï¼Œå¯èƒ½å›èª¿)")
        elif rsi_val < 30:
            trend_signals.append("ğŸŸ¢ RSI < 30 (è¶…è³£å€ï¼Œå¯èƒ½åå½ˆ)")
        else:
            trend_signals.append("âšª RSI åœ¨ä¸­æ€§å€é–“")

        # MACD åˆ¤æ–·
        if latest['macd'] > latest['signal']:
            trend_signals.append("âœ… MACD åœ¨ä¿¡è™Ÿç·šä¹‹ä¸Š (å¤šé ­å‹•èƒ½)")
        else:
            trend_signals.append("âŒ MACD åœ¨ä¿¡è™Ÿç·šä¹‹ä¸‹ (ç©ºé ­å‹•èƒ½)")

        # å¸ƒæ—é€šé“åˆ¤æ–·
        if latest['close'] > latest['bb_upper']:
            trend_signals.append("âš ï¸ è‚¡åƒ¹çªç ´å¸ƒæ—ä¸Šè»Œ (å¯èƒ½è¶…æ¼²)")
        elif latest['close'] < latest['bb_lower']:
            trend_signals.append("ğŸŸ¢ è‚¡åƒ¹è·Œç ´å¸ƒæ—ä¸‹è»Œ (å¯èƒ½è¶…è·Œ)")

        col1, col2 = st.columns(2)
        with col1:
            st.markdown("**è¶¨å‹¢ä¿¡è™Ÿ**")
            for signal in trend_signals:
                st.markdown(signal)

        with col2:
            # ç¶œåˆè©•åˆ†
            bullish_count = sum(1 for s in trend_signals if s.startswith("âœ…") or s.startswith("ğŸŸ¢"))
            bearish_count = sum(1 for s in trend_signals if s.startswith("âŒ") or s.startswith("âš ï¸"))
            total = bullish_count + bearish_count

            if total > 0:
                score = (bullish_count / total) * 100

                st.markdown("**ç¶œåˆè©•åˆ†**")
                if score >= 70:
                    st.success(f"ğŸŸ¢ åå¤š ({score:.0f}åˆ†)")
                    st.markdown("æŠ€è¡“é¢åå¤šé ­ï¼Œä½†ä»éœ€æ³¨æ„é¢¨éšªç®¡ç†")
                elif score >= 40:
                    st.warning(f"ğŸŸ¡ ä¸­æ€§ ({score:.0f}åˆ†)")
                    st.markdown("å¤šç©ºäº¤æˆ°ï¼Œå»ºè­°è§€æœ›æˆ–è¼•å€‰")
                else:
                    st.error(f"ğŸ”´ åç©º ({score:.0f}åˆ†)")
                    st.markdown("æŠ€è¡“é¢åç©ºé ­ï¼Œå»ºè­°è¬¹æ…æ“ä½œ")

            # æ”¯æ’å£“åŠ›
            st.markdown("**é—œéµåƒ¹ä½**")
            st.markdown(f"- æ”¯æ’: ${latest['bb_lower']:.2f} (å¸ƒæ—ä¸‹è»Œ)")
            st.markdown(f"- å£“åŠ›: ${latest['bb_upper']:.2f} (å¸ƒæ—ä¸Šè»Œ)")
            st.markdown(f"- 52é€±é«˜: ${high_52w:.2f}")
            st.markdown(f"- 52é€±ä½: ${low_52w:.2f}")


def render_stock_page(selected_date: date):
    """æ¸²æŸ“è‚¡ç¥¨æ•¸æ“šé é¢"""
    st.title("ğŸ“ˆ è‚¡ç¥¨æ•¸æ“šèˆ‡æ–°è")

    # æª¢æŸ¥é‡‘èè³‡æ–™åº«æ˜¯å¦å­˜åœ¨
    if not FINANCE_DB_PATH.exists():
        st.error("æ‰¾ä¸åˆ°é‡‘èè³‡æ–™åº« (finance.db)")
        st.info("è«‹åŸ·è¡Œ `python finance_collector.py --init --fast` ä¾†åˆå§‹åŒ–")
        return

    # å–å¾—è¿½è¹¤æ¸…å–®
    watchlist = get_watchlist()
    if not watchlist:
        st.warning("è¿½è¹¤æ¸…å–®ç‚ºç©ºï¼Œè«‹å…ˆåˆå§‹åŒ–")
        return

    # ä¾å¸‚å ´åˆ†çµ„
    markets = {}
    for stock in watchlist:
        market = stock["market"]
        if market not in markets:
            markets[market] = []
        markets[market].append(stock)

    # ========== å´é‚Šé¸æ“‡ ==========
    col1, col2 = st.columns([1, 3])

    with col1:
        # å¸‚å ´é¸æ“‡
        market_options = list(markets.keys())
        selected_market = st.selectbox("é¸æ“‡å¸‚å ´", market_options, index=0)

        # ä¾ç”¢æ¥­åˆ†çµ„
        stocks_in_market = markets[selected_market]

        # ç”¢æ¥­ç¯©é¸
        sectors = list(set(s.get("sector") or "æœªåˆ†é¡" for s in stocks_in_market))
        sectors.sort()
        sectors.insert(0, "å…¨éƒ¨")
        selected_sector = st.selectbox("ç”¢æ¥­ç¯©é¸", sectors, index=0)

        # ç¯©é¸è‚¡ç¥¨
        if selected_sector != "å…¨éƒ¨":
            filtered_stocks = [s for s in stocks_in_market if (s.get("sector") or "æœªåˆ†é¡") == selected_sector]
        else:
            filtered_stocks = stocks_in_market

        # è‚¡ç¥¨é¸æ“‡
        stock_options = {f"{s['symbol']} - {s['name'][:15] if s['name'] else s['symbol']}": s["symbol"] for s in filtered_stocks}
        selected_stock_label = st.selectbox("é¸æ“‡è‚¡ç¥¨", list(stock_options.keys()))
        selected_symbol = stock_options[selected_stock_label]

        # é¡¯ç¤ºè‚¡ç¥¨è³‡è¨Šå¡
        stock_info = get_stock_info(selected_symbol)
        if stock_info:
            st.markdown("---")
            st.markdown(f"**{stock_info['name']}**")
            if stock_info.get('sector'):
                st.markdown(f"ğŸ·ï¸ {stock_info['sector']}")
            if stock_info.get('industry'):
                st.markdown(f"ğŸ­ {stock_info['industry']}")
            if stock_info.get('description'):
                st.caption(stock_info['description'])

        st.markdown("---")

        # æ™‚é–“ç¯„åœ
        period_options = {
            "1å€‹æœˆ": 30,
            "3å€‹æœˆ": 90,
            "6å€‹æœˆ": 180,
            "1å¹´": 365,
        }
        selected_period = st.selectbox("æ™‚é–“ç¯„åœ", list(period_options.keys()), index=1)
        days = period_options[selected_period]

        end_date = selected_date
        start_date = end_date - timedelta(days=days)

    # ========== ä¸»è¦å…§å®¹ ==========
    with col2:
        # å–å¾—åƒ¹æ ¼æ•¸æ“š
        df = get_stock_prices(selected_symbol, start_date, end_date)

        if df.empty:
            st.warning(f"æ²’æœ‰ {selected_symbol} çš„åƒ¹æ ¼æ•¸æ“š")
            return

        # å–å¾—ç›¸é—œæ–°èæ•¸é‡
        news_counts = get_news_in_date_range(start_date, end_date)

        # å»ºç«‹åœ–è¡¨
        fig = make_subplots(
            rows=3, cols=1,
            shared_xaxes=True,
            vertical_spacing=0.05,
            row_heights=[0.5, 0.25, 0.25],
            subplot_titles=(f"{selected_symbol} åƒ¹æ ¼èµ°å‹¢", "æˆäº¤é‡", "ç›¸é—œæ–°èæ•¸é‡")
        )

        # åƒ¹æ ¼ K ç·šåœ–
        fig.add_trace(
            go.Candlestick(
                x=df["date"],
                open=df["open"],
                high=df["high"],
                low=df["low"],
                close=df["close"],
                name="åƒ¹æ ¼",
                increasing_line_color="#26a69a",
                decreasing_line_color="#ef5350"
            ),
            row=1, col=1
        )

        # åŠ å…¥å‡ç·š
        if len(df) >= 5:
            df["MA5"] = df["close"].rolling(window=5).mean()
            fig.add_trace(
                go.Scatter(x=df["date"], y=df["MA5"], name="MA5",
                           line=dict(color="orange", width=1)),
                row=1, col=1
            )

        if len(df) >= 20:
            df["MA20"] = df["close"].rolling(window=20).mean()
            fig.add_trace(
                go.Scatter(x=df["date"], y=df["MA20"], name="MA20",
                           line=dict(color="blue", width=1)),
                row=1, col=1
            )

        # æˆäº¤é‡
        colors = ["#26a69a" if row["close"] >= row["open"] else "#ef5350"
                  for _, row in df.iterrows()]
        fig.add_trace(
            go.Bar(x=df["date"], y=df["volume"], name="æˆäº¤é‡",
                   marker_color=colors, showlegend=False),
            row=2, col=1
        )

        # æ–°èæ•¸é‡
        news_dates = []
        news_values = []
        for d, count in sorted(news_counts.items()):
            news_dates.append(d)
            news_values.append(count)

        fig.add_trace(
            go.Bar(x=news_dates, y=news_values, name="æ–°èæ•¸",
                   marker_color="#2196f3", showlegend=False),
            row=3, col=1
        )

        # æ¨™è¨˜é¸æ“‡çš„æ—¥æœŸ
        fig.add_shape(
            type="line",
            x0=selected_date.strftime("%Y-%m-%d"),
            x1=selected_date.strftime("%Y-%m-%d"),
            y0=0,
            y1=1,
            yref="paper",
            line=dict(color="red", width=1, dash="dash"),
        )
        fig.add_annotation(
            x=selected_date.strftime("%Y-%m-%d"),
            y=1.02,
            yref="paper",
            text=f"é¸æ“‡æ—¥æœŸ",
            showarrow=False,
            font=dict(color="red", size=10)
        )

        # æ›´æ–°ç‰ˆé¢
        fig.update_layout(
            height=700,
            xaxis_rangeslider_visible=False,
            showlegend=True,
            legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1),
            margin=dict(l=50, r=50, t=80, b=50)
        )

        fig.update_xaxes(
            rangebreaks=[dict(bounds=["sat", "mon"])],  # éš±è—é€±æœ«
        )

        st.plotly_chart(fig, use_container_width=True)

    # ========== è‚¡ç¥¨è³‡è¨Šèˆ‡æ–°è ==========
    st.divider()

    col_info, col_news = st.columns([1, 2])

    with col_info:
        st.subheader("ğŸ“Š åŸºæœ¬é¢æ•¸æ“š")

        # å–å¾—æœ€æ–°åƒ¹æ ¼
        latest = df.iloc[-1] if not df.empty else None

        if latest is not None:
            prev_close = df.iloc[-2]["close"] if len(df) > 1 else latest["close"]
            change = latest["close"] - prev_close
            change_pct = (change / prev_close) * 100

            if change >= 0:
                st.metric("æœ€æ–°æ”¶ç›¤åƒ¹", f"${latest['close']:.2f}",
                          f"+{change:.2f} (+{change_pct:.2f}%)")
            else:
                st.metric("æœ€æ–°æ”¶ç›¤åƒ¹", f"${latest['close']:.2f}",
                          f"{change:.2f} ({change_pct:.2f}%)")

            col_a, col_b = st.columns(2)
            with col_a:
                st.metric("æœ€é«˜", f"${latest['high']:.2f}")
            with col_b:
                st.metric("æœ€ä½", f"${latest['low']:.2f}")

            st.metric("æˆäº¤é‡", f"{latest['volume']:,.0f}")

        # åŸºæœ¬é¢æ•¸æ“š
        fundamentals = get_stock_fundamentals(selected_symbol)
        if fundamentals:
            st.markdown("---")
            st.markdown("**ä¼°å€¼æŒ‡æ¨™**")

            fund_metrics = [
                ("æœ¬ç›Šæ¯” (P/E)", fundamentals.get("pe_ratio")),
                ("è‚¡åƒ¹æ·¨å€¼æ¯” (P/B)", fundamentals.get("pb_ratio")),
                ("æ®–åˆ©ç‡", fundamentals.get("dividend_yield")),
                ("Beta", fundamentals.get("beta")),
            ]

            for label, value in fund_metrics:
                if value is not None:
                    if "æ®–åˆ©ç‡" in label:
                        st.markdown(f"â€¢ {label}: {value*100:.2f}%")
                    else:
                        st.markdown(f"â€¢ {label}: {value:.2f}")

            if fundamentals.get("market_cap"):
                market_cap = fundamentals["market_cap"]
                if market_cap >= 1e12:
                    st.markdown(f"â€¢ å¸‚å€¼: ${market_cap/1e12:.2f}T")
                elif market_cap >= 1e9:
                    st.markdown(f"â€¢ å¸‚å€¼: ${market_cap/1e9:.2f}B")
                else:
                    st.markdown(f"â€¢ å¸‚å€¼: ${market_cap/1e6:.2f}M")

        # äº¤æ˜“å»ºè­°
        st.markdown("---")
        st.subheader("ğŸ¯ äº¤æ˜“å»ºè­°")

        try:
            analyzer = TechnicalAnalyzer(str(FINANCE_DB_PATH))
            analysis = analyzer.get_current_analysis(selected_symbol)

            rec = analysis['recommendation']
            rec_text = analysis.get('recommendation_text', rec)
            confidence = analysis['confidence']

            if rec in ['STRONG_BUY', 'BUY']:
                st.success(f"**{rec_text}** (ä¿¡å¿ƒåº¦: {confidence:.0f}%)")
            elif rec in ['STRONG_SELL', 'SELL']:
                st.error(f"**{rec_text}** (ä¿¡å¿ƒåº¦: {confidence:.0f}%)")
            else:
                st.warning(f"**{rec_text}** (ä¿¡å¿ƒåº¦: {confidence:.0f}%)")

            # ç°¡è¦ç†ç”±
            reasons = analysis.get('reasons', [])
            if reasons:
                with st.expander("ğŸ“‹ åˆ†æç†ç”±"):
                    for reason in reasons[:3]:
                        st.markdown(f"â€¢ {reason}")
        except Exception as e:
            st.info("ç„¡æ³•å–å¾—äº¤æ˜“å»ºè­°")

    with col_news:
        st.subheader(f"ğŸ“° {selected_date} ç›¸é—œæ–°è")

        # å–å¾—ç›¸é—œæ–°è
        related_news = get_news_for_stock(selected_symbol, selected_date)

        if related_news:
            st.markdown(f"æ‰¾åˆ° **{len(related_news)}** å‰‡ç›¸é—œæ–°è")
            st.divider()

            for news in related_news[:10]:
                # æƒ…ç·’åˆ†æ
                text = (news["title"] + " " + (news["content"] or "")).lower()
                sentiment = "ğŸŸ¡"
                for kw in POSITIVE_KEYWORDS:
                    if kw in text:
                        sentiment = "ğŸŸ¢"
                        break
                for kw in NEGATIVE_KEYWORDS:
                    if kw in text:
                        sentiment = "ğŸ”´" if sentiment == "ğŸŸ¡" else "ğŸŸ¡"
                        break

                with st.expander(f"{sentiment} {news['title'][:70]}...", expanded=False):
                    st.markdown(f"**ä¾†æº**: {news['source']}")
                    if news["published_at"]:
                        st.markdown(f"**æ™‚é–“**: {news['published_at']}")
                    if news["content"]:
                        st.write(news["content"][:300] + "..." if len(news["content"]) > 300 else news["content"])
                    if news["url"]:
                        st.link_button("ğŸ”— é–±è®€åŸæ–‡", news["url"])
        else:
            st.info(f"æ²’æœ‰æ‰¾åˆ°èˆ‡ {selected_symbol} ç›¸é—œçš„æ–°è")
            st.markdown("**å¯èƒ½åŸå› :**")
            st.markdown("- è©²æ—¥æœŸæ²’æœ‰æ”¶é›†æ–°è")
            st.markdown("- æ–°èæ¨™é¡Œ/å…§å®¹ä¸­æ²’æœ‰æåŠè©²è‚¡ç¥¨")

    # ========== å¤šè‚¡ç¥¨æ¯”è¼ƒ ==========
    st.divider()
    st.subheader("ğŸ“Š å¤šè‚¡ç¥¨æ¯”è¼ƒ")

    # é¸æ“‡æ¯”è¼ƒçš„è‚¡ç¥¨
    all_symbols = [s["symbol"] for s in watchlist]
    compare_symbols = st.multiselect(
        "é¸æ“‡è¦æ¯”è¼ƒçš„è‚¡ç¥¨ (æœ€å¤š5æª”)",
        all_symbols,
        default=[selected_symbol],
        max_selections=5
    )

    if len(compare_symbols) >= 2:
        # å–å¾—æ‰€æœ‰è‚¡ç¥¨çš„æ•¸æ“š
        compare_data = {}
        for sym in compare_symbols:
            sym_df = get_stock_prices(sym, start_date, end_date)
            if not sym_df.empty:
                # è¨ˆç®—å ±é…¬ç‡
                first_price = sym_df.iloc[0]["close"]
                sym_df["return"] = (sym_df["close"] / first_price - 1) * 100
                compare_data[sym] = sym_df

        if compare_data:
            # ç¹ªè£½æ¯”è¼ƒåœ–
            fig_compare = go.Figure()

            for sym, sym_df in compare_data.items():
                fig_compare.add_trace(
                    go.Scatter(
                        x=sym_df["date"],
                        y=sym_df["return"],
                        name=sym,
                        mode="lines"
                    )
                )

            fig_compare.update_layout(
                title="ç´¯ç©å ±é…¬ç‡æ¯”è¼ƒ (%)",
                height=400,
                xaxis_title="æ—¥æœŸ",
                yaxis_title="å ±é…¬ç‡ (%)",
                hovermode="x unified"
            )

            fig_compare.add_hline(y=0, line_dash="dash", line_color="gray")

            st.plotly_chart(fig_compare, use_container_width=True)

            # çµ±è¨ˆè¡¨æ ¼
            stats_data = []
            for sym, sym_df in compare_data.items():
                returns = sym_df["close"].pct_change().dropna()
                stats_data.append({
                    "è‚¡ç¥¨": sym,
                    "èµ·å§‹åƒ¹": f"${sym_df.iloc[0]['close']:.2f}",
                    "æœ€æ–°åƒ¹": f"${sym_df.iloc[-1]['close']:.2f}",
                    "ç´¯ç©å ±é…¬": f"{sym_df.iloc[-1]['return']:.2f}%",
                    "æ—¥å‡å ±é…¬": f"{returns.mean()*100:.3f}%",
                    "æ³¢å‹•ç‡": f"{returns.std()*100:.2f}%",
                })

            st.dataframe(pd.DataFrame(stats_data), use_container_width=True, hide_index=True)
    else:
        st.info("è«‹é¸æ“‡è‡³å°‘ 2 æª”è‚¡ç¥¨ä¾†é€²è¡Œæ¯”è¼ƒ")


def render_analysis_page():
    """æ¸²æŸ“è‚¡ç¥¨åˆ†æé é¢"""
    st.title("ğŸ¯ äº¤æ˜“ç­–ç•¥åˆ†æ")

    # æª¢æŸ¥é‡‘èè³‡æ–™åº«æ˜¯å¦å­˜åœ¨
    if not FINANCE_DB_PATH.exists():
        st.error("æ‰¾ä¸åˆ°é‡‘èè³‡æ–™åº« (finance.db)")
        return

    analyzer = TechnicalAnalyzer(str(FINANCE_DB_PATH))

    # åˆ†ææ¨¡å¼é¸æ“‡
    analysis_mode = st.radio(
        "åˆ†ææ¨¡å¼",
        ["ğŸ“Š å€‹è‚¡åˆ†æ", "ğŸ† è²·è³£æ’è¡Œæ¦œ", "ğŸ“ˆ ç­–ç•¥å›æ¸¬"],
        horizontal=True
    )

    st.divider()

    if analysis_mode == "ğŸ“Š å€‹è‚¡åˆ†æ":
        render_single_stock_analysis(analyzer)
    elif analysis_mode == "ğŸ† è²·è³£æ’è¡Œæ¦œ":
        render_top_picks(analyzer)
    else:
        render_strategy_backtest(analyzer)


def render_single_stock_analysis(analyzer: TechnicalAnalyzer):
    """å€‹è‚¡åˆ†æ"""
    # å–å¾—è‚¡ç¥¨æ¸…å–®
    watchlist = get_watchlist()
    if not watchlist:
        st.warning("è¿½è¹¤æ¸…å–®ç‚ºç©º")
        return

    # è‚¡ç¥¨é¸æ“‡
    col1, col2 = st.columns([1, 2])

    with col1:
        stock_options = {f"{s['symbol']} - {s['name'][:15] if s['name'] else s['symbol']}": s["symbol"] for s in watchlist}
        selected_stock_label = st.selectbox("é¸æ“‡è‚¡ç¥¨", list(stock_options.keys()), key="analysis_stock")
        selected_symbol = stock_options[selected_stock_label]

        # åŸ·è¡Œåˆ†ææŒ‰éˆ•
        analyze_btn = st.button("ğŸ” åŸ·è¡Œåˆ†æ", type="primary", use_container_width=True)

    if analyze_btn or 'last_analysis' in st.session_state:
        with st.spinner("åˆ†æä¸­..."):
            analysis = analyzer.get_current_analysis(selected_symbol)
            st.session_state['last_analysis'] = analysis

        # é¡¯ç¤ºå»ºè­°
        with col2:
            # å»ºè­°å¡ç‰‡
            rec = analysis['recommendation']
            rec_text = analysis.get('recommendation_text', rec)
            confidence = analysis['confidence']

            if rec in ['STRONG_BUY', 'BUY']:
                rec_color = "green"
                rec_icon = "ğŸŸ¢"
            elif rec in ['STRONG_SELL', 'SELL']:
                rec_color = "red"
                rec_icon = "ğŸ”´"
            else:
                rec_color = "orange"
                rec_icon = "ğŸŸ¡"

            st.markdown(f"""
            <div style="background-color: {'#e8f5e9' if rec_color == 'green' else '#ffebee' if rec_color == 'red' else '#fff3e0'};
                        padding: 20px; border-radius: 10px; text-align: center;">
                <h2 style="color: {rec_color}; margin: 0;">{rec_icon} {rec_text}</h2>
                <p style="margin: 10px 0;">ä¿¡å¿ƒåº¦: <strong>{confidence:.1f}%</strong></p>
            </div>
            """, unsafe_allow_html=True)

        st.divider()

        # æŠ€è¡“æŒ‡æ¨™
        st.subheader("ğŸ“Š æŠ€è¡“æŒ‡æ¨™")

        indicators = analysis.get('indicators', {})
        signals = analysis.get('signals', {})

        col_ind1, col_ind2, col_ind3, col_ind4 = st.columns(4)

        with col_ind1:
            price = indicators.get('price', 0)
            ma20 = indicators.get('MA20')
            if ma20:
                price_vs_ma = ((price - ma20) / ma20) * 100
                st.metric("æ”¶ç›¤åƒ¹", f"${price:.2f}", f"{price_vs_ma:+.2f}% vs MA20")
            else:
                st.metric("æ”¶ç›¤åƒ¹", f"${price:.2f}")

        with col_ind2:
            rsi = indicators.get('RSI')
            if rsi:
                rsi_status = "è¶…è³£" if rsi < 30 else "è¶…è²·" if rsi > 70 else "æ­£å¸¸"
                st.metric("RSI (14)", f"{rsi:.1f}", rsi_status)

        with col_ind3:
            macd = indicators.get('MACD')
            macd_signal = indicators.get('MACD_Signal')
            if macd is not None and macd_signal is not None:
                macd_diff = macd - macd_signal
                st.metric("MACD", f"{macd:.3f}", f"ä¿¡è™Ÿå·®: {macd_diff:+.3f}")

        with col_ind4:
            bb_pos = indicators.get('BB_Position')
            if bb_pos is not None:
                bb_status = "æ¥è¿‘ä¸Šè»Œ" if bb_pos > 0.8 else "æ¥è¿‘ä¸‹è»Œ" if bb_pos < 0.2 else "é€šé“ä¸­é–“"
                st.metric("å¸ƒæ—ä½ç½®", f"{bb_pos*100:.1f}%", bb_status)

        # ä¿¡è™Ÿè©³æƒ…
        st.subheader("ğŸ“¡ ç­–ç•¥ä¿¡è™Ÿ")

        signal_data = []
        signal_names = {
            'MA': ('å‡ç·šäº¤å‰', 'çŸ­æœŸå‡ç·š vs é•·æœŸå‡ç·š'),
            'RSI': ('RSI è¶…è²·è¶…è³£', 'RSI < 30 è²·å…¥, > 70 è³£å‡º'),
            'MACD': ('MACD äº¤å‰', 'MACD èˆ‡ä¿¡è™Ÿç·šäº¤å‰'),
            'BB': ('å¸ƒæ—é€šé“', 'åƒ¹æ ¼è§¸åŠé€šé“é‚Šç•Œ')
        }

        for key, (name, desc) in signal_names.items():
            signal_val = signals.get(key, 0)
            if signal_val == 1:
                signal_text = "ğŸŸ¢ è²·å…¥"
            elif signal_val == -1:
                signal_text = "ğŸ”´ è³£å‡º"
            else:
                signal_text = "ğŸŸ¡ è§€æœ›"

            signal_data.append({
                "ç­–ç•¥": name,
                "èªªæ˜": desc,
                "ä¿¡è™Ÿ": signal_text
            })

        st.dataframe(pd.DataFrame(signal_data), use_container_width=True, hide_index=True)

        # åˆ†æç†ç”±
        st.subheader("ğŸ’¡ åˆ†æç†ç”±")
        reasons = analysis.get('reasons', [])
        if reasons:
            for reason in reasons:
                st.markdown(f"â€¢ {reason}")
        else:
            st.info("ç„¡ç‰¹æ®Šä¿¡è™Ÿ")

        # å›æ¸¬çµæœ
        st.subheader("ğŸ“ˆ ç­–ç•¥å›æ¸¬ç¸¾æ•ˆ (éå»ä¸€å¹´)")

        backtest = analysis.get('backtest', {})
        if backtest:
            bt_data = []
            for strategy, results in backtest.items():
                bt_data.append({
                    "ç­–ç•¥": signal_names.get(strategy, (strategy, ''))[0],
                    "å¹´å ±é…¬ç‡": f"{results['total_return']:.2f}%",
                    "å‹ç‡": f"{results['win_rate']:.1f}%",
                    "äº¤æ˜“æ¬¡æ•¸": results['total_trades'],
                    "å¤æ™®æ¯”ç‡": f"{results['sharpe_ratio']:.2f}",
                    "æœ€å¤§å›æ’¤": f"{results['max_drawdown']:.2f}%",
                    "è²·å…¥æŒæœ‰": f"{results['buy_hold_return']:.2f}%"
                })

            st.dataframe(pd.DataFrame(bt_data), use_container_width=True, hide_index=True)

            # æ¯”è¼ƒåœ–
            st.markdown("**ç­–ç•¥å ±é…¬ vs è²·å…¥æŒæœ‰**")
            chart_data = []
            for strategy, results in backtest.items():
                chart_data.append({
                    "ç­–ç•¥": signal_names.get(strategy, (strategy, ''))[0],
                    "ç­–ç•¥å ±é…¬": results['total_return'],
                    "è²·å…¥æŒæœ‰": results['buy_hold_return']
                })

            df_chart = pd.DataFrame(chart_data)
            st.bar_chart(df_chart.set_index("ç­–ç•¥"))


def render_top_picks(analyzer: TechnicalAnalyzer):
    """è²·è³£æ’è¡Œæ¦œ"""
    st.subheader("ğŸ† ä»Šæ—¥è²·è³£å»ºè­°æ’è¡Œ")

    with st.spinner("æ­£åœ¨åˆ†ææ‰€æœ‰è‚¡ç¥¨..."):
        buy_picks, sell_picks = analyzer.get_top_picks(n=10)

    col_buy, col_sell = st.columns(2)

    with col_buy:
        st.markdown("### ğŸŸ¢ è²·é€²æ¨™çš„ TOP 10")

        if buy_picks:
            buy_data = []
            for i, pick in enumerate(buy_picks, 1):
                stock_info = get_stock_info(pick['symbol'])
                name = stock_info['name'] if stock_info else pick['symbol']
                signals = pick.get('signals', {})
                indicators = pick.get('indicators', {})

                buy_data.append({
                    "æ’å": i,
                    "ä»£ç¢¼": pick['symbol'],
                    "åç¨±": name[:10] if name else "-",
                    "å»ºè­°": pick.get('recommendation_text', '-'),
                    "ä¿¡å¿ƒåº¦": f"{pick['confidence']:.0f}%",
                    "RSI": f"{indicators.get('RSI', 0):.0f}" if indicators.get('RSI') else "-",
                    "ç¶œåˆåˆ†æ•¸": f"{pick['combined_signal']:.2f}"
                })

            st.dataframe(pd.DataFrame(buy_data), use_container_width=True, hide_index=True)

            # è©³ç´°åŸå› 
            with st.expander("ğŸ“‹ è©³ç´°åˆ†æ"):
                for pick in buy_picks[:5]:
                    st.markdown(f"**{pick['symbol']}**")
                    for reason in pick.get('reasons', [])[:3]:
                        st.markdown(f"  â€¢ {reason}")
                    st.markdown("---")
        else:
            st.info("ç›®å‰æ²’æœ‰å¼·çƒˆè²·é€²ä¿¡è™Ÿçš„è‚¡ç¥¨")

    with col_sell:
        st.markdown("### ğŸ”´ è³£å‡ºæ¨™çš„ TOP 10")

        if sell_picks:
            sell_data = []
            for i, pick in enumerate(sell_picks, 1):
                stock_info = get_stock_info(pick['symbol'])
                name = stock_info['name'] if stock_info else pick['symbol']
                signals = pick.get('signals', {})
                indicators = pick.get('indicators', {})

                sell_data.append({
                    "æ’å": i,
                    "ä»£ç¢¼": pick['symbol'],
                    "åç¨±": name[:10] if name else "-",
                    "å»ºè­°": pick.get('recommendation_text', '-'),
                    "ä¿¡å¿ƒåº¦": f"{pick['confidence']:.0f}%",
                    "RSI": f"{indicators.get('RSI', 0):.0f}" if indicators.get('RSI') else "-",
                    "ç¶œåˆåˆ†æ•¸": f"{pick['combined_signal']:.2f}"
                })

            st.dataframe(pd.DataFrame(sell_data), use_container_width=True, hide_index=True)

            # è©³ç´°åŸå› 
            with st.expander("ğŸ“‹ è©³ç´°åˆ†æ"):
                for pick in sell_picks[:5]:
                    st.markdown(f"**{pick['symbol']}**")
                    for reason in pick.get('reasons', [])[:3]:
                        st.markdown(f"  â€¢ {reason}")
                    st.markdown("---")
        else:
            st.info("ç›®å‰æ²’æœ‰å¼·çƒˆè³£å‡ºä¿¡è™Ÿçš„è‚¡ç¥¨")


def render_strategy_backtest(analyzer: TechnicalAnalyzer):
    """ç­–ç•¥å›æ¸¬ - ç›´è§€é¡¯ç¤ºè²·è³£é»å’Œç²åˆ©"""
    st.subheader("ğŸ“ˆ ç­–ç•¥å›æ¸¬æ¨¡æ“¬")

    # ç­–ç•¥é¡å‹é¸æ“‡
    strategy_type = st.radio(
        "é¸æ“‡ç­–ç•¥é¡å‹",
        ["ğŸ“Š å–®ä¸€è‚¡ç¥¨ç­–ç•¥", "ğŸ”„ å‹•æ…‹æ›è‚¡ç­–ç•¥"],
        horizontal=True
    )

    st.divider()

    if strategy_type == "ğŸ“Š å–®ä¸€è‚¡ç¥¨ç­–ç•¥":
        render_single_stock_backtest(analyzer)
    else:
        render_momentum_rotation()


def render_single_stock_backtest(analyzer: TechnicalAnalyzer):
    """å–®ä¸€è‚¡ç¥¨ç­–ç•¥å›æ¸¬"""
    st.info("ğŸ’¡ å‡è¨­åˆå§‹è³‡é‡‘ 10 è¬å…ƒï¼Œæ ¹æ“šç­–ç•¥ä¿¡è™Ÿè²·é€²è³£å‡ºï¼Œçœ‹çœ‹èƒ½è³ºå¤šå°‘éŒ¢")

    # å–å¾—è‚¡ç¥¨æ¸…å–®
    watchlist = get_watchlist()
    if not watchlist:
        st.warning("è¿½è¹¤æ¸…å–®ç‚ºç©º")
        return

    col1, col2, col3 = st.columns(3)

    with col1:
        stock_options = {f"{s['symbol']} - {s['name'][:12] if s['name'] else s['symbol']}": s["symbol"] for s in watchlist}
        selected_stock = st.selectbox("é¸æ“‡è‚¡ç¥¨", list(stock_options.keys()), key="bt_stock")
        selected_symbol = stock_options[selected_stock]

    with col2:
        strategy_options = {
            "ğŸ“ˆ è²·å…¥æŒæœ‰ (Buy & Hold)": "BH",
            "å‡ç·šäº¤å‰ (MA5/MA20)": "MA",
            "RSI è¶…è²·è¶…è³£": "RSI",
            "MACD é‡‘æ­»å‰": "MACD",
            "å¸ƒæ—é€šé“çªç ´": "BB"
        }
        selected_strategy_name = st.selectbox("é¸æ“‡ç­–ç•¥", list(strategy_options.keys()))
        selected_strategy = strategy_options[selected_strategy_name]

    with col3:
        initial_capital = st.number_input("åˆå§‹è³‡é‡‘", value=100000, step=10000, format="%d")

    # å›æ¸¬æœŸé–“é¸æ“‡
    st.markdown("##### å›æ¸¬æœŸé–“")
    date_col1, date_col2 = st.columns(2)
    with date_col1:
        single_start_year = st.selectbox("èµ·å§‹å¹´ä»½", [2021, 2022, 2023, 2024, 2025], index=0, key="single_start_year")
    with date_col2:
        single_end_year = st.selectbox("çµæŸå¹´ä»½", [2021, 2022, 2023, 2024, 2025, 2026], index=5, key="single_end_year")

    if single_start_year > single_end_year:
        st.error("èµ·å§‹å¹´ä»½ä¸èƒ½å¤§æ–¼çµæŸå¹´ä»½")
        return

    start_date_str = f"{single_start_year}-01-01"
    end_date_str = f"{single_end_year}-12-31" if single_end_year < 2026 else date.today().strftime("%Y-%m-%d")

    if st.button("ğŸš€ é–‹å§‹å›æ¸¬", type="primary", use_container_width=True):
        with st.spinner("å›æ¸¬è¨ˆç®—ä¸­..."):
            # æ ¹æ“šç­–ç•¥é¸æ“‡ä¸åŒçš„å›æ¸¬æ–¹æ³•
            if selected_strategy == "BH":
                # Buy and Hold ç­–ç•¥
                portfolio = PortfolioStrategy(str(FINANCE_DB_PATH))
                result = portfolio.buy_and_hold(
                    selected_symbol, initial_capital,
                    start_date=start_date_str, end_date=end_date_str
                )
            else:
                # æŠ€è¡“æŒ‡æ¨™ç­–ç•¥
                result = analyzer.get_trade_history(
                    selected_symbol, selected_strategy, initial_capital,
                    start_date=start_date_str, end_date=end_date_str
                )

        trades = result['trades']
        equity_curve = result['equity_curve']
        summary = result['summary']
        df = result.get('df', pd.DataFrame())

        if not trades:
            st.warning(f"æ­¤ç­–ç•¥åœ¨ {single_start_year}-{single_end_year} æœŸé–“æ²’æœ‰ç”¢ç”Ÿä»»ä½•äº¤æ˜“ä¿¡è™Ÿ")
            return

        # ========== ç¸½çµå¡ç‰‡ ==========
        st.divider()
        st.subheader("ğŸ’° å›æ¸¬çµæœç¸½çµ")

        col_s1, col_s2, col_s3, col_s4 = st.columns(4)

        total_profit = summary['total_profit']
        total_return = summary['total_return_pct']

        with col_s1:
            if total_profit >= 0:
                st.metric("ç¸½ç²åˆ©", f"${total_profit:,.0f}", f"+{total_return:.1f}%")
            else:
                st.metric("ç¸½è™§æ", f"${total_profit:,.0f}", f"{total_return:.1f}%")

        with col_s2:
            if selected_strategy == "BH":
                st.metric("æŒæœ‰å¤©æ•¸", f"{summary.get('holding_days', 0)} å¤©")
            else:
                st.metric("äº¤æ˜“æ¬¡æ•¸", f"{summary['total_trades']} æ¬¡",
                          f"å‹ç‡ {summary.get('win_rate', 0):.0f}%")

        with col_s3:
            st.metric("æœ€çµ‚è³‡é‡‘", f"${summary['final_equity']:,.0f}",
                      f"åˆå§‹ ${initial_capital:,}")

        with col_s4:
            if selected_strategy != "BH":
                buy_hold = summary.get('buy_hold_return', 0)
                diff = total_return - buy_hold
                st.metric("è²·å…¥æŒæœ‰å ±é…¬", f"{buy_hold:.1f}%",
                          f"ç­–ç•¥{'å‹' if diff > 0 else 'è² '} {abs(diff):.1f}%")
            else:
                max_dd = summary.get('max_drawdown', 0)
                st.metric("æœ€å¤§å›æ’¤", f"{max_dd:.1f}%")

        # ========== åƒ¹æ ¼åœ–è¡¨ + è²·è³£é» ==========
        st.divider()
        st.subheader("ğŸ“Š è²·è³£é»è¦–è¦ºåŒ–")

        # å»ºç«‹åœ–è¡¨
        fig = make_subplots(
            rows=2, cols=1,
            shared_xaxes=True,
            vertical_spacing=0.1,
            row_heights=[0.7, 0.3],
            subplot_titles=(f"{selected_symbol} åƒ¹æ ¼èˆ‡è²·è³£é»", "è³‡é‡‘æ›²ç·š")
        )

        # åƒ¹æ ¼ç·š
        fig.add_trace(
            go.Scatter(
                x=df['date'],
                y=df['close'],
                mode='lines',
                name='è‚¡åƒ¹',
                line=dict(color='#1f77b4', width=1.5)
            ),
            row=1, col=1
        )

        # è²·å…¥é»
        buy_dates = []
        buy_prices = []
        buy_texts = []
        for trade in trades:
            buy_dates.append(trade['entry_date'])
            buy_prices.append(trade['entry_price'])
            buy_texts.append(f"è²·å…¥ ${trade['entry_price']:.2f}<br>{trade['shares']}è‚¡")

        fig.add_trace(
            go.Scatter(
                x=buy_dates,
                y=buy_prices,
                mode='markers',
                name='è²·å…¥',
                marker=dict(
                    symbol='triangle-up',
                    size=15,
                    color='green'
                ),
                text=buy_texts,
                hoverinfo='text+x'
            ),
            row=1, col=1
        )

        # è³£å‡ºé»
        sell_dates = []
        sell_prices = []
        sell_texts = []
        for trade in trades:
            if trade.get('exit_date') != 'æŒæœ‰ä¸­':
                sell_dates.append(trade['exit_date'])
                sell_prices.append(trade['exit_price'])
                profit_sign = '+' if trade['profit'] >= 0 else ''
                sell_texts.append(f"è³£å‡º ${trade['exit_price']:.2f}<br>ç²åˆ© {profit_sign}${trade['profit']:.0f}")

        fig.add_trace(
            go.Scatter(
                x=sell_dates,
                y=sell_prices,
                mode='markers',
                name='è³£å‡º',
                marker=dict(
                    symbol='triangle-down',
                    size=15,
                    color='red'
                ),
                text=sell_texts,
                hoverinfo='text+x'
            ),
            row=1, col=1
        )

        # è³‡é‡‘æ›²ç·š
        eq_dates = [e['date'] for e in equity_curve]
        eq_values = [e['equity'] for e in equity_curve]

        fig.add_trace(
            go.Scatter(
                x=eq_dates,
                y=eq_values,
                mode='lines',
                name='è³‡é‡‘',
                line=dict(color='#ff7f0e', width=2),
                fill='tozeroy',
                fillcolor='rgba(255, 127, 14, 0.1)'
            ),
            row=2, col=1
        )

        # åˆå§‹è³‡é‡‘ç·š
        fig.add_hline(y=initial_capital, line_dash="dash", line_color="gray",
                      annotation_text=f"åˆå§‹è³‡é‡‘ ${initial_capital:,}", row=2, col=1)

        fig.update_layout(
            height=600,
            showlegend=True,
            legend=dict(orientation="h", yanchor="bottom", y=1.02),
            hovermode='x unified'
        )

        fig.update_yaxes(title_text="è‚¡åƒ¹", row=1, col=1)
        fig.update_yaxes(title_text="è³‡é‡‘", row=2, col=1)

        st.plotly_chart(fig, use_container_width=True)

        # ========== äº¤æ˜“æ˜ç´° ==========
        st.divider()
        st.subheader("ğŸ“‹ äº¤æ˜“æ˜ç´°")

        trade_data = []
        for i, trade in enumerate(trades, 1):
            profit_display = f"+${trade['profit']:,.0f}" if trade['profit'] >= 0 else f"-${abs(trade['profit']):,.0f}"
            pct_display = f"+{trade['profit_pct']:.1f}%" if trade['profit_pct'] >= 0 else f"{trade['profit_pct']:.1f}%"

            trade_data.append({
                "äº¤æ˜“": f"#{i}",
                "è²·å…¥æ—¥æœŸ": trade['entry_date'],
                "è²·å…¥åƒ¹": f"${trade['entry_price']:.2f}",
                "è³£å‡ºæ—¥æœŸ": trade['exit_date'],
                "è³£å‡ºåƒ¹": f"${trade['exit_price']:.2f}",
                "è‚¡æ•¸": trade['shares'],
                "ç²åˆ©": profit_display,
                "å ±é…¬ç‡": pct_display,
                "çµæœ": "ğŸŸ¢ ç²åˆ©" if trade['profit'] >= 0 else "ğŸ”´ è™§æ"
            })

        df_trades = pd.DataFrame(trade_data)
        st.dataframe(df_trades, use_container_width=True, hide_index=True)

        # ========== ç­–ç•¥èªªæ˜ ==========
        st.divider()
        with st.expander("ğŸ“– ç­–ç•¥èªªæ˜"):
            strategy_explanations = {
                "BH": """
                **è²·å…¥æŒæœ‰ç­–ç•¥ (Buy and Hold)**

                - **è²·å…¥æ™‚æ©Ÿ**: ç¬¬ä¸€å¤©é–‹ç›¤è²·å…¥
                - **è³£å‡ºæ™‚æ©Ÿ**: æŒæœ‰åˆ°æœ€å¾Œä¸€å¤©

                æœ€ç°¡å–®çš„é•·æœŸæŠ•è³‡ç­–ç•¥ï¼Œé©åˆçœ‹å¥½é•·æœŸè¶¨å‹¢çš„æŠ•è³‡äººã€‚
                å·´è²ç‰¹åè¨€ï¼šã€Œå¦‚æœä½ ä¸é¡˜æ„æŒæœ‰ä¸€æª”è‚¡ç¥¨åå¹´ï¼Œé‚£å°±é€£ååˆ†é˜éƒ½ä¸è¦æŒæœ‰ã€‚ã€
                """,
                "MA": """
                **å‡ç·šäº¤å‰ç­–ç•¥ (MA5/MA20)**

                - **è²·å…¥æ™‚æ©Ÿ**: ç•¶ 5 æ—¥å‡ç·šå¾ä¸‹æ–¹ç©¿è¶Š 20 æ—¥å‡ç·šï¼ˆé»ƒé‡‘äº¤å‰ï¼‰
                - **è³£å‡ºæ™‚æ©Ÿ**: ç•¶ 5 æ—¥å‡ç·šå¾ä¸Šæ–¹è·Œç ´ 20 æ—¥å‡ç·šï¼ˆæ­»äº¡äº¤å‰ï¼‰

                é€™æ˜¯æœ€å¸¸è¦‹çš„è¶¨å‹¢è·Ÿéš¨ç­–ç•¥ï¼Œé©åˆæ³¢æ®µæ“ä½œã€‚
                """,
                "RSI": """
                **RSI è¶…è²·è¶…è³£ç­–ç•¥**

                - **è²·å…¥æ™‚æ©Ÿ**: ç•¶ RSI ä½æ–¼ 30ï¼ˆè¶…è³£å€ï¼‰
                - **è³£å‡ºæ™‚æ©Ÿ**: ç•¶ RSI é«˜æ–¼ 70ï¼ˆè¶…è²·å€ï¼‰

                RSI æ˜¯å‹•é‡æŒ‡æ¨™ï¼Œé©åˆéœ‡ç›ªç›¤æ•´æ™‚ä½¿ç”¨ã€‚
                """,
                "MACD": """
                **MACD é‡‘æ­»å‰ç­–ç•¥**

                - **è²·å…¥æ™‚æ©Ÿ**: ç•¶ MACD ç·šå¾ä¸‹æ–¹ç©¿è¶Šä¿¡è™Ÿç·šï¼ˆé‡‘å‰ï¼‰
                - **è³£å‡ºæ™‚æ©Ÿ**: ç•¶ MACD ç·šå¾ä¸Šæ–¹è·Œç ´ä¿¡è™Ÿç·šï¼ˆæ­»å‰ï¼‰

                MACD çµåˆè¶¨å‹¢å’Œå‹•é‡ï¼Œæ˜¯è¼ƒç‚ºéˆæ•çš„æŒ‡æ¨™ã€‚
                """,
                "BB": """
                **å¸ƒæ—é€šé“çªç ´ç­–ç•¥**

                - **è²·å…¥æ™‚æ©Ÿ**: ç•¶è‚¡åƒ¹è·Œç ´å¸ƒæ—ä¸‹è»Œï¼ˆè¶…è³£åå½ˆï¼‰
                - **è³£å‡ºæ™‚æ©Ÿ**: ç•¶è‚¡åƒ¹çªç ´å¸ƒæ—ä¸Šè»Œï¼ˆè¶…è²·å›è½ï¼‰

                å¸ƒæ—é€šé“åˆ©ç”¨çµ±è¨ˆå­¸åŸç†åˆ¤æ–·åƒ¹æ ¼æ¥µç«¯ä½ç½®ã€‚
                """
            }
            st.markdown(strategy_explanations.get(selected_strategy, ""))


def render_momentum_rotation():
    """å‹•æ…‹æ›è‚¡ç­–ç•¥å›æ¸¬"""
    st.info("""
    ğŸ’¡ **å‹•æ…‹æ›è‚¡ç­–ç•¥ (Momentum Rotation)**

    è‡ªå‹•åˆ†æè‚¡ç¥¨æ± ä¸­æ‰€æœ‰è‚¡ç¥¨çš„å‹•èƒ½ï¼Œå®šæœŸé¸æ“‡è¡¨ç¾æœ€å¼·çš„è‚¡ç¥¨æŒæœ‰ã€‚

    - è¨ˆç®—æ¯æª”è‚¡ç¥¨éå» N å¤©çš„å ±é…¬ç‡ï¼ˆå‹•èƒ½ï¼‰
    - é¸æ“‡å‹•èƒ½æœ€å¼·çš„å‰ K æª”è‚¡ç¥¨
    - å¹³å‡åˆ†é…è³‡é‡‘
    - æ¯ M å¤©é‡æ–°èª¿æ•´æŒè‚¡
    """)

    col1, col2, col3 = st.columns(3)

    with col1:
        initial_capital = st.number_input("åˆå§‹è³‡é‡‘", value=100000, step=10000, format="%d", key="mom_capital")
        top_n = st.slider("æŒæœ‰è‚¡ç¥¨æ•¸", min_value=3, max_value=10, value=5, key="mom_top_n")

    with col2:
        market_options = {"ç¾è‚¡": "US", "å°è‚¡": "TW", "ETF": "ETF"}
        selected_market_name = st.selectbox("é¸æ“‡å¸‚å ´", list(market_options.keys()), key="mom_market")
        selected_market = market_options[selected_market_name]

        rebalance_days = st.slider("èª¿å€‰é€±æœŸï¼ˆå¤©ï¼‰", min_value=5, max_value=60, value=20, key="mom_rebal")

    with col3:
        lookback_days = st.slider("å‹•èƒ½è¨ˆç®—å¤©æ•¸", min_value=5, max_value=60, value=20, key="mom_lookback")

    # å›æ¸¬æœŸé–“é¸æ“‡
    st.markdown("##### å›æ¸¬æœŸé–“")
    mom_date_col1, mom_date_col2 = st.columns(2)
    with mom_date_col1:
        mom_start_year = st.selectbox("èµ·å§‹å¹´ä»½", [2021, 2022, 2023, 2024, 2025], index=0, key="mom_start_year")
    with mom_date_col2:
        mom_end_year = st.selectbox("çµæŸå¹´ä»½", [2021, 2022, 2023, 2024, 2025, 2026], index=5, key="mom_end_year")

    if mom_start_year > mom_end_year:
        st.error("èµ·å§‹å¹´ä»½ä¸èƒ½å¤§æ–¼çµæŸå¹´ä»½")
        return

    mom_start_date = f"{mom_start_year}-01-01"
    mom_end_date = f"{mom_end_year}-12-31" if mom_end_year < 2026 else date.today().strftime("%Y-%m-%d")

    # é€²éšé¸é …
    st.markdown("##### é€²éšé¸é …")
    adv_col1, adv_col2 = st.columns(2)
    with adv_col1:
        use_vol_adjust = st.checkbox("ğŸ“Š æ³¢å‹•ç‡æ ¡æ­£", value=True, key="use_vol_adjust",
                                      help="ä½¿ç”¨é¢¨éšªèª¿æ•´å¾Œçš„å‹•é‡æŒ‡æ¨™ï¼Œé™ä½é«˜æ³¢å‹•è‚¡ç¥¨çš„æ¬Šé‡")
        vol_method = st.selectbox("æ ¡æ­£æ–¹æ³•", ["sharpe", "sortino", "vol_scaled"],
                                   index=0, key="vol_method",
                                   help="sharpe=å¤æ™®æ¯”ç‡, sortino=ç´¢æè«¾æ¯”ç‡, vol_scaled=æ³¢å‹•ç‡ç¸®æ”¾")
    with adv_col2:
        run_robustness = st.checkbox("ğŸ”¬ é­¯æ£’æ€§æª¢æ¸¬", value=False, key="run_robustness",
                                      help="æ¸¬è©¦ä¸åŒåƒæ•¸çµ„åˆçš„ç©©å®šæ€§")
        run_walkforward = st.checkbox("ğŸ“ˆ èµ°å‹•å¼è©•ä¼°", value=False, key="run_walkforward",
                                       help="ä½¿ç”¨æ»¾å‹•è¦–çª—é€²è¡Œæ¨£æœ¬å¤–æ¸¬è©¦")

    # ä¸»è¦å›æ¸¬æŒ‰éˆ•
    if st.button("ğŸš€ é–‹å§‹å‹•æ…‹æ›è‚¡å›æ¸¬", type="primary", use_container_width=True):
        portfolio = PortfolioStrategy(str(FINANCE_DB_PATH))

        with st.spinner(f"æ­£åœ¨è¨ˆç®— {selected_market_name} å¸‚å ´ ({mom_start_year}-{mom_end_year}) çš„å‹•æ…‹æ›è‚¡ç­–ç•¥..."):
            if use_vol_adjust:
                result = portfolio.momentum_rotation_vol_adjusted(
                    symbols=None,
                    initial_capital=initial_capital,
                    top_n=top_n,
                    rebalance_days=rebalance_days,
                    lookback_days=lookback_days,
                    market=selected_market,
                    start_date=mom_start_date,
                    end_date=mom_end_date,
                    vol_adjust_method=vol_method
                )
            else:
                result = portfolio.momentum_rotation(
                    symbols=None,
                    initial_capital=initial_capital,
                    top_n=top_n,
                    rebalance_days=rebalance_days,
                    lookback_days=lookback_days,
                    market=selected_market,
                    start_date=mom_start_date,
                    end_date=mom_end_date
                )

        if 'error' in result:
            st.error(result['error'])
            return

        summary = result['summary']
        trades = result['trades']
        equity_curve = result['equity_curve']
        rebalance_records = result['rebalance_records']

        # ========== ç¸½çµå¡ç‰‡ ==========
        st.divider()
        st.subheader("ğŸ’° å‹•æ…‹æ›è‚¡ç­–ç•¥çµæœ")

        col_s1, col_s2, col_s3, col_s4 = st.columns(4)

        total_profit = summary['total_profit']
        total_return = summary['total_return_pct']

        with col_s1:
            if total_profit >= 0:
                st.metric("ç¸½ç²åˆ©", f"${total_profit:,.0f}", f"+{total_return:.1f}%")
            else:
                st.metric("ç¸½è™§æ", f"${total_profit:,.0f}", f"{total_return:.1f}%")

        with col_s2:
            st.metric("èª¿å€‰æ¬¡æ•¸", f"{summary['rebalance_count']} æ¬¡",
                      f"äº¤æ˜“ {summary['total_trades']} ç­†")

        with col_s3:
            st.metric("æœ€çµ‚è³‡é‡‘", f"${summary['final_equity']:,.0f}",
                      f"åˆå§‹ ${initial_capital:,}")

        with col_s4:
            buy_hold = summary.get('buy_hold_return', 0)
            diff = total_return - buy_hold
            st.metric("ç­‰æ¬ŠæŒæœ‰å ±é…¬", f"{buy_hold:.1f}%",
                      f"ç­–ç•¥{'å‹' if diff > 0 else 'è² '} {abs(diff):.1f}%")

        # ========== è³‡é‡‘æ›²ç·šåœ–è¡¨ ==========
        st.divider()
        st.subheader("ğŸ“ˆ è³‡é‡‘æ›²ç·š")

        if equity_curve:
            eq_dates = [e['date'] for e in equity_curve]
            eq_values = [e['equity'] for e in equity_curve]

            fig = go.Figure()

            # è³‡é‡‘æ›²ç·š
            fig.add_trace(
                go.Scatter(
                    x=eq_dates,
                    y=eq_values,
                    mode='lines',
                    name='å‹•æ…‹æ›è‚¡ç­–ç•¥',
                    line=dict(color='#1f77b4', width=2),
                    fill='tozeroy',
                    fillcolor='rgba(31, 119, 180, 0.1)'
                )
            )

            # åˆå§‹è³‡é‡‘ç·š
            fig.add_hline(y=initial_capital, line_dash="dash", line_color="gray",
                          annotation_text=f"åˆå§‹è³‡é‡‘ ${initial_capital:,}")

            # æ¨™è¨˜èª¿å€‰æ—¥
            rebal_dates = [r['date'] for r in rebalance_records]
            rebal_values = []
            for rd in rebal_dates:
                for eq in equity_curve:
                    if str(eq['date'])[:10] == rd:
                        rebal_values.append(eq['equity'])
                        break
                else:
                    rebal_values.append(None)

            fig.add_trace(
                go.Scatter(
                    x=rebal_dates,
                    y=rebal_values,
                    mode='markers',
                    name='èª¿å€‰æ—¥',
                    marker=dict(symbol='diamond', size=10, color='orange')
                )
            )

            fig.update_layout(
                height=400,
                xaxis_title="æ—¥æœŸ",
                yaxis_title="è³‡é‡‘",
                hovermode='x unified'
            )

            st.plotly_chart(fig, use_container_width=True)

        # ========== èª¿å€‰è¨˜éŒ„ ==========
        st.divider()
        st.subheader("ğŸ”„ èª¿å€‰è¨˜éŒ„")

        if rebalance_records:
            rebal_data = []
            for i, rec in enumerate(rebalance_records, 1):
                # è™•ç† momentum æˆ– adjusted_momentum
                mom_data = rec.get('momentum') or rec.get('adjusted_momentum', {})
                top_stock = rec['selected'][0] if rec['selected'] else "-"
                top_mom = mom_data.get(top_stock, "-") if mom_data and top_stock != "-" else "-"

                rebal_data.append({
                    "æ¬¡æ•¸": i,
                    "æ—¥æœŸ": rec['date'],
                    "é¸ä¸­è‚¡ç¥¨": ", ".join(rec['selected'][:5]),
                    "å‹•èƒ½æœ€å¼·": f"{top_stock} ({top_mom})" if top_stock != "-" else "-",
                    "çµ„åˆåƒ¹å€¼": f"${rec['total_value']:,.0f}"
                })

            st.dataframe(pd.DataFrame(rebal_data), use_container_width=True, hide_index=True)

        # ========== ç•¶å‰æŒè‚¡ ==========
        st.divider()
        st.subheader("ğŸ“Š æœ€çµ‚æŒè‚¡")

        final_holdings = summary.get('final_holdings', {})
        if final_holdings:
            holdings_data = []
            for sym, shares in final_holdings.items():
                holdings_data.append({
                    "è‚¡ç¥¨": sym,
                    "è‚¡æ•¸": shares
                })
            st.dataframe(pd.DataFrame(holdings_data), use_container_width=True, hide_index=True)
        else:
            st.info("ç­–ç•¥çµæŸæ™‚å·²å…¨éƒ¨å‡ºæ¸…")

        # ========== äº¤æ˜“æ˜ç´° ==========
        with st.expander("ğŸ“‹ è©³ç´°äº¤æ˜“è¨˜éŒ„"):
            if trades:
                trade_data = []
                for trade in trades[-50:]:  # é¡¯ç¤ºæœ€è¿‘50ç­†
                    # è™•ç† momentum æˆ– adjusted_momentum
                    mom_val = trade.get('momentum') or trade.get('adjusted_momentum', '-')
                    trade_data.append({
                        "æ—¥æœŸ": trade['date'],
                        "å‹•ä½œ": "ğŸŸ¢ è²·å…¥" if trade['action'] == 'BUY' else "ğŸ”´ è³£å‡º",
                        "è‚¡ç¥¨": trade['symbol'],
                        "è‚¡æ•¸": trade['shares'],
                        "åƒ¹æ ¼": f"${trade['price']:.2f}",
                        "é‡‘é¡": f"${trade['value']:,.0f}",
                        "åŸå› ": trade.get('reason', '-'),
                        "å‹•èƒ½": mom_val
                    })

                st.dataframe(pd.DataFrame(trade_data), use_container_width=True, hide_index=True)
                st.caption(f"é¡¯ç¤ºæœ€è¿‘ 50 ç­†äº¤æ˜“ (å…± {len(trades)} ç­†)")

        # ========== ç­–ç•¥èªªæ˜ ==========
        st.divider()
        with st.expander("ğŸ“– ç­–ç•¥èªªæ˜"):
            st.markdown(f"""
            **å‹•æ…‹æ›è‚¡ç­–ç•¥ (Momentum Rotation)**

            æœ¬ç­–ç•¥åŸºæ–¼ã€Œå‹•é‡æ•ˆæ‡‰ã€ï¼šéå»è¡¨ç¾å¥½çš„è‚¡ç¥¨ï¼ŒçŸ­æœŸå…§å‚¾å‘æ–¼ç¹¼çºŒè¡¨ç¾å¥½ã€‚

            **åƒæ•¸è¨­å®š:**
            - è‚¡ç¥¨æ± : {selected_market_name} å¸‚å ´ ({summary.get('stock_pool_size', 0)} æª”)
            - æŒæœ‰æ•¸é‡: {top_n} æª”
            - èª¿å€‰é€±æœŸ: æ¯ {rebalance_days} å¤©
            - å‹•èƒ½è¨ˆç®—: éå» {lookback_days} å¤©å ±é…¬ç‡

            **ç­–ç•¥é‚è¼¯:**
            1. è¨ˆç®—è‚¡ç¥¨æ± ä¸­æ¯æª”è‚¡ç¥¨éå» {lookback_days} å¤©çš„å ±é…¬ç‡
            2. é¸æ“‡å ±é…¬ç‡æœ€é«˜çš„å‰ {top_n} æª”
            3. å¹³å‡åˆ†é…è³‡é‡‘è²·å…¥
            4. æ¯ {rebalance_days} å¤©é‡æ–°è¨ˆç®—ï¼Œæ±°å¼±ç•™å¼·

            **å„ªé»:**
            - è‡ªå‹•è¿½è¹¤å¸‚å ´ç†±é»
            - åˆ†æ•£æŠ•è³‡é™ä½é¢¨éšª
            - å®šæœŸèª¿æ•´é¿å…æŠ±æ­»

            **é¢¨éšª:**
            - é »ç¹äº¤æ˜“ç”¢ç”Ÿæˆæœ¬
            - å‹•èƒ½åè½‰æ™‚å¯èƒ½è™§æ
            - éå»ç¸¾æ•ˆä¸ä»£è¡¨æœªä¾†
            """)

    # ========== é­¯æ£’æ€§æª¢æ¸¬ ==========
    if run_robustness:
        st.divider()
        st.subheader("ğŸ”¬ é­¯æ£’æ€§æª¢æ¸¬")
        st.info("æ¸¬è©¦ä¸åŒåƒæ•¸çµ„åˆçš„ç¸¾æ•ˆç©©å®šæ€§")

        if st.button("åŸ·è¡Œé­¯æ£’æ€§æª¢æ¸¬", key="run_robust_btn"):
            robust_portfolio = PortfolioStrategy(str(FINANCE_DB_PATH))
            with st.spinner("æ­£åœ¨é€²è¡Œåƒæ•¸æ•æ„Ÿåº¦åˆ†æ... (å¯èƒ½éœ€è¦æ•¸åˆ†é˜)"):
                robust_result = robust_portfolio.robustness_test(
                    symbols=None,
                    initial_capital=initial_capital,
                    market=selected_market,
                    start_date=mom_start_date,
                    end_date=mom_end_date
                )

            if 'error' in robust_result:
                st.error(robust_result['error'])
            else:
                robust_summary = robust_result['summary']

                st.markdown("#### ğŸ“Š æª¢æ¸¬çµæœæ‘˜è¦")

                rcol1, rcol2, rcol3, rcol4 = st.columns(4)
                with rcol1:
                    st.metric("æ¸¬è©¦çµ„åˆæ•¸", robust_summary['total_tests'])
                with rcol2:
                    vol_benefit = robust_summary['vol_adjustment_benefit']
                    st.metric("æ³¢å‹•ç‡æ ¡æ­£æ•ˆç›Š",
                              f"{vol_benefit:+.3f}",
                              "å¤æ™®æå‡" if vol_benefit > 0 else "å¤æ™®ä¸‹é™")
                with rcol3:
                    st.metric("åŸå§‹ç­–ç•¥æ­£å ±é…¬æ¯”ä¾‹", f"{robust_summary['raw_positive_pct']:.1f}%")
                with rcol4:
                    st.metric("æ ¡æ­£ç­–ç•¥æ­£å ±é…¬æ¯”ä¾‹", f"{robust_summary['vol_positive_pct']:.1f}%")

                st.markdown("#### ğŸ† æœ€ä½³åƒæ•¸")
                best_col1, best_col2 = st.columns(2)
                with best_col1:
                    st.markdown("**åŸå§‹å‹•é‡ç­–ç•¥**")
                    st.write(f"- top_n: {robust_summary['best_raw_params']['top_n']}")
                    st.write(f"- èª¿å€‰é€±æœŸ: {robust_summary['best_raw_params']['rebalance_days']} å¤©")
                    st.write(f"- å›é¡§å¤©æ•¸: {robust_summary['best_raw_params']['lookback_days']} å¤©")
                    st.write(f"- å¤æ™®æ¯”ç‡: {robust_summary['best_raw_sharpe']:.3f}")
                    st.write(f"- å ±é…¬ç‡: {robust_summary['best_raw_return']:.2f}%")

                with best_col2:
                    st.markdown("**æ³¢å‹•ç‡æ ¡æ­£ç­–ç•¥**")
                    st.write(f"- top_n: {robust_summary['best_vol_params']['top_n']}")
                    st.write(f"- èª¿å€‰é€±æœŸ: {robust_summary['best_vol_params']['rebalance_days']} å¤©")
                    st.write(f"- å›é¡§å¤©æ•¸: {robust_summary['best_vol_params']['lookback_days']} å¤©")
                    st.write(f"- å¤æ™®æ¯”ç‡: {robust_summary['best_vol_sharpe']:.3f}")
                    st.write(f"- å ±é…¬ç‡: {robust_summary['best_vol_return']:.2f}%")

                # åƒæ•¸æ•æ„Ÿåº¦åœ–
                st.markdown("#### ğŸ“ˆ åƒæ•¸æ•æ„Ÿåº¦åˆ†æ")

                sensitivity = robust_result['sensitivity']

                # Top N æ•æ„Ÿåº¦
                fig_sens = go.Figure()

                top_n_vals = list(sensitivity['top_n']['raw_sharpe'].keys())
                raw_sharpes = [sensitivity['top_n']['raw_sharpe'][k] for k in top_n_vals]
                vol_sharpes = [sensitivity['top_n']['vol_sharpe'][k] for k in top_n_vals]

                fig_sens.add_trace(go.Bar(name='åŸå§‹å‹•é‡', x=[str(x) for x in top_n_vals], y=raw_sharpes))
                fig_sens.add_trace(go.Bar(name='æ³¢å‹•ç‡æ ¡æ­£', x=[str(x) for x in top_n_vals], y=vol_sharpes))

                fig_sens.update_layout(
                    title="Top N åƒæ•¸å°å¤æ™®æ¯”ç‡çš„å½±éŸ¿",
                    xaxis_title="æŒè‚¡æ•¸é‡ (Top N)",
                    yaxis_title="å¹³å‡å¤æ™®æ¯”ç‡",
                    barmode='group',
                    height=300
                )
                st.plotly_chart(fig_sens, use_container_width=True)

                # è©³ç´°çµæœè¡¨æ ¼
                with st.expander("ğŸ“‹ å®Œæ•´æ¸¬è©¦çµæœ"):
                    param_df = pd.DataFrame(robust_result['param_results'])
                    param_df = param_df.round(3)
                    st.dataframe(param_df, use_container_width=True, hide_index=True)

    # ========== èµ°å‹•å¼è©•ä¼° ==========
    if run_walkforward:
        st.divider()
        st.subheader("ğŸ“ˆ èµ°å‹•å¼è©•ä¼° (Walk-Forward Analysis)")
        st.info("ä½¿ç”¨æ»¾å‹•è¦–çª—é€²è¡Œæ¨£æœ¬å¤–æ¸¬è©¦ï¼Œé©—è­‰ç­–ç•¥çš„ç©©å®šæ€§")

        wf_col1, wf_col2 = st.columns(2)
        with wf_col1:
            wf_train = st.slider("è¨“ç·´æœŸ (æœˆ)", min_value=3, max_value=12, value=6, key="wf_train")
        with wf_col2:
            wf_test = st.slider("æ¸¬è©¦æœŸ (æœˆ)", min_value=1, max_value=6, value=3, key="wf_test")

        if st.button("åŸ·è¡Œèµ°å‹•å¼è©•ä¼°", key="run_wf_btn"):
            wf_portfolio = PortfolioStrategy(str(FINANCE_DB_PATH))
            with st.spinner("æ­£åœ¨é€²è¡Œèµ°å‹•å¼è©•ä¼°... (å¯èƒ½éœ€è¦è¼ƒé•·æ™‚é–“)"):
                wf_result = wf_portfolio.walk_forward_analysis(
                    symbols=None,
                    initial_capital=initial_capital,
                    market=selected_market,
                    start_date=mom_start_date,
                    end_date=mom_end_date,
                    train_months=wf_train,
                    test_months=wf_test,
                    vol_adjusted=use_vol_adjust
                )

            if 'error' in wf_result.get('summary', {}):
                st.error(wf_result['summary']['error'])
            else:
                wf_summary = wf_result['summary']

                st.markdown("#### ğŸ“Š èµ°å‹•å¼è©•ä¼°çµæœ")

                wfcol1, wfcol2, wfcol3, wfcol4 = st.columns(4)
                with wfcol1:
                    st.metric("è©•ä¼°è¦–çª—æ•¸", wf_summary['total_windows'])
                with wfcol2:
                    st.metric("å¹³å‡æ¸¬è©¦å ±é…¬", f"{wf_summary['avg_test_return_pct']:.2f}%")
                with wfcol3:
                    st.metric("å¹³å‡æ¸¬è©¦å¤æ™®", f"{wf_summary['avg_test_sharpe']:.3f}")
                with wfcol4:
                    st.metric("ä¸€è‡´æ€§ (æ­£å ±é…¬æ¯”ä¾‹)", f"{wf_summary['consistency_pct']:.1f}%",
                              f"{wf_summary['positive_windows']}/{wf_summary['total_windows']} è¦–çª—")

                # å„è¦–çª—çµæœ
                st.markdown("#### ğŸ“‹ å„è¦–çª—çµæœ")
                window_df = pd.DataFrame(wf_result['window_results'])
                window_df['test_return'] = window_df['test_return'].round(2)
                window_df['test_sharpe'] = window_df['test_sharpe'].round(3)
                window_df['test_max_dd'] = window_df['test_max_dd'].round(2)
                window_df['train_sharpe'] = window_df['train_sharpe'].round(3)

                # æ ¹æ“šå ±é…¬ç‡è‘—è‰²
                def color_returns(val):
                    if isinstance(val, (int, float)):
                        color = 'green' if val > 0 else 'red'
                        return f'color: {color}'
                    return ''

                styled_df = window_df.style.applymap(color_returns, subset=['test_return'])
                st.dataframe(window_df, use_container_width=True, hide_index=True)

                # è¦–è¦ºåŒ–å„è¦–çª—å ±é…¬
                fig_wf = go.Figure()
                fig_wf.add_trace(go.Bar(
                    x=window_df['test_period'],
                    y=window_df['test_return'],
                    marker_color=['green' if r > 0 else 'red' for r in window_df['test_return']],
                    name='æ¸¬è©¦æœŸå ±é…¬'
                ))
                fig_wf.update_layout(
                    title="å„è¦–çª—æ¸¬è©¦æœŸå ±é…¬ç‡",
                    xaxis_title="æ¸¬è©¦æœŸé–“",
                    yaxis_title="å ±é…¬ç‡ (%)",
                    height=350
                )
                st.plotly_chart(fig_wf, use_container_width=True)


def render_watchlist_page():
    st.title("ğŸ“‹ è‚¡ç¥¨è¿½è¹¤æ¸…å–®")

    # æª¢æŸ¥é‡‘èè³‡æ–™åº«æ˜¯å¦å­˜åœ¨
    if not FINANCE_DB_PATH.exists():
        st.error("æ‰¾ä¸åˆ°é‡‘èè³‡æ–™åº« (finance.db)")
        return

    # å–å¾—è¿½è¹¤æ¸…å–®
    watchlist = get_watchlist()
    if not watchlist:
        st.warning("è¿½è¹¤æ¸…å–®ç‚ºç©º")
        return

    # ä¾å¸‚å ´åˆ†çµ„
    us_stocks = [s for s in watchlist if s.get("market") == "US"]
    tw_stocks = [s for s in watchlist if s.get("market") == "TW"]
    etf_stocks = [s for s in watchlist if s.get("market") == "ETF"]
    index_stocks = [s for s in watchlist if s.get("market") == "INDEX"]
    other_stocks = [s for s in watchlist if s.get("market") not in ["US", "TW", "ETF", "INDEX"]]

    # çµ±è¨ˆ
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("ç¸½è‚¡ç¥¨æ•¸", len(watchlist))
    with col2:
        st.metric("ç¾è‚¡", len(us_stocks))
    with col3:
        st.metric("å°è‚¡", len(tw_stocks))
    with col4:
        st.metric("ETF/æŒ‡æ•¸", len(etf_stocks) + len(index_stocks))

    st.divider()

    # åˆ†é é¡¯ç¤º
    tab1, tab2, tab3, tab4 = st.tabs([
        f"ğŸ‡ºğŸ‡¸ ç¾è‚¡ ({len(us_stocks)})",
        f"ğŸ‡¹ğŸ‡¼ å°è‚¡ ({len(tw_stocks)})",
        f"ğŸ“Š ETF ({len(etf_stocks)})",
        f"ğŸ“ˆ æŒ‡æ•¸ ({len(index_stocks)})"
    ])

    def render_stock_table(stocks: list, show_market: bool = False):
        """æ¸²æŸ“è‚¡ç¥¨è¡¨æ ¼"""
        if not stocks:
            st.info("ç›®å‰æ²’æœ‰è‚¡ç¥¨")
            return

        # ä¾ç”¢æ¥­åˆ†çµ„
        by_sector = {}
        for s in stocks:
            sec = s.get("sector") or "æœªåˆ†é¡"
            if sec not in by_sector:
                by_sector[sec] = []
            by_sector[sec].append(s)

        # ç”¢æ¥­ç¯©é¸
        sectors = ["å…¨éƒ¨"] + sorted(by_sector.keys())
        sector_filter = st.selectbox("ç”¢æ¥­ç¯©é¸", sectors, key=f"sector_{id(stocks)}")

        if sector_filter != "å…¨éƒ¨":
            by_sector = {sector_filter: by_sector.get(sector_filter, [])}

        for sector, sector_stocks in sorted(by_sector.items()):
            with st.expander(f"**{sector}** ({len(sector_stocks)} æª”)", expanded=True):
                table_data = []
                for s in sector_stocks:
                    row = {
                        "ä»£ç¢¼": s["symbol"],
                        "åç¨±": s.get("name") or s["symbol"],
                        "ç´°åˆ†ç”¢æ¥­": s.get("industry") or "-",
                    }
                    if show_market:
                        row["å¸‚å ´"] = s.get("market") or "-"
                    table_data.append(row)

                df = pd.DataFrame(table_data)
                st.dataframe(df, use_container_width=True, hide_index=True)

    # Tab 1: ç¾è‚¡
    with tab1:
        st.subheader("ğŸ‡ºğŸ‡¸ ç¾è‚¡æ¸…å–®")
        render_stock_table(us_stocks)

    # Tab 2: å°è‚¡
    with tab2:
        st.subheader("ğŸ‡¹ğŸ‡¼ å°è‚¡æ¸…å–®")
        render_stock_table(tw_stocks)

    # Tab 3: ETF
    with tab3:
        st.subheader("ğŸ“Š ETF æ¸…å–®")
        if etf_stocks:
            table_data = [{
                "ä»£ç¢¼": s["symbol"],
                "åç¨±": s.get("name") or s["symbol"],
                "èªªæ˜": s.get("description") or "-"
            } for s in etf_stocks]
            st.dataframe(pd.DataFrame(table_data), use_container_width=True, hide_index=True)
        else:
            st.info("ç›®å‰æ²’æœ‰ ETF")

    # Tab 4: æŒ‡æ•¸
    with tab4:
        st.subheader("ğŸ“ˆ æŒ‡æ•¸æ¸…å–®")
        if index_stocks:
            table_data = [{
                "ä»£ç¢¼": s["symbol"],
                "åç¨±": s.get("name") or s["symbol"],
            } for s in index_stocks]
            st.dataframe(pd.DataFrame(table_data), use_container_width=True, hide_index=True)
        else:
            st.info("ç›®å‰æ²’æœ‰æŒ‡æ•¸")

    # ç”¢æ¥­åˆ†ä½ˆåœ–
    st.divider()
    st.subheader("ğŸ“Š ç”¢æ¥­åˆ†ä½ˆ")

    col_chart1, col_chart2 = st.columns(2)

    with col_chart1:
        st.markdown("**ä¾å¸‚å ´**")
        market_df = pd.DataFrame([
            {"å¸‚å ´": k, "æ•¸é‡": v} for k, v in sorted(markets.items(), key=lambda x: -x[1])
        ])
        st.bar_chart(market_df.set_index("å¸‚å ´"))

    with col_chart2:
        st.markdown("**ä¾ç”¢æ¥­**")
        # åªé¡¯ç¤ºå‰10å¤§ç”¢æ¥­
        top_sectors = sorted(sectors.items(), key=lambda x: -x[1])[:10]
        sector_df = pd.DataFrame([
            {"ç”¢æ¥­": k, "æ•¸é‡": v} for k, v in top_sectors
        ])
        st.bar_chart(sector_df.set_index("ç”¢æ¥­"))


# ========== ç¸½ç¶“åˆ†æé é¢ ==========
def render_macro_analysis_page():
    """ç¸½ç¶“åˆ†æèˆ‡å¸‚å ´é€±æœŸé é¢"""
    st.title("ğŸŒ ç¸½ç¶“åˆ†æèˆ‡å¸‚å ´é€±æœŸ")

    # åˆå§‹åŒ–
    try:
        macro_db = MacroDatabase()
        cycle_analyzer = MarketCycleAnalyzer(db=macro_db)
        strategy_selector = CycleBasedStrategySelector(macro_db=macro_db)
    except Exception as e:
        st.error(f"åˆå§‹åŒ–å¤±æ•—: {e}")
        st.info("è«‹å…ˆåŸ·è¡Œ `python macro_scheduler.py --full` æ”¶é›†ç¸½ç¶“æ•¸æ“š")
        return

    # å–å¾—ç•¶å‰é€±æœŸ
    try:
        current_cycle = cycle_analyzer.get_current_cycle()
        current_strategy = strategy_selector.get_current_strategy()
    except Exception as e:
        st.warning(f"ç„¡æ³•å–å¾—é€±æœŸåˆ†æ: {e}")
        st.info("è«‹å…ˆåŸ·è¡Œ `python macro_scheduler.py --full` æ”¶é›†ç¸½ç¶“æ•¸æ“š")
        current_cycle = None
        current_strategy = None

    # é ‚éƒ¨é€±æœŸç‡ˆè™Ÿ
    if current_cycle:
        col1, col2, col3, col4 = st.columns(4)

        with col1:
            phase_color = current_cycle.get("phase_color", "#888888")
            st.markdown(f"""
            <div style="background-color: {phase_color}; padding: 20px; border-radius: 10px; text-align: center;">
                <h2 style="color: white; margin: 0;">{current_cycle.get('phase_emoji', '')} {current_cycle.get('phase_name', current_cycle['phase'])}</h2>
                <p style="color: white; margin: 5px 0 0 0;">ç•¶å‰å¸‚å ´é€±æœŸ</p>
            </div>
            """, unsafe_allow_html=True)

        with col2:
            score = current_cycle.get("score", 0)
            score_color = "#00C851" if score > 0 else "#ff4444" if score < 0 else "#ffbb33"
            st.metric("é€±æœŸåˆ†æ•¸", f"{score:.2f}", delta=None)
            st.progress((score + 1) / 2)  # è½‰æ› -1~1 åˆ° 0~1

        with col3:
            confidence = current_cycle.get("confidence", 0)
            st.metric("åˆ¤æ–·ä¿¡å¿ƒåº¦", f"{confidence:.0%}")

        with col4:
            if current_strategy:
                st.metric("å»ºè­°ç­–ç•¥", current_strategy["strategy"]["name"])

    st.divider()

    # åˆ†é 
    tab1, tab2, tab3, tab4, tab5 = st.tabs(["ğŸ“Š å¸‚å ´é€±æœŸ", "ğŸ“ˆ ç¸½ç¶“æŒ‡æ¨™", "ğŸ“‰ æ­·å²è¶¨å‹¢", "ğŸ’¡ ç­–ç•¥å»ºè­°", "ğŸ”¬ ç­–ç•¥å›æ¸¬"])

    # Tab 1: å¸‚å ´é€±æœŸ
    with tab1:
        render_macro_cycle_tab(current_cycle, macro_db)

    # Tab 2: ç¸½ç¶“æŒ‡æ¨™
    with tab2:
        render_macro_indicators_tab(macro_db)

    # Tab 3: æ­·å²è¶¨å‹¢
    with tab3:
        render_macro_history_tab(macro_db)

    # Tab 4: ç­–ç•¥å»ºè­°
    with tab4:
        render_macro_strategy_tab(current_strategy, strategy_selector)

    # Tab 5: ç­–ç•¥å›æ¸¬
    with tab5:
        render_backtest_tab(macro_db)


def render_macro_cycle_tab(current_cycle, macro_db):
    """å¸‚å ´é€±æœŸåˆ†é """
    if not current_cycle:
        st.warning("å°šç„¡é€±æœŸåˆ†æè³‡æ–™")
        return

    st.subheader("é€±æœŸéšæ®µèªªæ˜")
    st.markdown(f"**{current_cycle.get('phase_description', '')}**")

    st.divider()
    st.subheader("å„ç¶­åº¦åˆ†æ")

    signals = current_cycle.get("signals", {})
    weights = current_cycle.get("weights", {})

    # é¡¯ç¤ºå„ç¶­åº¦åˆ†æçµæœ
    dimension_names = {
        "yield_curve": "æ®–åˆ©ç‡æ›²ç·š",
        "employment": "å°±æ¥­å¸‚å ´",
        "growth": "ç¶“æ¿Ÿæˆé•·",
        "inflation": "é€šè²¨è†¨è„¹",
        "sentiment": "å¸‚å ´æƒ…ç·’"
    }

    cols = st.columns(len(signals))
    for i, (dim, data) in enumerate(signals.items()):
        with cols[i]:
            dim_name = dimension_names.get(dim, dim)
            score = data.get("score", 0)
            signal = data.get("signal", "N/A")
            weight = weights.get(dim, 0)

            # é¡è‰²
            if score > 0.3:
                color = "#00C851"
            elif score > 0:
                color = "#8BC34A"
            elif score > -0.3:
                color = "#ffbb33"
            else:
                color = "#ff4444"

            st.markdown(f"""
            <div style="background-color: {color}20; border-left: 4px solid {color}; padding: 15px; border-radius: 5px;">
                <h4 style="margin: 0;">{dim_name}</h4>
                <p style="margin: 5px 0; font-size: 24px; font-weight: bold;">{score:.2f}</p>
                <p style="margin: 0; font-size: 12px;">ä¿¡è™Ÿ: {signal}</p>
                <p style="margin: 0; font-size: 12px;">æ¬Šé‡: {weight:.0%}</p>
            </div>
            """, unsafe_allow_html=True)

            # é¡¯ç¤ºè©³ç´°è³‡æ–™
            details = data.get("details", {})
            if isinstance(details, dict):
                with st.expander("è©³ç´°æ•¸æ“š"):
                    for key, value in details.items():
                        if value is not None:
                            if isinstance(value, float):
                                st.write(f"**{key}**: {value:.2f}")
                            else:
                                st.write(f"**{key}**: {value}")

    # é€±æœŸæ­·å²
    st.divider()
    st.subheader("é€±æœŸæ­·å²è¨˜éŒ„")

    # å¹´ä»½é¸æ“‡å™¨
    cycle_col1, cycle_col2 = st.columns(2)
    with cycle_col1:
        cycle_start_year = st.selectbox("èµ·å§‹å¹´ä»½", [2021, 2022, 2023, 2024, 2025], index=0, key="cycle_start")
    with cycle_col2:
        cycle_end_year = st.selectbox("çµæŸå¹´ä»½", [2021, 2022, 2023, 2024, 2025, 2026], index=5, key="cycle_end")

    if cycle_start_year > cycle_end_year:
        st.error("èµ·å§‹å¹´ä»½ä¸èƒ½å¤§æ–¼çµæŸå¹´ä»½")
    else:
        try:
            backtester = CycleBacktester(macro_db=macro_db)
            start_date = date(cycle_start_year, 1, 1)
            end_date = date(cycle_end_year, 12, 31) if cycle_end_year < 2026 else date.today()

            cycles = backtester.get_historical_cycles(start_date, end_date)

            if cycles:
                history_df = pd.DataFrame(cycles)
                history_df["date"] = pd.to_datetime(history_df["date"])

                # é€±æœŸåˆ†æ•¸èµ°å‹¢åœ– (å¸¶é¡è‰²æ¨™è¨˜é€±æœŸ)
                fig = go.Figure()

                # æ ¹æ“šé€±æœŸä¸Šè‰²
                phase_colors = {
                    "EXPANSION": "#00C851",
                    "PEAK": "#ffbb33",
                    "CONTRACTION": "#ff4444",
                    "TROUGH": "#33b5e5"
                }

                from config.macro_indicators import MARKET_CYCLES
                for phase in phase_colors.keys():
                    phase_data = history_df[history_df["phase"] == phase]
                    if not phase_data.empty:
                        phase_info = MARKET_CYCLES.get(phase, {})
                        phase_name = phase_info.get("name", phase)

                        fig.add_trace(go.Scatter(
                            x=phase_data["date"],
                            y=phase_data["score"],
                            mode="markers",
                            name=f"{phase_info.get('emoji', '')} {phase_name}",
                            marker=dict(color=phase_colors[phase], size=8)
                        ))

                # åŠ å…¥è¶¨å‹¢ç·š
                fig.add_trace(go.Scatter(
                    x=history_df["date"],
                    y=history_df["score"],
                    mode="lines",
                    name="åˆ†æ•¸è¶¨å‹¢",
                    line=dict(color="rgba(100,100,100,0.3)", width=1),
                    showlegend=False
                ))

                fig.add_hline(y=0, line_dash="dash", line_color="gray", annotation_text="ä¸­æ€§")
                fig.update_layout(
                    title=f"å¸‚å ´é€±æœŸåˆ†æ•¸èµ°å‹¢ ({cycle_start_year}-{cycle_end_year})",
                    xaxis_title="æ—¥æœŸ",
                    yaxis_title="é€±æœŸåˆ†æ•¸",
                    yaxis_range=[-0.5, 0.5],
                    height=450,
                    legend=dict(orientation="h", yanchor="bottom", y=1.02)
                )
                st.plotly_chart(fig, use_container_width=True)

                # é€±æœŸçµ±è¨ˆè¡¨
                st.markdown("#### é€±æœŸåˆ†ä½ˆçµ±è¨ˆ")
                phase_counts = history_df["phase"].value_counts()
                total = len(history_df)

                col1, col2, col3, col4 = st.columns(4)
                cols = [col1, col2, col3, col4]
                for i, phase in enumerate(["EXPANSION", "PEAK", "CONTRACTION", "TROUGH"]):
                    with cols[i]:
                        count = phase_counts.get(phase, 0)
                        pct = count / total * 100 if total > 0 else 0
                        phase_info = MARKET_CYCLES.get(phase, {})
                        st.metric(
                            f"{phase_info.get('emoji', '')} {phase_info.get('name', phase)}",
                            f"{count} å€‹æœˆ",
                            f"{pct:.1f}%"
                        )
            else:
                st.info("å°šç„¡æ­·å²è¨˜éŒ„")
        except Exception as e:
            st.warning(f"ç„¡æ³•è¼‰å…¥é€±æœŸæ­·å²: {e}")


def render_macro_indicators_tab(macro_db):
    """ç¸½ç¶“æŒ‡æ¨™åˆ†é """
    st.subheader("é—œéµç¸½ç¶“æŒ‡æ¨™")

    # å–å¾—æ‰€æœ‰æœ€æ–°æ•¸æ“š
    all_data = macro_db.get_all_latest_data()

    if not all_data:
        st.warning("å°šç„¡ç¸½ç¶“æ•¸æ“šï¼Œè«‹å…ˆåŸ·è¡Œæ•¸æ“šæ”¶é›†")
        return

    # æŒ‰é¡åˆ¥åˆ†çµ„
    categories = {}
    for series_id, data in all_data.items():
        category = data.get("category", "other")
        if category not in categories:
            categories[category] = []
        categories[category].append(data)

    category_names = {
        "yield_curve": "æ®–åˆ©ç‡æ›²ç·š",
        "employment": "å°±æ¥­å¸‚å ´",
        "growth": "ç¶“æ¿Ÿæˆé•·",
        "inflation": "é€šè²¨è†¨è„¹",
        "interest_rate": "åˆ©ç‡æ”¿ç­–",
        "sentiment": "å¸‚å ´æƒ…ç·’"
    }

    # é¡¯ç¤ºå„é¡åˆ¥
    for category, items in categories.items():
        cat_name = category_names.get(category, category)
        st.markdown(f"### {cat_name}")

        cols = st.columns(min(len(items), 3))
        for i, item in enumerate(items):
            with cols[i % 3]:
                value = item.get("value", 0)
                change_pct = item.get("change_pct")
                name = item.get("name", item.get("series_id"))
                unit = item.get("unit", "")

                delta = f"{change_pct:+.2f}%" if change_pct else None
                st.metric(
                    label=name,
                    value=f"{value:.2f}" if isinstance(value, float) else str(value),
                    delta=delta
                )
                st.caption(f"æ›´æ–°: {item.get('date', 'N/A')}")

        st.divider()


def render_macro_history_tab(macro_db):
    """æ­·å²è¶¨å‹¢åˆ†é """
    st.subheader("æŒ‡æ¨™æ­·å²èµ°å‹¢")

    # é¸æ“‡æŒ‡æ¨™
    indicators = macro_db.get_indicators(active_only=True)
    if not indicators:
        st.warning("å°šç„¡æŒ‡æ¨™è³‡æ–™")
        return

    indicator_options = {f"{i['name']} ({i['series_id']})": i['series_id'] for i in indicators}
    selected_names = st.multiselect(
        "é¸æ“‡æŒ‡æ¨™",
        options=list(indicator_options.keys()),
        default=list(indicator_options.keys())[:2]
    )

    # æ™‚é–“ç¯„åœé¸æ“‡
    col1, col2, col3 = st.columns(3)
    with col1:
        start_year = st.selectbox("èµ·å§‹å¹´ä»½", [2021, 2022, 2023, 2024, 2025], index=0)
    with col2:
        end_year = st.selectbox("çµæŸå¹´ä»½", [2021, 2022, 2023, 2024, 2025, 2026], index=5)
    with col3:
        chart_type = st.selectbox("åœ–è¡¨é¡å‹", ["æŠ˜ç·šåœ–", "é¢ç©åœ–"], index=0)

    # è¨ˆç®—æ—¥æœŸç¯„åœ
    start_date = date(start_year, 1, 1)
    end_date = date(end_year, 12, 31) if end_year < 2026 else date.today()

    if start_year > end_year:
        st.error("èµ·å§‹å¹´ä»½ä¸èƒ½å¤§æ–¼çµæŸå¹´ä»½")
        return

    if not selected_names:
        st.info("è«‹é¸æ“‡è‡³å°‘ä¸€å€‹æŒ‡æ¨™")
        return

    # ç¹ªè£½åœ–è¡¨
    fig = go.Figure()

    for name in selected_names:
        series_id = indicator_options[name]
        data = macro_db.get_macro_data(series_id, start_date=start_date, end_date=end_date)

        if data:
            df = pd.DataFrame(data)
            df["date"] = pd.to_datetime(df["date"])
            df = df.sort_values("date")

            if chart_type == "æŠ˜ç·šåœ–":
                fig.add_trace(go.Scatter(
                    x=df["date"],
                    y=df["value"],
                    mode="lines",
                    name=name.split(" (")[0]
                ))
            else:
                fig.add_trace(go.Scatter(
                    x=df["date"],
                    y=df["value"],
                    mode="lines",
                    fill="tozeroy",
                    name=name.split(" (")[0]
                ))

    fig.update_layout(
        title=f"æŒ‡æ¨™èµ°å‹¢æ¯”è¼ƒ ({start_year} - {end_year})",
        xaxis_title="æ—¥æœŸ",
        yaxis_title="æ•¸å€¼",
        height=500,
        legend=dict(orientation="h", yanchor="bottom", y=1.02)
    )
    st.plotly_chart(fig, use_container_width=True)

    # é¡¯ç¤ºçµ±è¨ˆæ‘˜è¦
    if selected_names:
        st.subheader("æœŸé–“çµ±è¨ˆæ‘˜è¦")
        stats_data = []
        for name in selected_names:
            series_id = indicator_options[name]
            data = macro_db.get_macro_data(series_id, start_date=start_date, end_date=end_date)
            if data:
                values = [d["value"] for d in data if d["value"] is not None]
                if values:
                    stats_data.append({
                        "æŒ‡æ¨™": name.split(" (")[0],
                        "èµ·å§‹å€¼": f"{values[-1]:.2f}",
                        "æœ€æ–°å€¼": f"{values[0]:.2f}",
                        "æœ€é«˜": f"{max(values):.2f}",
                        "æœ€ä½": f"{min(values):.2f}",
                        "å¹³å‡": f"{sum(values)/len(values):.2f}",
                        "è®ŠåŒ–": f"{values[0] - values[-1]:+.2f}"
                    })
        if stats_data:
            st.dataframe(pd.DataFrame(stats_data), use_container_width=True, hide_index=True)


def render_macro_strategy_tab(current_strategy, strategy_selector):
    """ç­–ç•¥å»ºè­°åˆ†é  - å¤šç¶­åº¦è©•åˆ†ç³»çµ±"""
    if not current_strategy:
        st.warning("å°šç„¡ç­–ç•¥å»ºè­°")
        return

    strategy = current_strategy.get("strategy", {})
    allocation = current_strategy.get("allocation", {})

    st.subheader(f"ç•¶å‰å»ºè­°: {strategy.get('name', 'N/A')}")
    st.markdown(f"**é¢¨éšªå®¹å¿åº¦**: {strategy.get('risk_tolerance', 'N/A')}")

    col1, col2, col3 = st.columns([1, 1, 1])

    with col1:
        # è³‡ç”¢é…ç½®åœ“é¤…åœ–
        st.markdown("### è³‡ç”¢é…ç½®")
        chart_data = strategy_selector.get_allocation_chart_data()

        fig = go.Figure(data=[go.Pie(
            labels=chart_data["labels"],
            values=chart_data["values"],
            marker_colors=chart_data["colors"],
            hole=0.4
        )])
        fig.update_layout(
            height=280,
            margin=dict(t=0, b=0, l=0, r=0)
        )
        st.plotly_chart(fig, use_container_width=True)

    with col2:
        # æ¿å¡Šåå¥½
        st.markdown("### æ¿å¡Šé…ç½®")

        st.markdown("**åå¥½æ¿å¡Š:**")
        preferred = current_strategy.get("preferred_sectors", [])
        for sector in preferred:
            st.markdown(f"- ğŸŸ¢ {sector}")

        st.markdown("**è¿´é¿æ¿å¡Š:**")
        avoid = current_strategy.get("avoid_sectors", [])
        for sector in avoid:
            st.markdown(f"- ğŸ”´ {sector}")

    with col3:
        # è©•åˆ†æ¬Šé‡èªªæ˜
        st.markdown("### è©•åˆ†æ¬Šé‡")
        st.markdown("""
        å€‹è‚¡æ¨è–¦ä¾ä»¥ä¸‹å„ªå…ˆé †åºè©•åˆ†ï¼š

        1. **é€±æœŸå¥‘åˆåº¦** (30%)
           - æ˜¯å¦ç¬¦åˆç•¶å‰é€±æœŸåå¥½æ¿å¡Š
        2. **ç¨€ç¼ºæ€§/è­·åŸæ²³** (30%)
           - åˆ©æ½¤ç‡ã€ROEã€æ©Ÿæ§‹æŒè‚¡
        3. **æœªä¾†ç™¼å±•æ€§** (25%)
           - PEGã€Forward PEæŠ˜åƒ¹ã€è² å‚µæ¯”
        4. **å‹•èƒ½** (15%)
           - æŠ€è¡“åˆ†æä¿¡è™Ÿ
        """)

    # è‚¡ç¥¨æ¨è–¦
    st.divider()
    st.subheader("å€‹è‚¡æ¨è–¦ (å¤šç¶­åº¦è©•åˆ†)")

    try:
        recommendations = strategy_selector.get_stock_recommendations(limit=10)
        st.caption(f"å…±åˆ†æ {recommendations.get('total_analyzed', 0)} æ”¯è‚¡ç¥¨")

        col1, col2 = st.columns(2)

        with col1:
            st.markdown("#### ğŸ“ˆ è²·é€²æ¨è–¦")
            buy_recs = recommendations.get("buy_recommendations", [])
            if buy_recs:
                for rec in buy_recs[:5]:
                    symbol = rec.get("symbol", "")
                    total_score = rec.get("total_score", 0)
                    sector = rec.get("sector", "N/A")
                    is_preferred = rec.get("in_preferred_sector", False)
                    scores = rec.get("scores", {})

                    pref_badge = "â­" if is_preferred else ""

                    # é¡¯ç¤ºç¸½åˆ†å’Œè‚¡ç¥¨è³‡è¨Š
                    st.markdown(f"**{symbol}** {pref_badge} - ç¸½åˆ†: **{total_score:.2f}**")
                    st.caption(f"æ¿å¡Š: {sector}")

                    # å±•é–‹é¡¯ç¤ºè©³ç´°è©•åˆ†
                    with st.expander(f"æŸ¥çœ‹ {symbol} è©•åˆ†è©³æƒ…"):
                        for dim_name, dim_data in scores.items():
                            dim_labels = {
                                "cycle_fit": "é€±æœŸå¥‘åˆåº¦",
                                "moat": "ç¨€ç¼ºæ€§/è­·åŸæ²³",
                                "growth": "æœªä¾†ç™¼å±•æ€§",
                                "momentum": "å‹•èƒ½"
                            }
                            label = dim_labels.get(dim_name, dim_name)
                            score = dim_data.get("score", 0)
                            weight = dim_data.get("weight", 0)
                            reasons = dim_data.get("reasons", [])

                            # åˆ†æ•¸é¡è‰²
                            if score >= 0.7:
                                color = "green"
                            elif score >= 0.5:
                                color = "orange"
                            else:
                                color = "red"

                            st.markdown(f"**{label}**: :{color}[{score:.2f}] (æ¬Šé‡ {weight:.0%})")
                            for reason in reasons[:2]:
                                st.caption(f"  {reason}")
            else:
                st.info("ç›®å‰ç„¡è²·é€²æ¨è–¦")

        with col2:
            st.markdown("#### ğŸ“‰ è³£å‡ºè­¦ç¤º")
            sell_recs = recommendations.get("sell_recommendations", [])
            if sell_recs:
                for rec in sell_recs[:5]:
                    symbol = rec.get("symbol", "")
                    total_score = rec.get("total_score", 0)
                    sector = rec.get("sector", "N/A")
                    in_avoid = rec.get("in_avoid_sector", False)
                    scores = rec.get("scores", {})

                    avoid_badge = "âš ï¸è¿´é¿æ¿å¡Š" if in_avoid else ""

                    st.markdown(f"**{symbol}** - ç¸½åˆ†: **{total_score:.2f}** {avoid_badge}")
                    st.caption(f"æ¿å¡Š: {sector}")

                    with st.expander(f"æŸ¥çœ‹ {symbol} è©•åˆ†è©³æƒ…"):
                        for dim_name, dim_data in scores.items():
                            dim_labels = {
                                "cycle_fit": "é€±æœŸå¥‘åˆåº¦",
                                "moat": "ç¨€ç¼ºæ€§/è­·åŸæ²³",
                                "growth": "æœªä¾†ç™¼å±•æ€§",
                                "momentum": "å‹•èƒ½"
                            }
                            label = dim_labels.get(dim_name, dim_name)
                            score = dim_data.get("score", 0)
                            reasons = dim_data.get("reasons", [])

                            if score >= 0.7:
                                color = "green"
                            elif score >= 0.5:
                                color = "orange"
                            else:
                                color = "red"

                            st.markdown(f"**{label}**: :{color}[{score:.2f}]")
                            for reason in reasons[:2]:
                                st.caption(f"  {reason}")
            else:
                st.info("ç›®å‰ç„¡è³£å‡ºè­¦ç¤º")

    except Exception as e:
        st.error(f"å–å¾—æ¨è–¦å¤±æ•—: {e}")


def render_backtest_tab(macro_db):
    """ç­–ç•¥å›æ¸¬åˆ†é """
    st.subheader("ğŸ”¬ é€±æœŸç­–ç•¥æ­·å²å›æ¸¬")

    st.markdown("""
    å›æ¸¬èªªæ˜ï¼š
    - æ ¹æ“šæ­·å²ç¸½ç¶“æ•¸æ“šåˆ¤æ–·å¸‚å ´é€±æœŸ
    - ä¾æ“šé€±æœŸç­–ç•¥èª¿æ•´è‚¡ç¥¨é…ç½®
    - åå¥½é€±æœŸç›¸é—œæ¿å¡Šï¼Œé¿é–‹ä¸åˆ©æ¿å¡Š
    """)

    col1, col2, col3 = st.columns(3)

    with col1:
        start_year = st.selectbox("èµ·å§‹å¹´ä»½", [2021, 2022, 2023, 2024, 2025], index=0, key="bt_start")
    with col2:
        end_year = st.selectbox("çµæŸå¹´ä»½", [2021, 2022, 2023, 2024, 2025, 2026], index=5, key="bt_end")
    with col3:
        initial_capital = st.number_input("åˆå§‹è³‡é‡‘", value=100000, step=10000)

    if start_year > end_year:
        st.error("èµ·å§‹å¹´ä»½ä¸èƒ½å¤§æ–¼çµæŸå¹´ä»½")
    elif st.button("åŸ·è¡Œå›æ¸¬", type="primary"):
        with st.spinner("å›æ¸¬é€²è¡Œä¸­..."):
            try:
                backtester = CycleBacktester(macro_db=macro_db)

                start_date = date(start_year, 1, 1)
                end_date = date(end_year, 12, 31) if end_year < 2026 else date.today()

                result = backtester.backtest_strategy(
                    start_date=start_date,
                    end_date=end_date,
                    initial_capital=initial_capital
                )

                if "error" in result:
                    st.error(result["error"])
                    return

                # é¡¯ç¤ºç¸¾æ•ˆæŒ‡æ¨™
                st.divider()
                st.subheader("ğŸ“Š å›æ¸¬ç¸¾æ•ˆ")

                perf = result["performance"]

                col1, col2, col3, col4 = st.columns(4)
                with col1:
                    st.metric("ç¸½å ±é…¬ç‡", f"{perf['total_return_pct']:.1f}%")
                with col2:
                    st.metric("å¹´åŒ–å ±é…¬", f"{perf['annualized_return_pct']:.1f}%")
                with col3:
                    st.metric("æœ€å¤§å›æ’¤", f"{perf['max_drawdown_pct']:.1f}%")
                with col4:
                    st.metric("å¤æ™®æ¯”ç‡", f"{perf['sharpe_ratio']:.2f}")

                col1, col2, col3, col4 = st.columns(4)
                with col1:
                    st.metric("åˆå§‹è³‡é‡‘", f"${perf['initial_capital']:,.0f}")
                with col2:
                    st.metric("æœŸæœ«åƒ¹å€¼", f"${perf['final_value']:,.0f}")
                with col3:
                    st.metric("å‹ç‡", f"{perf['win_rate_pct']:.1f}%")
                with col4:
                    st.metric("äº¤æ˜“æ¬¡æ•¸", perf['total_trades'])

                # èˆ‡åŸºæº–æ¯”è¼ƒ
                benchmark = backtester.compare_with_benchmark(start_date, end_date)
                if "error" not in benchmark:
                    st.divider()
                    st.subheader("ğŸ“Œ èˆ‡åŸºæº–æ¯”è¼ƒ (SPY è²·å…¥æŒæœ‰)")

                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric("ç­–ç•¥å ±é…¬", f"{perf['total_return_pct']:.1f}%")
                    with col2:
                        st.metric("SPY å ±é…¬", f"{benchmark['total_return_pct']:.1f}%")
                    with col3:
                        alpha = perf['total_return_pct'] - benchmark['total_return_pct']
                        st.metric("è¶…é¡å ±é…¬ (Alpha)", f"{alpha:.1f}%",
                                  delta=f"{alpha:.1f}%" if alpha > 0 else None)

                # æ¬Šç›Šæ›²ç·šåœ–
                st.divider()
                st.subheader("ğŸ“ˆ æ¬Šç›Šæ›²ç·š")

                equity_data = result["equity_curve"]
                if equity_data:
                    equity_df = pd.DataFrame(equity_data)
                    equity_df["date"] = pd.to_datetime(equity_df["date"])

                    fig = go.Figure()
                    fig.add_trace(go.Scatter(
                        x=equity_df["date"],
                        y=equity_df["value"],
                        mode="lines+markers",
                        name="ç­–ç•¥æ¬Šç›Š",
                        line=dict(color="#2196F3", width=2)
                    ))

                    # æ¨™è¨˜é€±æœŸ
                    colors = {"EXPANSION": "green", "PEAK": "orange",
                              "CONTRACTION": "red", "TROUGH": "blue"}
                    for _, row in equity_df.iterrows():
                        fig.add_annotation(
                            x=row["date"],
                            y=row["value"],
                            text=row["phase"][:3],
                            showarrow=False,
                            yshift=10,
                            font=dict(size=8, color=colors.get(row["phase"], "gray"))
                        )

                    fig.update_layout(
                        title="ç­–ç•¥æ¬Šç›Šæ›²ç·š",
                        xaxis_title="æ—¥æœŸ",
                        yaxis_title="æ¬Šç›Šåƒ¹å€¼",
                        height=400
                    )
                    st.plotly_chart(fig, use_container_width=True)

                # å„é€±æœŸç¸¾æ•ˆ
                st.divider()
                st.subheader("ğŸ“Š å„é€±æœŸç¸¾æ•ˆ")

                phase_perf = result.get("phase_performance", {})
                if phase_perf:
                    phase_data = []
                    for phase, data in phase_perf.items():
                        from config.macro_indicators import MARKET_CYCLES
                        phase_info = MARKET_CYCLES.get(phase, {})
                        phase_data.append({
                            "é€±æœŸ": f"{phase_info.get('emoji', '')} {phase_info.get('name', phase)}",
                            "æœˆæ•¸": data["months"],
                            "å¹³å‡æœˆå ±é…¬": f"{data['avg_return']:.2f}%",
                            "ç´¯è¨ˆå ±é…¬": f"{data['total_return']:.2f}%"
                        })
                    st.table(pd.DataFrame(phase_data))

                # é€±æœŸè®ŠåŒ–è¨˜éŒ„
                st.divider()
                st.subheader("ğŸ”„ é€±æœŸè®ŠåŒ–è¨˜éŒ„")

                cycle_changes = result.get("cycle_changes", [])
                if cycle_changes:
                    for change in cycle_changes:
                        from_phase = change.get("from_phase") or "åˆå§‹"
                        to_phase = change.get("to_phase")
                        st.markdown(f"**{change['date']}**: {from_phase} â†’ {to_phase} (åˆ†æ•¸: {change['score']:.2f})")

                # æœ€è¿‘äº¤æ˜“è¨˜éŒ„
                st.divider()
                st.subheader("ğŸ“ è¿‘æœŸäº¤æ˜“è¨˜éŒ„")

                trades = result.get("trades", [])
                if trades:
                    trades_df = pd.DataFrame(trades)
                    trades_df = trades_df[["date", "symbol", "action", "shares", "price", "value", "reason"]]
                    trades_df.columns = ["æ—¥æœŸ", "è‚¡ç¥¨", "å‹•ä½œ", "è‚¡æ•¸", "åƒ¹æ ¼", "é‡‘é¡", "ç†ç”±"]
                    st.dataframe(trades_df, use_container_width=True)

            except Exception as e:
                st.error(f"å›æ¸¬å¤±æ•—: {e}")
                import traceback
                st.code(traceback.format_exc())


def render_sentiment_backtest_page():
    """æ¸²æŸ“æƒ…ç·’åˆ†æé é¢ - ç†±é–€è‚¡ç¥¨ã€é—œéµå­—ã€æƒ…ç·’èˆ‡è‚¡åƒ¹ç›¸é—œæ€§"""
    st.title("ğŸ“‰ æ–°èæƒ…ç·’åˆ†æ")
    st.markdown("åˆ†ææ¯æ—¥ç†±é–€è‚¡ç¥¨ã€è¨è«–é—œéµå­—ã€å¤šç©ºæƒ…ç·’ï¼Œä»¥åŠèˆ‡è‚¡åƒ¹çš„ç›¸é—œæ€§")

    analyzer = DailyHotStocksAnalyzer()
    backtester = SentimentBacktester()

    # Tab åˆ†é 
    tab1, tab2, tab3, tab4 = st.tabs([
        "ğŸ”¥ ä»Šæ—¥ç†±é–€è‚¡ç¥¨",
        "ğŸ“Š ç†±é–€é—œéµå­—",
        "ğŸ“ˆ æƒ…ç·’vsè‚¡åƒ¹",
        "ğŸ“‹ ETFå›æ¸¬"
    ])

    # ========== Tab 1: ä»Šæ—¥ç†±é–€è‚¡ç¥¨ ==========
    with tab1:
        st.subheader("ğŸ”¥ ä»Šæ—¥ç†±é–€è¨è«–è‚¡ç¥¨")

        # æ—¥æœŸé¸æ“‡
        col1, col2 = st.columns([1, 3])
        with col1:
            analysis_date = st.date_input(
                "é¸æ“‡æ—¥æœŸ",
                value=date.today() - timedelta(days=1),
                max_value=date.today(),
                key="hot_stocks_date"
            )

        with st.spinner("åˆ†æä¸­..."):
            daily_summary = analyzer.get_daily_summary(analysis_date)

        # æ•´é«”æƒ…ç·’
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("æ–°èç¸½æ•¸", daily_summary["news_count"])
        with col2:
            st.metric("æ­£é¢é—œéµå­—", daily_summary.get("positive_count", 0))
        with col3:
            overall = daily_summary.get("overall_sentiment", "ç„¡æ•¸æ“š")
            st.metric("æ•´é«”æƒ…ç·’", overall)

        st.divider()

        # ç†±é–€è‚¡ç¥¨è¡¨æ ¼
        hot_stocks = daily_summary.get("hot_stocks", [])
        if hot_stocks:
            st.markdown("### ğŸ“‹ è¨è«–ç†±åº¦æ’è¡Œ")

            table_data = []
            for stock in hot_stocks[:15]:
                table_data.append({
                    "æ’å": len(table_data) + 1,
                    "è‚¡ç¥¨": stock["symbol"],
                    "è¨è«–æ¬¡æ•¸": stock["mentions"],
                    "çœ‹å¤š": stock["bullish"],
                    "çœ‹ç©º": stock["bearish"],
                    "æƒ…ç·’": stock["sentiment"],
                    "æƒ…ç·’åˆ†æ•¸": f"{stock['sentiment_score']:.2f}"
                })

            df = pd.DataFrame(table_data)
            st.dataframe(df, use_container_width=True, hide_index=True)

            # è¨è«–ç†±åº¦åœ–
            st.markdown("### ğŸ“Š è¨è«–ç†±åº¦åˆ†ä½ˆ")
            fig = go.Figure()

            symbols = [s["symbol"] for s in hot_stocks[:10]]
            mentions = [s["mentions"] for s in hot_stocks[:10]]
            sentiments = [s["sentiment_score"] for s in hot_stocks[:10]]
            colors = ['green' if s > 0.2 else ('red' if s < -0.2 else 'gray') for s in sentiments]

            fig.add_trace(go.Bar(
                x=symbols,
                y=mentions,
                marker_color=colors,
                text=mentions,
                textposition='outside'
            ))

            fig.update_layout(
                xaxis_title="è‚¡ç¥¨",
                yaxis_title="è¨è«–æ¬¡æ•¸",
                height=400
            )
            st.plotly_chart(fig, use_container_width=True)

            # ç¯„ä¾‹æ–°èæ¨™é¡Œ
            st.markdown("### ğŸ“° ç†±é–€è‚¡ç¥¨ç›¸é—œæ–°è")
            for stock in hot_stocks[:5]:
                if stock.get("sample_titles"):
                    with st.expander(f"**{stock['symbol']}** {stock['sentiment']} ({stock['mentions']} å‰‡)"):
                        for title in stock["sample_titles"]:
                            st.markdown(f"â€¢ {title}")
        else:
            st.info("è©²æ—¥ç„¡è¶³å¤ æ–°èæ•¸æ“š")

        # ä¸€é€±ç†±é–€
        st.divider()
        st.markdown("### ğŸ“… æœ¬é€±ç†±é–€è‚¡ç¥¨ (éå»7å¤©)")

        with st.spinner("åˆ†æä¸­..."):
            weekly_hot = analyzer.get_weekly_hot_stocks(analysis_date, days=7)

        if weekly_hot:
            weekly_data = []
            for stock in weekly_hot[:20]:
                weekly_data.append({
                    "è‚¡ç¥¨": stock["symbol"],
                    "ç¸½è¨è«–æ¬¡æ•¸": stock["total_mentions"],
                    "å‡ºç¾å¤©æ•¸": stock["days_mentioned"],
                    "çœ‹å¤š": stock["bullish"],
                    "çœ‹ç©º": stock["bearish"],
                    "æƒ…ç·’": stock["sentiment"]
                })

            df_weekly = pd.DataFrame(weekly_data)
            st.dataframe(df_weekly, use_container_width=True, hide_index=True)

    # ========== Tab 2: ç†±é–€é—œéµå­— ==========
    with tab2:
        st.subheader("ğŸ“Š ç†±é–€è¨è«–é—œéµå­—")

        col1, col2 = st.columns([1, 3])
        with col1:
            keyword_date = st.date_input(
                "é¸æ“‡æ—¥æœŸ",
                value=date.today() - timedelta(days=1),
                max_value=date.today(),
                key="keywords_date"
            )

        with st.spinner("åˆ†æä¸­..."):
            daily_summary = analyzer.get_daily_summary(keyword_date)

        trending = daily_summary.get("trending_keywords", [])

        if trending:
            # é—œéµå­—è¡¨æ ¼
            kw_data = []
            for kw in trending:
                kw_data.append({
                    "é—œéµå­—": kw["keyword"],
                    "è¨è«–æ¬¡æ•¸": kw["mentions"],
                    "æ­£é¢": kw["bullish"],
                    "è² é¢": kw["bearish"],
                    "æƒ…ç·’": kw["sentiment"]
                })

            df_kw = pd.DataFrame(kw_data)
            st.dataframe(df_kw, use_container_width=True, hide_index=True)

            # é—œéµå­—é›²åœ–ï¼ˆç”¨æŸ±ç‹€åœ–ä»£æ›¿ï¼‰
            st.markdown("### ğŸ“Š é—œéµå­—ç†±åº¦")
            fig = go.Figure()

            keywords = [k["keyword"] for k in trending[:12]]
            counts = [k["mentions"] for k in trending[:12]]
            sentiments = [k["sentiment_score"] for k in trending[:12]]
            colors = ['green' if s > 0.2 else ('red' if s < -0.2 else 'orange') for s in sentiments]

            fig.add_trace(go.Bar(
                y=keywords[::-1],
                x=counts[::-1],
                orientation='h',
                marker_color=colors[::-1],
                text=counts[::-1],
                textposition='outside'
            ))

            fig.update_layout(
                xaxis_title="è¨è«–æ¬¡æ•¸",
                yaxis_title="é—œéµå­—",
                height=500
            )
            st.plotly_chart(fig, use_container_width=True)
        else:
            st.info("è©²æ—¥ç„¡è¶³å¤ æ–°èæ•¸æ“š")

    # ========== Tab 3: æƒ…ç·’ vs è‚¡åƒ¹ç›¸é—œæ€§ ==========
    with tab3:
        st.subheader("ğŸ“ˆ å€‹è‚¡æƒ…ç·’ vs è‚¡åƒ¹ç›¸é—œæ€§åˆ†æ")

        # é¸æ“‡è‚¡ç¥¨
        col1, col2, col3 = st.columns(3)

        with col1:
            # å¾ STOCK_KEYWORDS å–å¾—è‚¡ç¥¨åˆ—è¡¨
            from src.finance.sentiment_backtest import STOCK_KEYWORDS
            stock_options = list(STOCK_KEYWORDS.keys())
            selected_stock = st.selectbox("é¸æ“‡è‚¡ç¥¨", stock_options, index=0)

        with col2:
            corr_days = st.selectbox(
                "åˆ†ææœŸé–“",
                [30, 60, 90, 180],
                index=2,
                format_func=lambda x: f"{x} å¤©"
            )

        with col3:
            lead_days = st.selectbox(
                "é ˜å…ˆå¤©æ•¸",
                [1, 2, 3, 5],
                index=0,
                help="æƒ…ç·’é ˜å…ˆè‚¡åƒ¹å¤šå°‘å¤©",
                key="stock_lead_days"
            )

        if st.button("ğŸ” åˆ†æç›¸é—œæ€§", type="primary"):
            with st.spinner(f"åˆ†æ {selected_stock} æƒ…ç·’èˆ‡è‚¡åƒ¹ç›¸é—œæ€§..."):
                # è¨ˆç®—è©²è‚¡ç¥¨çš„æ¯æ—¥æƒ…ç·’
                end_date = date.today()
                start_date = end_date - timedelta(days=corr_days)

                # å–å¾—è‚¡ç¥¨åƒ¹æ ¼
                conn = sqlite3.connect("finance.db")
                price_query = """
                    SELECT date, close
                    FROM daily_prices
                    WHERE symbol = ?
                    AND date BETWEEN ? AND ?
                    ORDER BY date
                """
                price_df = pd.read_sql_query(price_query, conn, params=(
                    selected_stock,
                    start_date.strftime('%Y-%m-%d'),
                    end_date.strftime('%Y-%m-%d')
                ))
                conn.close()

                if price_df.empty:
                    st.warning(f"ç„¡æ³•å–å¾— {selected_stock} çš„åƒ¹æ ¼æ•¸æ“š")
                else:
                    price_df['date'] = pd.to_datetime(price_df['date'])
                    price_df['return_1d'] = price_df['close'].pct_change(1) * 100

                    # è¨ˆç®—è©²è‚¡ç¥¨çš„æ¯æ—¥æƒ…ç·’
                    news_conn = sqlite3.connect("news.db")
                    keywords = STOCK_KEYWORDS.get(selected_stock, [])
                    keyword_conditions = " OR ".join([
                        f"LOWER(title || ' ' || COALESCE(content, '')) LIKE '%{kw.lower()}%'"
                        for kw in keywords
                    ])

                    sentiment_query = f"""
                        SELECT
                            DATE(COALESCE(
                                CASE WHEN source_type = 'ptt' THEN published_at ELSE collected_at END,
                                collected_at
                            )) as news_date,
                            title,
                            content
                        FROM news
                        WHERE DATE(COALESCE(
                            CASE WHEN source_type = 'ptt' THEN published_at ELSE collected_at END,
                            collected_at
                        )) BETWEEN ? AND ?
                        AND ({keyword_conditions})
                    """

                    news_df = pd.read_sql_query(sentiment_query, news_conn, params=(
                        start_date.strftime('%Y-%m-%d'),
                        end_date.strftime('%Y-%m-%d')
                    ))
                    news_conn.close()

                    if news_df.empty:
                        st.warning(f"ç„¡æ³•å–å¾— {selected_stock} çš„æ–°èæ•¸æ“š")
                    else:
                        # è¨ˆç®—æ¯æ—¥æƒ…ç·’
                        from src.finance.sentiment_backtest import POSITIVE_KEYWORDS, NEGATIVE_KEYWORDS

                        daily_sentiment = []
                        for news_date, group in news_df.groupby('news_date'):
                            text_all = " ".join([
                                (str(row['title']) + " " + str(row['content'] or "")).lower()
                                for _, row in group.iterrows()
                            ])
                            pos = sum(1 for kw in POSITIVE_KEYWORDS if kw in text_all)
                            neg = sum(1 for kw in NEGATIVE_KEYWORDS if kw in text_all)
                            total = pos + neg
                            score = (pos - neg) / total if total > 0 else 0

                            daily_sentiment.append({
                                'date': news_date,
                                'mentions': len(group),
                                'sentiment_score': score,
                                'bullish': pos,
                                'bearish': neg
                            })

                        sentiment_df = pd.DataFrame(daily_sentiment)
                        sentiment_df['date'] = pd.to_datetime(sentiment_df['date'])

                        # åˆä½µæ•¸æ“š
                        merged = pd.merge(sentiment_df, price_df, on='date', how='inner')

                        if len(merged) < 10:
                            st.warning("æ•¸æ“šé»ä¸è¶³ï¼Œç„¡æ³•é€²è¡Œæœ‰æ•ˆåˆ†æ")
                        else:
                            # è¨ˆç®—ç›¸é—œæ€§
                            merged['sentiment_lagged'] = merged['sentiment_score'].shift(lead_days)
                            merged['mentions_lagged'] = merged['mentions'].shift(lead_days)
                            analysis_df = merged.dropna()

                            if len(analysis_df) > 5:
                                corr_sentiment = analysis_df['sentiment_lagged'].corr(analysis_df['return_1d'])
                                corr_mentions = analysis_df['mentions_lagged'].corr(analysis_df['return_1d'])

                                # é¡¯ç¤ºçµæœ
                                st.success(f"âœ… åˆ†æå®Œæˆï¼å…± {len(analysis_df)} å€‹æ•¸æ“šé»")

                                col1, col2, col3 = st.columns(3)
                                with col1:
                                    st.metric(
                                        "æƒ…ç·’-å ±é…¬ç›¸é—œæ€§",
                                        f"{corr_sentiment:.4f}",
                                        help="æ­£å€¼è¡¨ç¤ºæƒ…ç·’æ­£é¢æ™‚è‚¡åƒ¹å‚¾å‘ä¸Šæ¼²"
                                    )
                                with col2:
                                    st.metric(
                                        "è¨è«–é‡-å ±é…¬ç›¸é—œæ€§",
                                        f"{corr_mentions:.4f}",
                                        help="æ­£å€¼è¡¨ç¤ºè¨è«–å¢åŠ æ™‚è‚¡åƒ¹å‚¾å‘ä¸Šæ¼²"
                                    )
                                with col3:
                                    avg_mentions = analysis_df['mentions'].mean()
                                    st.metric("å¹³å‡æ¯æ—¥è¨è«–", f"{avg_mentions:.1f} å‰‡")

                                # çµè«–
                                st.divider()
                                st.markdown("### ğŸ“ åˆ†æçµè«–")

                                if abs(corr_sentiment) > 0.15:
                                    st.success(f"âœ… {selected_stock} çš„æ–°èæƒ…ç·’èˆ‡è‚¡åƒ¹æœ‰è¼ƒå¼·ç›¸é—œæ€§ ({corr_sentiment:.3f})")
                                elif abs(corr_sentiment) > 0.08:
                                    st.warning(f"âš ï¸ {selected_stock} çš„æ–°èæƒ…ç·’èˆ‡è‚¡åƒ¹æœ‰å¼±ç›¸é—œæ€§ ({corr_sentiment:.3f})")
                                else:
                                    st.info(f"â„¹ï¸ {selected_stock} çš„æ–°èæƒ…ç·’èˆ‡è‚¡åƒ¹å¹¾ä¹ç„¡ç›¸é—œ ({corr_sentiment:.3f})")

                                if corr_mentions > 0.1:
                                    st.info("ğŸ’¡ è¨è«–é‡å¢åŠ æ™‚ï¼Œè‚¡åƒ¹å‚¾å‘ä¸Šæ¼²")
                                elif corr_mentions < -0.1:
                                    st.info("ğŸ’¡ è¨è«–é‡å¢åŠ æ™‚ï¼Œè‚¡åƒ¹å‚¾å‘ä¸‹è·Œï¼ˆå¯èƒ½æ˜¯åˆ©ç©ºæ¶ˆæ¯ï¼‰")

                                # èµ°å‹¢åœ–
                                st.divider()
                                st.markdown("### ğŸ“Š æƒ…ç·’ vs è‚¡åƒ¹èµ°å‹¢")

                                fig = make_subplots(
                                    rows=3, cols=1,
                                    shared_xaxes=True,
                                    vertical_spacing=0.08,
                                    row_heights=[0.4, 0.3, 0.3],
                                    subplot_titles=(f"{selected_stock} è‚¡åƒ¹", "æ–°èæƒ…ç·’", "è¨è«–æ¬¡æ•¸")
                                )

                                # è‚¡åƒ¹
                                fig.add_trace(
                                    go.Scatter(x=merged['date'], y=merged['close'],
                                              name="è‚¡åƒ¹", line=dict(color='#1f77b4', width=2)),
                                    row=1, col=1
                                )

                                # æƒ…ç·’
                                colors = ['green' if s > 0 else 'red' for s in merged['sentiment_score']]
                                fig.add_trace(
                                    go.Bar(x=merged['date'], y=merged['sentiment_score'],
                                          name="æƒ…ç·’", marker_color=colors, opacity=0.7),
                                    row=2, col=1
                                )
                                fig.add_hline(y=0, line_dash="dash", line_color="gray", row=2, col=1)

                                # è¨è«–é‡
                                fig.add_trace(
                                    go.Bar(x=merged['date'], y=merged['mentions'],
                                          name="è¨è«–æ¬¡æ•¸", marker_color='orange', opacity=0.7),
                                    row=3, col=1
                                )

                                fig.update_layout(height=700, showlegend=False)
                                st.plotly_chart(fig, use_container_width=True)

    # ========== Tab 4: ETF å›æ¸¬ ==========
    with tab4:
        st.subheader("ğŸ“‹ æ•´é«”å¸‚å ´æƒ…ç·’ vs ETF å›æ¸¬")

        col1, col2 = st.columns(2)
        with col1:
            lookback_days = st.selectbox(
                "å›æ¸¬æœŸé–“",
                [30, 90, 180, 365],
                index=2,
                format_func=lambda x: f"{x} å¤©",
                key="etf_lookback"
            )
        with col2:
            etf_options = ["SPY", "QQQ", "DIA", "IWM", "VGT", "XLF", "XLE", "XLV"]
            selected_etf = st.selectbox("é¸æ“‡ ETF", etf_options, index=0, key="etf_select")

        end_date = date.today()
        start_date = end_date - timedelta(days=lookback_days)

        with st.spinner("åŸ·è¡Œå›æ¸¬..."):
            result = backtester.run_backtest(
                etf_symbol=selected_etf,
                start_date=start_date,
                end_date=end_date,
                lead_days=1
            )

        if "error" in result:
            st.error(result["error"])
        else:
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("ç›¸é—œä¿‚æ•¸", f"{result['correlation']:.4f}")
            with col2:
                st.metric("æ•´é«”å‹ç‡", f"{result['win_rate']['overall']:.1f}%")
            with col3:
                st.metric("æƒ…ç·’æ­£â†’æ¼²", f"{result['win_rate']['positive_sentiment_up']:.1f}%")
            with col4:
                st.metric("æƒ…ç·’è² â†’è·Œ", f"{result['win_rate']['negative_sentiment_down']:.1f}%")

            # å¤šETFæ¯”è¼ƒ
            st.divider()
            st.markdown("### ğŸ“Š å¤š ETF æ¯”è¼ƒ")

            with st.spinner("æ¯”è¼ƒä¸­..."):
                results = backtester.run_multi_etf_backtest(
                    etf_symbols=etf_options,
                    start_date=start_date,
                    end_date=end_date
                )

            if results:
                comparison_data = [{
                    "ETF": r["etf_symbol"],
                    "ç›¸é—œä¿‚æ•¸": f"{r['correlation']:.4f}",
                    "å‹ç‡": f"{r['win_rate']['overall']:.1f}%",
                    "æ­£â†’æ¼²": f"{r['win_rate']['positive_sentiment_up']:.1f}%"
                } for r in results]

                st.dataframe(pd.DataFrame(comparison_data), use_container_width=True, hide_index=True)


# ========== å´é‚Šæ¬„ ==========
st.sidebar.title("ğŸ“ˆ è‚¡ç¥¨èˆ‡æ–°èåˆ†æ")
st.sidebar.markdown("---")

# Supabase æ¨¡å¼æç¤º
if USE_SUPABASE:
    st.sidebar.success("â˜ï¸ **Supabase é›²ç«¯è³‡æ–™åº«**")
elif DEMO_MODE:
    st.sidebar.info("ğŸ“Œ **ç¤ºç¯„æ¨¡å¼**\nä½¿ç”¨æœ‰é™çš„ç¤ºç¯„æ•¸æ“š")
    st.toast("æ­£åœ¨ä½¿ç”¨ç¤ºç¯„è³‡æ–™åº«ï¼Œæ•¸æ“šæœ‰é™", icon="â„¹ï¸")

# æª¢æŸ¥è³‡æ–™åº«æ˜¯å¦å­˜åœ¨
db_exists = DB_PATH.exists() or USE_SUPABASE
finance_db_exists = FINANCE_DB_PATH.exists() or USE_SUPABASE

if not db_exists and not finance_db_exists:
    st.error("âš ï¸ è³‡æ–™åº«æª”æ¡ˆä¸å­˜åœ¨")
    st.info("""
    **é€™æ˜¯ä¸€å€‹è‚¡ç¥¨æ–°èåˆ†æç³»çµ±ï¼Œéœ€è¦æœ¬åœ°è³‡æ–™åº«æ‰èƒ½é‹è¡Œã€‚**

    è«‹åœ¨æœ¬åœ°ç’°å¢ƒåŸ·è¡Œä»¥ä¸‹æ­¥é©Ÿï¼š

    1. å®‰è£å¥—ä»¶ï¼š`pip install -r requirements.txt`
    2. åˆå§‹åŒ–æ–°èæ”¶é›†ï¼š`python main.py`
    3. åˆå§‹åŒ–è‚¡ç¥¨æ•¸æ“šï¼š`python finance_collector.py --init --fast`
    4. å•Ÿå‹•æ‡‰ç”¨ï¼š`streamlit run app.py`

    **GitHub**: https://github.com/manibari/news
    """)
    st.stop()

st.sidebar.subheader("ğŸ“… é¸æ“‡æ—¥æœŸ")

# å®‰å…¨å–å¾—å¯ç”¨æ—¥æœŸ
if USE_SUPABASE or db_exists:
    try:
        available_dates = get_available_dates()
    except Exception as e:
        available_dates = []
else:
    available_dates = []

if available_dates:
    min_date = min(available_dates)
    max_date = max(available_dates)

    # åˆå§‹åŒ– session_state ä¸­çš„æ—¥æœŸ
    if "selected_date" not in st.session_state:
        st.session_state.selected_date = max_date

    # å¿«é€Ÿé¸æ“‡æŒ‰éˆ•çš„å›èª¿å‡½æ•¸
    def set_today():
        st.session_state.selected_date = date.today()

    def set_yesterday():
        st.session_state.selected_date = date.today() - timedelta(days=1)

    # æ—¥æœŸé¸æ“‡å™¨ï¼ˆä½¿ç”¨ session_stateï¼‰
    selected_date = st.sidebar.date_input(
        "æ—¥æœŸ",
        value=st.session_state.selected_date,
        min_value=min_date,
        max_value=max_date,
        format="YYYY-MM-DD",
        key="date_picker"
    )
    # åŒæ­¥æ›´æ–° session_state
    st.session_state.selected_date = selected_date

    st.sidebar.markdown("**å¿«é€Ÿé¸æ“‡:**")
    col1, col2 = st.sidebar.columns(2)
    with col1:
        st.button("ä»Šå¤©", use_container_width=True, on_click=set_today)
    with col2:
        st.button("æ˜¨å¤©", use_container_width=True, on_click=set_yesterday)

    # ä½¿ç”¨ session_state çš„å€¼
    selected_date = st.session_state.selected_date

    st.sidebar.markdown(f"*è³‡æ–™ç¯„åœ: {min_date} ~ {max_date}*")
else:
    st.sidebar.warning("è³‡æ–™åº«ä¸­æ²’æœ‰æ–°è")
    selected_date = date.today()

    # é¡¯ç¤ºå¦‚ä½•æ”¶é›†æ–°èçš„èªªæ˜
    st.warning("âš ï¸ è³‡æ–™åº«ä¸­æ²’æœ‰æ–°èæ•¸æ“š")
    st.info("""
    **è«‹å…ˆåŸ·è¡Œæ–°èæ”¶é›†ï¼š**

    ```bash
    # æ”¶é›†ä»Šæ—¥æ–°è
    python main.py

    # æ”¶é›† PTT æ­·å²æ–‡ç«  (éå»ä¸€å¹´)
    python collect_ptt_historical.py --pages 500

    # æ”¶é›†è‚¡ç¥¨æ•¸æ“š
    python collect_stock_historical.py
    ```

    æ”¶é›†å®Œæˆå¾Œé‡æ–°æ•´ç†é é¢å³å¯ã€‚
    """)

st.sidebar.markdown("---")

# ========== æ–°èç¯©é¸è¨­å®š ==========
st.sidebar.subheader("ğŸ” æ–°èç¯©é¸")

# PTT æœ€ä½æ¨æ–‡æ•¸
if "ptt_min_push" not in st.session_state:
    st.session_state.ptt_min_push = 30

ptt_min_push = st.sidebar.slider(
    "PTT æœ€ä½æ¨æ–‡æ•¸",
    min_value=0,
    max_value=100,
    value=st.session_state.ptt_min_push,
    step=10,
    help="åªé¡¯ç¤ºæ¨æ–‡æ•¸ >= æ­¤å€¼çš„ PTT æ–‡ç« "
)
st.session_state.ptt_min_push = ptt_min_push

# æ’é™¤ç¤¾è«–
if "exclude_editorial" not in st.session_state:
    st.session_state.exclude_editorial = True

exclude_editorial = st.sidebar.checkbox(
    "æ’é™¤ç¤¾è«–/è©•è«–",
    value=st.session_state.exclude_editorial,
    help="éæ¿¾æ‰å€‹äººè©•è«–ã€ç¤¾è«–ã€å°ˆæ¬„é¡æ–‡ç« "
)
st.session_state.exclude_editorial = exclude_editorial

st.sidebar.markdown("---")

# å¸‚å ´é€±æœŸç‡ˆè™Ÿ
try:
    _macro_db = MacroDatabase()
    _latest_cycle = _macro_db.get_latest_market_cycle()
    if _latest_cycle:
        from config.macro_indicators import MARKET_CYCLES
        _phase = _latest_cycle.get("phase", "")
        _phase_info = MARKET_CYCLES.get(_phase, {})
        _phase_name = _phase_info.get("name", _phase)
        _phase_emoji = _phase_info.get("emoji", "")
        _phase_color = _phase_info.get("color", "#888888")
        st.sidebar.markdown("**å¸‚å ´é€±æœŸ:**")
        st.sidebar.markdown(f"""
        <div style="background-color: {_phase_color}; padding: 10px; border-radius: 8px; text-align: center; margin-bottom: 10px;">
            <span style="color: white; font-weight: bold;">{_phase_emoji} {_phase_name}</span>
        </div>
        """, unsafe_allow_html=True)
except:
    pass

st.sidebar.markdown("---")

# ç‡ˆè™Ÿèªªæ˜
st.sidebar.markdown("**ç‡ˆè™Ÿèªªæ˜:**")
st.sidebar.markdown("ğŸŸ¢ æ­£é¢è¶¨å‹¢")
st.sidebar.markdown("ğŸŸ¡ ä¸­æ€§/è§€æœ›")
st.sidebar.markdown("ğŸ”´ è² é¢è¶¨å‹¢")

st.sidebar.markdown("---")

page = st.sidebar.radio(
    "é¸æ“‡é é¢",
    ["ğŸ“Š æ–°èç¸½çµ", "ğŸ¯ è¶¨å‹¢é›·é”", "ğŸ’° å­£åº¦å›æ¸¬", "ğŸ”¬ å€‹è‚¡åˆ†æ", "ğŸ“ˆ è‚¡ç¥¨æ•¸æ“š", "ğŸ¯ äº¤æ˜“åˆ†æ", "ğŸŒ ç¸½ç¶“åˆ†æ", "ğŸ“‰ æƒ…ç·’å›æ¸¬", "ğŸ“‹ è‚¡ç¥¨æ¸…å–®", "ğŸ“° æ–°èåˆ—è¡¨", "ğŸ‡¹ğŸ‡¼ PTT Stock"],
    index=0
)

st.sidebar.markdown("---")
st.sidebar.markdown(f"**æ›´æ–°æ™‚é–“**: {datetime.now().strftime('%H:%M:%S')}")

if st.sidebar.button("ğŸ”„ é‡æ–°æ•´ç†", use_container_width=True):
    st.cache_resource.clear()
    st.rerun()

# ========== é é¢è·¯ç”± ==========
if page == "ğŸ“Š æ–°èç¸½çµ":
    render_summary_page(selected_date)
elif page == "ğŸ¯ è¶¨å‹¢é›·é”":
    render_trend_radar_page()
elif page == "ğŸ’° å­£åº¦å›æ¸¬":
    render_quarterly_backtest_page()
elif page == "ğŸ”¬ å€‹è‚¡åˆ†æ":
    render_individual_stock_page(selected_date)
elif page == "ğŸ“ˆ è‚¡ç¥¨æ•¸æ“š":
    render_stock_page(selected_date)
elif page == "ğŸ¯ äº¤æ˜“åˆ†æ":
    render_analysis_page()
elif page == "ğŸŒ ç¸½ç¶“åˆ†æ":
    render_macro_analysis_page()
elif page == "ğŸ“‰ æƒ…ç·’å›æ¸¬":
    render_sentiment_backtest_page()
elif page == "ğŸ“‹ è‚¡ç¥¨æ¸…å–®":
    render_watchlist_page()
elif page == "ğŸ“° æ–°èåˆ—è¡¨":
    render_news_list_page(selected_date)
elif page == "ğŸ‡¹ğŸ‡¼ PTT Stock":
    render_ptt_page(selected_date)
