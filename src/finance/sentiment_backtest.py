"""
æ–°èæƒ…ç·’ vs ETF å›æ¸¬åˆ†ææ¨¡çµ„

åˆ†ææ–°èæƒ…ç·’æ˜¯å¦èƒ½ä½œç‚º ETF å¤§ç›¤çš„é ˜å…ˆæŒ‡æ¨™
"""

import sqlite3
import logging
from datetime import datetime, timedelta, date
from typing import Dict, List, Optional, Tuple
import pandas as pd
import numpy as np

logger = logging.getLogger(__name__)

# æ­£é¢é—œéµå­—
POSITIVE_KEYWORDS = [
    "surge", "soar", "jump", "rally", "gain", "rise", "climb", "beat",
    "record high", "all-time high", "bullish", "optimistic", "strong",
    "growth", "profit", "upgrade", "buy", "outperform", "exceed",
    "æ¼²", "é£†", "å‰µæ–°é«˜", "åˆ©å¤š", "çœ‹å¥½", "æˆé•·", "ç²åˆ©", "å„ªæ–¼é æœŸ",
    "çªç ´", "åå½ˆ", "å›å‡", "ç†±çµ¡", "å¼·å‹¢", "æ¨‚è§€"
]

# è² é¢é—œéµå­—
NEGATIVE_KEYWORDS = [
    "plunge", "crash", "tumble", "drop", "fall", "decline", "sink",
    "miss", "cut", "layoff", "bearish", "pessimistic", "weak",
    "loss", "downgrade", "sell", "underperform", "disappoint",
    "recession", "crisis", "fear", "panic", "warning", "risk",
    "è·Œ", "å´©", "æš´è·Œ", "åˆ©ç©º", "çœ‹å£", "è¡°é€€", "è™§æ", "ä¸å¦‚é æœŸ",
    "ä¸‹ä¿®", "ç–²è»Ÿ", "èç¸®", "æ‚²è§€", "è­¦ç¤º", "é¢¨éšª"
]

# ç”¢æ¥­å°æ‡‰çš„ ETF
SECTOR_ETF_MAPPING = {
    "åŠå°é«”": ["SMH", "SOXX", "VGT", "QQQ"],
    "è»Ÿé«”/é›²ç«¯": ["IGV", "WCLD", "VGT", "QQQ"],
    "ç¶²è·¯/ç¤¾ç¾¤": ["SOCL", "QQQ", "VGT"],
    "AIäººå·¥æ™ºæ…§": ["BOTZ", "ROBO", "QQQ", "VGT"],
    "é‡‘è": ["XLF", "KBE", "KRE"],
    "é†«ç™‚ä¿å¥": ["XLV", "IBB", "XBI"],
    "èƒ½æº": ["XLE", "OIH", "XOP"],
    "æ±½è»Š": ["CARZ", "DRIV", "QQQ"],
    "é›¶å”®/æ¶ˆè²»": ["XRT", "XLY", "VCR"],
    "å·¥æ¥­": ["XLI", "IYJ"],
    "å…¬ç”¨äº‹æ¥­": ["XLU", "VPU"],
    "åŸºç¤ææ–™": ["XLB", "VAW"],
    "é‹¼éµ/çŸ³åŒ–/æ°´æ³¥": ["SLX", "XLB"],
    "é€šè¨Šæœå‹™": ["XLC", "VOX"],
    "æˆ¿åœ°ç”¢": ["VNQ", "IYR", "XLRE"],
    "åŠ å¯†è²¨å¹£": ["BITO", "GBTC"],
    # å¤§ç›¤
    "æ•´é«”å¸‚å ´": ["SPY", "QQQ", "DIA", "IWM"],
    "ç§‘æŠ€": ["VGT", "XLK", "QQQ"],
}

# ä¸»è¦è¿½è¹¤çš„ ETFï¼ˆæœ‰åƒ¹æ ¼æ•¸æ“šçš„ï¼‰
PRIMARY_ETFS = ["SPY", "QQQ", "VGT", "XLF", "XLE", "XLV", "DIA", "IWM", "GLD", "TLT"]

# è‚¡ç¥¨é—œéµå­—å°ç…§è¡¨ï¼ˆç”¨æ–¼è­˜åˆ¥æ–°èä¸­æåˆ°çš„è‚¡ç¥¨ï¼‰
STOCK_KEYWORDS = {
    # ç¾è‚¡ç§‘æŠ€å·¨é ­
    "NVDA": ["nvidia", "nvda", "è¼é”", "é»ƒä»å‹³"],
    "AAPL": ["apple", "aapl", "è˜‹æœ", "iphone", "ipad", "mac"],
    "MSFT": ["microsoft", "msft", "å¾®è»Ÿ", "azure", "windows"],
    "GOOGL": ["google", "googl", "alphabet", "è°·æ­Œ", "youtube"],
    "META": ["meta", "facebook", "fb", "è‡‰æ›¸", "instagram", "whatsapp"],
    "AMZN": ["amazon", "amzn", "äºé¦¬éœ", "aws"],
    "TSLA": ["tesla", "tsla", "ç‰¹æ–¯æ‹‰", "é¦¬æ–¯å…‹", "musk"],
    # åŠå°é«”
    "AMD": ["amd", "è¶…å¾®"],
    "INTC": ["intel", "intc", "è‹±ç‰¹çˆ¾"],
    "AVGO": ["broadcom", "avgo", "åšé€š"],
    "QCOM": ["qualcomm", "qcom", "é«˜é€š"],
    "MU": ["micron", "mu", "ç¾å…‰"],
    "ASML": ["asml", "è‰¾å¸æ‘©çˆ¾"],
    "TSM": ["tsmc", "å°ç©é›»", "2330"],
    "SMCI": ["supermicro", "smci", "è¶…å¾®é›»è…¦"],
    # AI/SaaS
    "PLTR": ["palantir", "pltr"],
    "CRM": ["salesforce", "crm"],
    "SNOW": ["snowflake", "snow"],
    "CRWD": ["crowdstrike", "crwd"],
    "DDOG": ["datadog", "ddog"],
    "MDB": ["mongodb", "mdb"],
    # é‡‘è
    "JPM": ["jpmorgan", "jpm", "æ‘©æ ¹å¤§é€š"],
    "GS": ["goldman", "gs", "é«˜ç››"],
    "MS": ["morgan stanley", "ms", "æ‘©æ ¹å£«ä¸¹åˆ©"],
    "BAC": ["bank of america", "bac", "ç¾éŠ€"],
    "V": ["visa"],
    "MA": ["mastercard", "ma"],
    # é†«ç™‚
    "UNH": ["unitedhealth", "unh"],
    "LLY": ["eli lilly", "lly", "ç¦®ä¾†"],
    "NVO": ["novo nordisk", "nvo", "è«¾å’Œè«¾å¾·"],
    "PFE": ["pfizer", "pfe", "è¼ç‘"],
    "MRNA": ["moderna", "mrna", "è«å¾·ç´"],
    # èƒ½æº
    "XOM": ["exxon", "xom", "åŸƒå…‹æ£®"],
    "CVX": ["chevron", "cvx", "é›ªä½›é¾"],
    # æ¶ˆè²»
    "WMT": ["walmart", "wmt", "æ²ƒçˆ¾ç‘ª"],
    "COST": ["costco", "cost", "å¥½å¸‚å¤š"],
    "NKE": ["nike", "nke", "è€å‰"],
    "SBUX": ["starbucks", "sbux", "æ˜Ÿå·´å…‹"],
    # å…¶ä»–
    "NFLX": ["netflix", "nflx", "ç¶²é£›"],
    "DIS": ["disney", "dis", "è¿ªå£«å°¼"],
    "BA": ["boeing", "ba", "æ³¢éŸ³"],
    "CAT": ["caterpillar", "cat", "é–‹æ‹“é‡å·¥"],
    # å°è‚¡
    "2330.TW": ["å°ç©é›»", "tsmc", "2330"],
    "2454.TW": ["è¯ç™¼ç§‘", "mtk", "2454"],
    "2317.TW": ["é´»æµ·", "foxconn", "2317"],
    "2308.TW": ["å°é”é›»", "delta", "2308"],
    "2382.TW": ["å»£é”", "quanta", "2382"],
    "2303.TW": ["è¯é›»", "umc", "2303"],
    "3711.TW": ["æ—¥æœˆå…‰", "ase", "3711"],
    "2379.TW": ["ç‘æ˜±", "realtek", "2379"],
    "3034.TW": ["è¯è© ", "novatek", "3034"],
    "2357.TW": ["è¯ç¢©", "asus", "2357"],
    "3231.TW": ["ç·¯å‰µ", "wistron", "3231"],
    "6669.TW": ["ç·¯ç©", "wiwynn", "6669"],
    "2408.TW": ["å—äºç§‘", "nanya", "2408"],
    "2412.TW": ["ä¸­è¯é›»", "cht", "2412"],
    "1301.TW": ["å°å¡‘", "formosa", "1301"],
    "2002.TW": ["ä¸­é‹¼", "csc", "2002"],
    "1101.TW": ["å°æ³¥", "tcc", "1101"],
}

# ç†±é–€è©±é¡Œé—œéµå­—
TRENDING_KEYWORDS = {
    # AI ç›¸é—œ
    "AI": ["ai", "äººå·¥æ™ºæ…§", "artificial intelligence", "æ©Ÿå™¨å­¸ç¿’", "deep learning"],
    "ChatGPT": ["chatgpt", "gpt", "openai", "claude", "gemini"],
    "AIæ™¶ç‰‡": ["ai chip", "gpu", "h100", "h200", "b100", "blackwell"],
    # è²¡ç¶“ä¸»é¡Œ
    "è²¡å ±": ["earnings", "è²¡å ±", "quarterly", "å­£å ±", "eps", "ç‡Ÿæ”¶", "revenue"],
    "é™æ¯": ["rate cut", "é™æ¯", "fed cut", "åˆ©ç‡ä¸‹èª¿"],
    "å‡æ¯": ["rate hike", "å‡æ¯", "fed hike", "åˆ©ç‡ä¸Šèª¿"],
    "é€šè†¨": ["inflation", "é€šè†¨", "cpi", "ç‰©åƒ¹"],
    "è¡°é€€": ["recession", "è¡°é€€", "ç¶“æ¿Ÿè¡°é€€"],
    "è£å“¡": ["layoff", "è£å“¡", "job cut", "workforce reduction"],
    # å¸‚å ´ä¸»é¡Œ
    "IPO": ["ipo", "ä¸Šå¸‚", "é¦–æ¬¡å…¬é–‹ç™¼è¡Œ"],
    "ä½µè³¼": ["merger", "acquisition", "m&a", "ä½µè³¼", "æ”¶è³¼"],
    "å›è³¼": ["buyback", "å›è³¼", "è‚¡ç¥¨å›è³¼"],
    "åˆ†æ‹†": ["spin-off", "spinoff", "åˆ†æ‹†"],
    # åœ°ç·£æ”¿æ²»
    "é—œç¨…": ["tariff", "é—œç¨…", "trade war", "è²¿æ˜“æˆ°"],
    "åˆ¶è£": ["sanction", "åˆ¶è£", "ban", "ç¦ä»¤"],
    "ä¸­åœ‹": ["china", "ä¸­åœ‹", "beijing", "åŒ—äº¬"],
    "å°ç£": ["taiwan", "å°ç£", "taipei"],
    # åŠ å¯†è²¨å¹£
    "æ¯”ç‰¹å¹£": ["bitcoin", "btc", "æ¯”ç‰¹å¹£"],
    "ä»¥å¤ªåŠ": ["ethereum", "eth", "ä»¥å¤ªåŠ"],
}


class DailyHotStocksAnalyzer:
    """æ¯æ—¥ç†±é–€è‚¡ç¥¨åˆ†æå™¨"""

    def __init__(self, news_db: str = "news.db", finance_db: str = "finance.db"):
        self.news_db = news_db
        self.finance_db = finance_db

    def get_news_conn(self):
        return sqlite3.connect(self.news_db)

    def get_finance_conn(self):
        return sqlite3.connect(self.finance_db)

    def get_daily_news(self, target_date: date) -> List[Dict]:
        """å–å¾—æŒ‡å®šæ—¥æœŸçš„æ–°è"""
        conn = self.get_news_conn()
        cursor = conn.cursor()

        query = """
            SELECT id, title, content, source, source_type,
                   COALESCE(
                       CASE WHEN source_type = 'ptt' THEN published_at ELSE collected_at END,
                       collected_at
                   ) as news_date
            FROM news
            WHERE DATE(COALESCE(
                CASE WHEN source_type = 'ptt' THEN published_at ELSE collected_at END,
                collected_at
            )) = ?
        """

        cursor.execute(query, (target_date.strftime('%Y-%m-%d'),))
        rows = cursor.fetchall()
        conn.close()

        return [
            {
                "id": row[0],
                "title": row[1],
                "content": row[2],
                "source": row[3],
                "source_type": row[4],
                "date": row[5]
            }
            for row in rows
        ]

    def analyze_stock_mentions(self, news_list: List[Dict]) -> List[Dict]:
        """
        åˆ†ææ–°èä¸­æåˆ°çš„è‚¡ç¥¨åŠå…¶æƒ…ç·’

        Returns:
            List of {symbol, name, mentions, bullish, bearish, sentiment, sample_news}
        """
        stock_data = {}

        for news in news_list:
            text = (news.get("title", "") + " " + (news.get("content") or "")).lower()

            # æª¢æŸ¥æ¯å€‹è‚¡ç¥¨æ˜¯å¦è¢«æåŠ
            for symbol, keywords in STOCK_KEYWORDS.items():
                mentioned = any(kw.lower() in text for kw in keywords)

                if mentioned:
                    if symbol not in stock_data:
                        stock_data[symbol] = {
                            "symbol": symbol,
                            "mentions": 0,
                            "bullish": 0,
                            "bearish": 0,
                            "news_ids": [],
                            "sample_titles": []
                        }

                    stock_data[symbol]["mentions"] += 1
                    stock_data[symbol]["news_ids"].append(news["id"])

                    if len(stock_data[symbol]["sample_titles"]) < 3:
                        stock_data[symbol]["sample_titles"].append(news["title"])

                    # è¨ˆç®—è©²æ–°èçš„æƒ…ç·’
                    pos = sum(1 for kw in POSITIVE_KEYWORDS if kw.lower() in text)
                    neg = sum(1 for kw in NEGATIVE_KEYWORDS if kw.lower() in text)

                    if pos > neg:
                        stock_data[symbol]["bullish"] += 1
                    elif neg > pos:
                        stock_data[symbol]["bearish"] += 1

        # è¨ˆç®—æƒ…ç·’åˆ†æ•¸ä¸¦æ’åº
        results = []
        for symbol, data in stock_data.items():
            total = data["bullish"] + data["bearish"]
            if total > 0:
                sentiment_score = (data["bullish"] - data["bearish"]) / total
            else:
                sentiment_score = 0

            # åˆ¤æ–·æƒ…ç·’
            if sentiment_score > 0.2:
                sentiment = "ğŸŸ¢ çœ‹å¤š"
            elif sentiment_score < -0.2:
                sentiment = "ğŸ”´ çœ‹ç©º"
            else:
                sentiment = "ğŸŸ¡ ä¸­æ€§"

            results.append({
                "symbol": symbol,
                "mentions": data["mentions"],
                "bullish": data["bullish"],
                "bearish": data["bearish"],
                "sentiment_score": sentiment_score,
                "sentiment": sentiment,
                "sample_titles": data["sample_titles"]
            })

        # æŒ‰æåŠæ¬¡æ•¸æ’åº
        results.sort(key=lambda x: x["mentions"], reverse=True)
        return results

    def analyze_trending_keywords(self, news_list: List[Dict]) -> List[Dict]:
        """
        åˆ†æç†±é–€é—œéµå­—

        Returns:
            List of {keyword, mentions, sentiment}
        """
        keyword_data = {}

        for news in news_list:
            text = (news.get("title", "") + " " + (news.get("content") or "")).lower()

            for topic, keywords in TRENDING_KEYWORDS.items():
                mentioned = any(kw.lower() in text for kw in keywords)

                if mentioned:
                    if topic not in keyword_data:
                        keyword_data[topic] = {
                            "keyword": topic,
                            "mentions": 0,
                            "bullish": 0,
                            "bearish": 0
                        }

                    keyword_data[topic]["mentions"] += 1

                    # è¨ˆç®—æƒ…ç·’
                    pos = sum(1 for kw in POSITIVE_KEYWORDS if kw.lower() in text)
                    neg = sum(1 for kw in NEGATIVE_KEYWORDS if kw.lower() in text)

                    if pos > neg:
                        keyword_data[topic]["bullish"] += 1
                    elif neg > pos:
                        keyword_data[topic]["bearish"] += 1

        # è¨ˆç®—æƒ…ç·’åˆ†æ•¸
        results = []
        for topic, data in keyword_data.items():
            total = data["bullish"] + data["bearish"]
            if total > 0:
                sentiment_score = (data["bullish"] - data["bearish"]) / total
            else:
                sentiment_score = 0

            if sentiment_score > 0.2:
                sentiment = "ğŸŸ¢ æ­£é¢"
            elif sentiment_score < -0.2:
                sentiment = "ğŸ”´ è² é¢"
            else:
                sentiment = "ğŸŸ¡ ä¸­æ€§"

            results.append({
                "keyword": topic,
                "mentions": data["mentions"],
                "bullish": data["bullish"],
                "bearish": data["bearish"],
                "sentiment_score": sentiment_score,
                "sentiment": sentiment
            })

        results.sort(key=lambda x: x["mentions"], reverse=True)
        return results

    def get_daily_summary(self, target_date: date) -> Dict:
        """
        å–å¾—æ¯æ—¥æƒ…ç·’æ‘˜è¦

        Returns:
            {
                date, news_count,
                hot_stocks: [...],
                trending_keywords: [...],
                overall_sentiment
            }
        """
        news_list = self.get_daily_news(target_date)

        if not news_list:
            return {
                "date": target_date,
                "news_count": 0,
                "hot_stocks": [],
                "trending_keywords": [],
                "overall_sentiment": "ç„¡æ•¸æ“š"
            }

        hot_stocks = self.analyze_stock_mentions(news_list)
        trending_keywords = self.analyze_trending_keywords(news_list)

        # è¨ˆç®—æ•´é«”æƒ…ç·’
        all_text = " ".join([
            (n.get("title", "") + " " + (n.get("content") or "")).lower()
            for n in news_list
        ])

        pos = sum(1 for kw in POSITIVE_KEYWORDS if kw.lower() in all_text)
        neg = sum(1 for kw in NEGATIVE_KEYWORDS if kw.lower() in all_text)

        if pos > neg * 1.3:
            overall = "ğŸŸ¢ æ•´é«”åå¤š"
        elif neg > pos * 1.3:
            overall = "ğŸ”´ æ•´é«”åç©º"
        else:
            overall = "ğŸŸ¡ å¤šç©ºäº¤ç¹”"

        return {
            "date": target_date,
            "news_count": len(news_list),
            "hot_stocks": hot_stocks[:20],  # Top 20
            "trending_keywords": trending_keywords[:15],  # Top 15
            "overall_sentiment": overall,
            "positive_count": pos,
            "negative_count": neg
        }

    def get_weekly_hot_stocks(self, end_date: date, days: int = 7) -> List[Dict]:
        """å–å¾—ä¸€é€±å…§çš„ç†±é–€è‚¡ç¥¨çµ±è¨ˆ"""
        all_stocks = {}

        for i in range(days):
            target_date = end_date - timedelta(days=i)
            daily = self.get_daily_summary(target_date)

            for stock in daily.get("hot_stocks", []):
                symbol = stock["symbol"]
                if symbol not in all_stocks:
                    all_stocks[symbol] = {
                        "symbol": symbol,
                        "total_mentions": 0,
                        "total_bullish": 0,
                        "total_bearish": 0,
                        "days_mentioned": 0
                    }

                all_stocks[symbol]["total_mentions"] += stock["mentions"]
                all_stocks[symbol]["total_bullish"] += stock["bullish"]
                all_stocks[symbol]["total_bearish"] += stock["bearish"]
                all_stocks[symbol]["days_mentioned"] += 1

        # è¨ˆç®—æƒ…ç·’ä¸¦æ’åº
        results = []
        for symbol, data in all_stocks.items():
            total = data["total_bullish"] + data["total_bearish"]
            if total > 0:
                score = (data["total_bullish"] - data["total_bearish"]) / total
            else:
                score = 0

            if score > 0.2:
                sentiment = "ğŸŸ¢ çœ‹å¤š"
            elif score < -0.2:
                sentiment = "ğŸ”´ çœ‹ç©º"
            else:
                sentiment = "ğŸŸ¡ ä¸­æ€§"

            results.append({
                "symbol": symbol,
                "total_mentions": data["total_mentions"],
                "days_mentioned": data["days_mentioned"],
                "bullish": data["total_bullish"],
                "bearish": data["total_bearish"],
                "sentiment_score": score,
                "sentiment": sentiment
            })

        results.sort(key=lambda x: x["total_mentions"], reverse=True)
        return results[:30]


class SentimentBacktester:
    """æ–°èæƒ…ç·’å›æ¸¬åˆ†æå™¨"""

    def __init__(self, news_db: str = "news.db", finance_db: str = "finance.db"):
        self.news_db = news_db
        self.finance_db = finance_db

    def get_news_conn(self):
        return sqlite3.connect(self.news_db)

    def get_finance_conn(self):
        return sqlite3.connect(self.finance_db)

    def calculate_daily_sentiment(self, start_date: date, end_date: date) -> pd.DataFrame:
        """
        è¨ˆç®—æ¯æ—¥æ•´é«”æ–°èæƒ…ç·’åˆ†æ•¸

        Returns:
            DataFrame with columns: date, positive_count, negative_count,
                                   sentiment_score, news_count
        """
        conn = self.get_news_conn()

        # å–å¾—æ—¥æœŸç¯„åœå…§çš„æ–°è
        query = """
            SELECT
                DATE(COALESCE(
                    CASE WHEN source_type = 'ptt' THEN published_at ELSE collected_at END,
                    collected_at
                )) as news_date,
                title,
                content
            FROM news
            WHERE DATE(COALESCE(
                CASE WHEN source_type = 'ptt' THEN published_at ELSE collected_at END,
                collected_at
            )) BETWEEN ? AND ?
            ORDER BY news_date
        """

        df = pd.read_sql_query(query, conn, params=(
            start_date.strftime('%Y-%m-%d'),
            end_date.strftime('%Y-%m-%d')
        ))
        conn.close()

        if df.empty:
            return pd.DataFrame()

        # è¨ˆç®—æ¯æ—¥æƒ…ç·’
        daily_sentiment = []
        for news_date, group in df.groupby('news_date'):
            text_all = " ".join([
                (str(row['title']) + " " + str(row['content'] or "")).lower()
                for _, row in group.iterrows()
            ])

            pos_count = sum(1 for kw in POSITIVE_KEYWORDS if kw in text_all)
            neg_count = sum(1 for kw in NEGATIVE_KEYWORDS if kw in text_all)

            # è¨ˆç®—æƒ…ç·’åˆ†æ•¸ (-1 åˆ° 1)
            total = pos_count + neg_count
            if total > 0:
                score = (pos_count - neg_count) / total
            else:
                score = 0

            daily_sentiment.append({
                'date': news_date,
                'positive_count': pos_count,
                'negative_count': neg_count,
                'sentiment_score': score,
                'news_count': len(group)
            })

        result = pd.DataFrame(daily_sentiment)
        result['date'] = pd.to_datetime(result['date'])
        return result

    def calculate_category_sentiment(self, category: str, keywords: List[str],
                                     start_date: date, end_date: date) -> pd.DataFrame:
        """
        è¨ˆç®—ç‰¹å®šç”¢æ¥­é¡åˆ¥çš„æ¯æ—¥æƒ…ç·’åˆ†æ•¸

        Args:
            category: ç”¢æ¥­é¡åˆ¥åç¨±
            keywords: è©²é¡åˆ¥çš„é—œéµå­—åˆ—è¡¨
            start_date: é–‹å§‹æ—¥æœŸ
            end_date: çµæŸæ—¥æœŸ

        Returns:
            DataFrame with daily sentiment for this category
        """
        conn = self.get_news_conn()

        # å»ºç«‹é—œéµå­—æœå°‹æ¢ä»¶
        keyword_conditions = " OR ".join([
            f"LOWER(title || ' ' || COALESCE(content, '')) LIKE '%{kw.lower()}%'"
            for kw in keywords
        ])

        query = f"""
            SELECT
                DATE(COALESCE(
                    CASE WHEN source_type = 'ptt' THEN published_at ELSE collected_at END,
                    collected_at
                )) as news_date,
                title,
                content
            FROM news
            WHERE DATE(COALESCE(
                CASE WHEN source_type = 'ptt' THEN published_at ELSE collected_at END,
                collected_at
            )) BETWEEN ? AND ?
            AND ({keyword_conditions})
            ORDER BY news_date
        """

        df = pd.read_sql_query(query, conn, params=(
            start_date.strftime('%Y-%m-%d'),
            end_date.strftime('%Y-%m-%d')
        ))
        conn.close()

        if df.empty:
            return pd.DataFrame()

        # è¨ˆç®—æ¯æ—¥æƒ…ç·’
        daily_sentiment = []
        for news_date, group in df.groupby('news_date'):
            text_all = " ".join([
                (str(row['title']) + " " + str(row['content'] or "")).lower()
                for _, row in group.iterrows()
            ])

            pos_count = sum(1 for kw in POSITIVE_KEYWORDS if kw in text_all)
            neg_count = sum(1 for kw in NEGATIVE_KEYWORDS if kw in text_all)

            total = pos_count + neg_count
            if total > 0:
                score = (pos_count - neg_count) / total
            else:
                score = 0

            daily_sentiment.append({
                'date': news_date,
                'category': category,
                'positive_count': pos_count,
                'negative_count': neg_count,
                'sentiment_score': score,
                'news_count': len(group)
            })

        result = pd.DataFrame(daily_sentiment)
        if not result.empty:
            result['date'] = pd.to_datetime(result['date'])
        return result

    def get_etf_returns(self, symbol: str, start_date: date, end_date: date) -> pd.DataFrame:
        """
        å–å¾— ETF çš„æ¯æ—¥å ±é…¬ç‡

        Returns:
            DataFrame with columns: date, close, return_1d, return_5d
        """
        conn = self.get_finance_conn()

        query = """
            SELECT date, close
            FROM daily_prices
            WHERE symbol = ?
            AND date BETWEEN ? AND ?
            ORDER BY date
        """

        df = pd.read_sql_query(query, conn, params=(
            symbol,
            start_date.strftime('%Y-%m-%d'),
            end_date.strftime('%Y-%m-%d')
        ))
        conn.close()

        if df.empty:
            return pd.DataFrame()

        df['date'] = pd.to_datetime(df['date'])

        # è¨ˆç®—å ±é…¬ç‡
        df['return_1d'] = df['close'].pct_change(1) * 100  # 1æ—¥å ±é…¬ç‡ (%)
        df['return_5d'] = df['close'].pct_change(5) * 100  # 5æ—¥å ±é…¬ç‡ (%)
        df['return_next_1d'] = df['return_1d'].shift(-1)   # éš”æ—¥å ±é…¬ç‡
        df['return_next_5d'] = df['close'].pct_change(5).shift(-5) * 100  # æœªä¾†5æ—¥å ±é…¬ç‡

        return df

    def run_backtest(self, etf_symbol: str = "SPY",
                     start_date: date = None,
                     end_date: date = None,
                     lead_days: int = 1) -> Dict:
        """
        åŸ·è¡Œå›æ¸¬ï¼šæ¯”è¼ƒæ–°èæƒ…ç·’èˆ‡ ETF è¡¨ç¾

        Args:
            etf_symbol: ETF ä»£ç¢¼
            start_date: é–‹å§‹æ—¥æœŸ
            end_date: çµæŸæ—¥æœŸ
            lead_days: é ˜å…ˆå¤©æ•¸ï¼ˆæƒ…ç·’é ˜å…ˆåƒ¹æ ¼å¤šå°‘å¤©ï¼‰

        Returns:
            å›æ¸¬çµæœå­—å…¸
        """
        if end_date is None:
            end_date = date.today()
        if start_date is None:
            start_date = end_date - timedelta(days=365)

        # å–å¾—æƒ…ç·’æ•¸æ“š
        sentiment_df = self.calculate_daily_sentiment(start_date, end_date)
        if sentiment_df.empty:
            return {"error": "ç„¡æ–°èæ•¸æ“š"}

        # å–å¾— ETF æ•¸æ“š
        etf_df = self.get_etf_returns(etf_symbol, start_date, end_date)
        if etf_df.empty:
            return {"error": f"ç„¡ {etf_symbol} åƒ¹æ ¼æ•¸æ“š"}

        # åˆä½µæ•¸æ“š
        merged = pd.merge(sentiment_df, etf_df, on='date', how='inner')

        if merged.empty:
            return {"error": "ç„¡æ³•åˆä½µæ•¸æ“š"}

        # è¨ˆç®—é ˜å…ˆæŒ‡æ¨™ï¼ˆæƒ…ç·’é ˜å…ˆåƒ¹æ ¼ N å¤©ï¼‰
        merged['sentiment_lagged'] = merged['sentiment_score'].shift(lead_days)

        # ç§»é™¤ NaN
        analysis_df = merged.dropna(subset=['sentiment_lagged', 'return_1d'])

        if len(analysis_df) < 10:
            return {"error": "æ•¸æ“šé»ä¸è¶³"}

        # è¨ˆç®—ç›¸é—œæ€§
        correlation = analysis_df['sentiment_lagged'].corr(analysis_df['return_1d'])

        # è¨ˆç®—å‹ç‡
        # ç•¶æƒ…ç·’ç‚ºæ­£ï¼ˆ>0ï¼‰ï¼Œéš”æ—¥ä¸Šæ¼²çš„æ©Ÿç‡
        positive_sentiment = analysis_df[analysis_df['sentiment_lagged'] > 0]
        if len(positive_sentiment) > 0:
            win_rate_positive = (positive_sentiment['return_1d'] > 0).mean() * 100
        else:
            win_rate_positive = 0

        # ç•¶æƒ…ç·’ç‚ºè² ï¼ˆ<0ï¼‰ï¼Œéš”æ—¥ä¸‹è·Œçš„æ©Ÿç‡
        negative_sentiment = analysis_df[analysis_df['sentiment_lagged'] < 0]
        if len(negative_sentiment) > 0:
            win_rate_negative = (negative_sentiment['return_1d'] < 0).mean() * 100
        else:
            win_rate_negative = 0

        # è¨ˆç®—æƒ…ç·’æ¥µç«¯å€¼çš„é æ¸¬åŠ›
        # æƒ…ç·’æ¥µåº¦æ¨‚è§€ï¼ˆ>0.3ï¼‰
        very_positive = analysis_df[analysis_df['sentiment_lagged'] > 0.3]
        if len(very_positive) > 0:
            avg_return_very_positive = very_positive['return_1d'].mean()
            win_rate_very_positive = (very_positive['return_1d'] > 0).mean() * 100
        else:
            avg_return_very_positive = 0
            win_rate_very_positive = 0

        # æƒ…ç·’æ¥µåº¦æ‚²è§€ï¼ˆ<-0.3ï¼‰
        very_negative = analysis_df[analysis_df['sentiment_lagged'] < -0.3]
        if len(very_negative) > 0:
            avg_return_very_negative = very_negative['return_1d'].mean()
            win_rate_very_negative = (very_negative['return_1d'] > 0).mean() * 100
        else:
            avg_return_very_negative = 0
            win_rate_very_negative = 0

        # è¨ˆç®—ä¸åŒé ˜å…ˆå¤©æ•¸çš„ç›¸é—œæ€§
        lead_correlations = {}
        for lead in [1, 2, 3, 5, 7]:
            temp_df = merged.copy()
            temp_df['sent_lead'] = temp_df['sentiment_score'].shift(lead)
            temp_df = temp_df.dropna(subset=['sent_lead', 'return_1d'])
            if len(temp_df) > 10:
                lead_correlations[lead] = temp_df['sent_lead'].corr(temp_df['return_1d'])

        return {
            "etf_symbol": etf_symbol,
            "period": f"{start_date} ~ {end_date}",
            "data_points": len(analysis_df),
            "lead_days": lead_days,
            "correlation": correlation,
            "lead_correlations": lead_correlations,
            "win_rate": {
                "positive_sentiment_up": win_rate_positive,
                "negative_sentiment_down": win_rate_negative,
                "overall": (win_rate_positive + win_rate_negative) / 2
            },
            "extreme_sentiment": {
                "very_positive": {
                    "count": len(very_positive),
                    "avg_return": avg_return_very_positive,
                    "win_rate": win_rate_very_positive
                },
                "very_negative": {
                    "count": len(very_negative),
                    "avg_return": avg_return_very_negative,
                    "win_rate": win_rate_very_negative  # é€™è£¡æ˜¯ä¸Šæ¼²æ©Ÿç‡
                }
            },
            "sentiment_stats": {
                "mean": analysis_df['sentiment_score'].mean(),
                "std": analysis_df['sentiment_score'].std(),
                "min": analysis_df['sentiment_score'].min(),
                "max": analysis_df['sentiment_score'].max()
            },
            "return_stats": {
                "mean": analysis_df['return_1d'].mean(),
                "std": analysis_df['return_1d'].std(),
                "total_return": ((1 + analysis_df['return_1d']/100).prod() - 1) * 100
            },
            "merged_data": merged  # ç”¨æ–¼ç¹ªåœ–
        }

    def run_multi_etf_backtest(self, etf_symbols: List[str] = None,
                                start_date: date = None,
                                end_date: date = None) -> List[Dict]:
        """
        å°å¤šå€‹ ETF åŸ·è¡Œå›æ¸¬
        """
        if etf_symbols is None:
            etf_symbols = PRIMARY_ETFS

        results = []
        for symbol in etf_symbols:
            try:
                result = self.run_backtest(symbol, start_date, end_date)
                if "error" not in result:
                    results.append(result)
            except Exception as e:
                logger.error(f"å›æ¸¬ {symbol} å¤±æ•—: {e}")

        return results

    def get_sentiment_signal(self, lookback_days: int = 7) -> Dict:
        """
        å–å¾—ç•¶å‰æƒ…ç·’ä¿¡è™Ÿ

        Args:
            lookback_days: å›é¡§å¤©æ•¸

        Returns:
            ç•¶å‰æƒ…ç·’ä¿¡è™Ÿå’Œå»ºè­°
        """
        end_date = date.today()
        start_date = end_date - timedelta(days=lookback_days)

        sentiment_df = self.calculate_daily_sentiment(start_date, end_date)

        if sentiment_df.empty:
            return {"signal": "NEUTRAL", "score": 0, "message": "ç„¡è¶³å¤ æ•¸æ“š"}

        # è¨ˆç®—è¿‘æœŸå¹³å‡æƒ…ç·’
        avg_sentiment = sentiment_df['sentiment_score'].mean()
        recent_sentiment = sentiment_df['sentiment_score'].iloc[-1] if len(sentiment_df) > 0 else 0

        # è¨ˆç®—æƒ…ç·’è¶¨å‹¢
        if len(sentiment_df) >= 3:
            trend = sentiment_df['sentiment_score'].iloc[-3:].mean() - \
                    sentiment_df['sentiment_score'].iloc[:3].mean()
        else:
            trend = 0

        # åˆ¤æ–·ä¿¡è™Ÿ
        if avg_sentiment > 0.2 and trend > 0:
            signal = "BULLISH"
            message = "æƒ…ç·’æ¨‚è§€ä¸”æŒçºŒå‘ä¸Šï¼Œå¸‚å ´å¯èƒ½å»¶çºŒæ¼²å‹¢"
        elif avg_sentiment < -0.2 and trend < 0:
            signal = "BEARISH"
            message = "æƒ…ç·’æ‚²è§€ä¸”æŒçºŒå‘ä¸‹ï¼Œå¸‚å ´å¯èƒ½å»¶çºŒè·Œå‹¢"
        elif avg_sentiment > 0.3:
            signal = "CAUTION_HIGH"
            message = "æƒ…ç·’éåº¦æ¨‚è§€ï¼Œæ³¨æ„å¯èƒ½çš„åè½‰é¢¨éšª"
        elif avg_sentiment < -0.3:
            signal = "CONTRARIAN_BUY"
            message = "æƒ…ç·’æ¥µåº¦æ‚²è§€ï¼Œå¯èƒ½æ˜¯é€†å‹¢ä½ˆå±€æ©Ÿæœƒ"
        else:
            signal = "NEUTRAL"
            message = "æƒ…ç·’ä¸­æ€§ï¼Œå¸‚å ´æ–¹å‘ä¸æ˜ç¢º"

        return {
            "signal": signal,
            "avg_sentiment": avg_sentiment,
            "recent_sentiment": recent_sentiment,
            "trend": trend,
            "message": message,
            "data": sentiment_df
        }


def run_full_analysis():
    """åŸ·è¡Œå®Œæ•´åˆ†æä¸¦è¼¸å‡ºå ±å‘Š"""
    backtester = SentimentBacktester()

    print("=" * 60)
    print("æ–°èæƒ…ç·’ vs ETF å›æ¸¬åˆ†æ")
    print("=" * 60)

    # å°ä¸»è¦ ETF åŸ·è¡Œå›æ¸¬
    results = backtester.run_multi_etf_backtest()

    for result in results:
        print(f"\nğŸ“Š {result['etf_symbol']}")
        print(f"   æœŸé–“: {result['period']}")
        print(f"   æ•¸æ“šé»: {result['data_points']}")
        print(f"   ç›¸é—œæ€§: {result['correlation']:.4f}")
        print(f"   å‹ç‡ (æƒ…ç·’æ­£â†’æ¼²): {result['win_rate']['positive_sentiment_up']:.1f}%")
        print(f"   å‹ç‡ (æƒ…ç·’è² â†’è·Œ): {result['win_rate']['negative_sentiment_down']:.1f}%")

    # ç•¶å‰ä¿¡è™Ÿ
    signal = backtester.get_sentiment_signal()
    print(f"\n{'=' * 60}")
    print(f"ğŸ“¡ ç•¶å‰æƒ…ç·’ä¿¡è™Ÿ: {signal['signal']}")
    print(f"   {signal['message']}")

    return results


if __name__ == "__main__":
    run_full_analysis()
